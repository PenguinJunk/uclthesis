@String { ao        = {Applied Optics} }
@String { csr       = {Continental Shelf Research} }
@String { cvgip     = {Computing Vision Graphics Image Processing} }
@String { iefuzzy   = {IEEE Transactions on Fuzzy Systems} }
@String { iegrs     = {IEEE Transactions on Geoscience and Remote Sensing} }
@String { ieip      = {IEEE Transactions on Image Processing} }
@String { ieit      = {IEEE Transactions on Information Theory} }
@String { iepami    = {IEEE Transactions on Pattern Analysis and Machine Intelligence} }
@String { iesmc     = {IEEE Transactions on Systems Man and Cybernetics} }
@String { iesp      = {IEEE Transactions on Signal Processing} }
@String { ijprs     = {International Journal of Photogrammetry and Remote Sensing} }
@String { ijrs      = {International Journal of Remote Sensing} }
@String { jgr       = {Journal of Geophysical Research} }
@String { jmr       = {Journal of Marine Research} }
@String { joptsocam = {Journal of the Optical Society of America} }
@String { jpr       = {Journal of Plankton Research} }
@String { lo        = {Limnology and Oceanography} }
@String { pers      = {Photogrammetric Engineering and Remote Sensing} }
@String { rse       = {Remote Sensing of Environment} }

@InProceedings{TrespAN93,
  author    = {Volker Tresp and Subutai Ahmad and Ralph Neuneier},
  title     = {Training Neural Networks with Deficient Data},
  booktitle = {NIPS},
  year      = {1993},
  pages     = {128-135},
  url       = {http://nips.djvuzone.org/djvu/nips06/0128.djvu},
  comment   = {Paper describing how to deal with missing data when training neural networks. Show why giving average value can result in poor test results. Give details of procedure for training with incomplete data: 1. Train the network with the complete data and estimate ?the squared standard deviation of the target values 2. Estimate the input density using Gaussian mixtures 3. Include the incomplete training patterns in the training 4. For every incomplete training pattern, approximate the joint probability of the complete inputs and outputs and the differential of this. I don't understand the value of this step and certainly don't understand why it doesn't appear to use the information in the incomplete inputs. Goes on to suggest easier approximations to this method.},
  crossref  = {DBLP:conf/nips/1993},
  keywords  = {Machine Learning, incomplete data},
  owner     = {ISargent},
  creationdate = {2013.11.15},
}

@InProceedings{AdlerEHR13,
  author       = {Amir Adler and Michael Elad and Yacov Hel-Or and Ehud Rivlin},
  booktitle    = {2013 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING},
  title        = {Sparse Coding with Anomaly Detection},
  address      = {SOUTHAMPTON, UK},
  comment      = {Given a dictionary, D, a signal, y, can be encoded as y = Dx where x is sparse. A collection of signals, Y, can be encoded as Y = DX where X is sparse a matrix. In the standard case, ``each signal is assumed to be a single measurement associated with a unique non-zero pattern of its sparse representation (i.e. a unique combination of atoms)''. This is the Single Measurement Vector (SMV) model. However, the case ``in which all the sparse representations share the same non-zero pattern is referred to as the Multiple Measurement Vector (MMV) or joint-sparsity model''. This paper gives simple graphical representations of this although I can't quite understand the when the MMV would apply. For anomaly detection, the model is extended to Y = DX + E +V. It assumed that D is known, ``E has few non-zero columns that equal to the deviation of each outlier from the sparse representations model, and V is a low-energy noise component''. Provides lots of pseudo code and applies the Alternating Directoin method of Multipliers (ADMM) to finding anomalies. Applied method of arrhythmia detection and to remove specular reflectance and shadow from natural images.},
  creationdate = {2013.09.27},
  keywords     = {Machine Learning, Sparse Coding, Anomaly Detection},
  month        = {9},
  owner        = {ISargent},
  year         = {2013},
}

@Misc{Foresight20202015,
  author    = {AGI},
  title     = {Foresight Report 2020},
  owner     = {ISargent},
  creationdate = {2016.12.05},
  year      = {2015},
}

@InProceedings{AlamWWFCP2013,
  author       = {N. Alam and D. Wagner and M. Wewetzer and von Falkenhausen, J. and V. Coors and M. Pries},
  title        = {Towards Automatic Validation and Healing of {CityGML} Models for Geometric and Semantic Consistency},
  booktitle    = {ISPRS 8th 3DGeoInfo Conference \& WG II/2 Workshop},
  year         = {2013},
  date         = {27 -- 29 November},
  volume       = {II-2/W1},
  url          = {http://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/II-2-W1/1/2013/isprsannals-II-2-W1-1-2013.pdf},
  address      = {Istanbul, Turkey},
  comment      = {Method of validating 3D data that checks against 17 criteria of logical consistency which are divided into polygon checks, solid checks and semantic checks. This is a method of quality assessing the model. Methods of 'healing' faults are also presented. No reference to customer needs as this is really about modelling rather than real-world representation, IMO.},
  journaltitle = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  keywords     = {3D Quality, 3DCharsPaper},
  owner        = {ISargent},
  creationdate    = {2015.11.05},
}

@InProceedings{HassanS13,
  author    = {Ali Hassan, Arslan Shaukat},
  title     = {Covariate Shift Approach For Invariant Texture Classification},
  booktitle = {2013 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SI GNAL PROCESSING},
  year      = {2013},
  month     = {9},
  address   = {SOUTHAMPTON, UK},
  comment   = {The covariate shift is the shift in the input densities between the training data set and the testing data set. This occurs even thought the conditional distribution remains the same. Importance weighting is a method of overcoming this by weighting training inputs according to how well they represent the testing input distribution (references given). One method of importance weighting uses the Kullback-Leibler (KL) divergence. This was applied to SVM to created an importance weighted support vector machine (IW-SVM). This paper used the Brodatz texture album which contains textures from real-world things. The album also contains rotated and scaled textures. The IW-SVM doesn't perform as well as other method with the non-rotated test set but performance is better for the rotated and the scaled data.},
  keywords  = {Machine Learning, Classification, Texture Analysis},
  owner     = {ISargent},
  creationdate = {2013.09.27},
}

@InProceedings{AliNNB13,
  author    = {Ali, Wafa Bel Haj and Nock, Richard and Frank Nielsen and Michel Barlaud},
  title     = {Fast Newton Nearest Neighbors Boosting For Image Classification},
  booktitle = {2013 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING},
  year      = {2013},
  month     = {9},
  address   = {SOUTHAMPTON, UK},
  comment   = {Massive databases of images, each of which belong to one or more class. Propose Newton-Raphson variant of k-NN to perform classification.},
  keywords  = {Machine Learning, Image Categorisation, ImageLearn},
  owner     = {isargent},
  creationdate = {2013.10.03},
}

@Article{AndersonBT2000,
  author       = {David R. Anderson and Kenneth P. Burnham and William L. Thompson},
  journaltitle = {The Journal of Wildlife Management},
  title        = {Null Hypothesis Testing: Problems, Prevalence, and an Alternative},
  number       = {4},
  pages        = {912-923},
  url          = {http://warnercnr.colostate.edu/~anderson/PDF_files/TESTING.pdf},
  volume       = {64},
  comment      = {Paper cited by Nate Silver as one of many rejecting Fisherian (frequentist) statistics. ``We recommend [tests of statistical null hypothesis] by reduced in favor of more informative apporaches. Towards this objective, we describe a relatively new paradigm of data analysis based on Kullback-Leibler information. This paradigm is an extension of likelihood theory and, when used correctly, avoids many of the fundamental limitations and common misuses of null hypothesis testing''. Go into K-L and Akeike's information criterion (AIC) in more detail as well as giving an example of its use for understanding Eider survival. Also suggest that Bayesian approaches are a good alternative ``but seem computationally difficult''.},
  creationdate = {2014.10.03},
  keywords     = {statistics},
  owner        = {ISargent},
  year         = {2000},
}

@Article{AndrieuDDJ03,
  author       = {Christophe Andrieu and De Freitas, Nando and Arnaud Doucet and Jordan, Michael I.},
  journaltitle = {Machine Learning},
  title        = {An Introduction to MCMC for Machine Learning},
  pages        = {5--43},
  volume       = {50},
  comment      = {http://cis.temple.edu/~latecki/Courses/RobotFall07/PapersFall07/andrieu03introduction.pdf
Review:
Gives a fairly detailed history of MCMC including references. Reasons for doing MCMC: 1) Bayesian Inference uses MCMC for Normalisation, Marginalisation and Expectation; 2) Statistical mechanics uses MCMC for computing the partition (which is analogous to normalisation); 3) Optimisation and; 4) Penalised likelihood model selection. Also simulation of physical systems. Also describes rejection sampling and importance sampling. the MCMC ``mechanism is constructed so that the chain spends more time in the most important regions''. Convergence will occur is the transition matrix obeys ``Irreducibility: For any state of the Markov chain, there is a positive probability of visiting all other states. That is, the matrix T cannot be reduced to separate smaller matrices, which is also the same as stating that the transition graph is connected. Aperiodicity: The chain should not get trapped in cycles.''. Includes Metropolis-Hastings algorithm, Simulated Annealing for global optimisation, Mixtures and cycles of MCMC kernels, The Gibbs sampler, Monte Carlo EM (expectation-maximisation) and much more. Includes pseudo-code. Not finished reading.},
  creationdate = {2014.06.04},
  keywords     = {Markov Chain Monte Carlo, machine learning, sampling},
  owner        = {ISargent},
  year         = {2003},
}

@Article{Neuromorphic13,
  author       = {Anon},
  journaltitle = {The Economist},
  title        = {Neuromorphic computing: The machine of a new soul},
  url          = {http://www.economist.com/news/science-and-technology/21582495-computers-will-help-people-understand-brains-better-and-understanding-brains?fsrc=scn/tw_ec/the_machine_of_a_new_soul},
  comment      = {Building computers that operate like brains. This is more than just neural network software but hardware that works a bit like brains. Karlheinz Meier, University of Heidelberg three characteristics that brains have and computers do not: 1) low power consumption (human brains use about 20 watts, whereas the supercomputers currently used to try to simulate them need megawatts) 2) fault tolerance (losing just one transistor can wreck a microprocessor, but brains lose neurons all the time) 3) a lack of need to be programmed (brains learn and change spontaneously as they interact with the world, instead of following the fixed paths and branches of a predetermined algorithm) ``Science has a passable knowledge of how individual nerve cells, known as neurons, work. It also knows which visible lobes and ganglia of the brain do what. But how the neurons are organised in these lobes and ganglia remains obscure. Yet this is the level of organisation that does the actual thinking, and is, presumably, the seat of consciousness.'' ``asynchronous signalling ... can process data more quickly than the synchronous sort, since no time is wasted waiting for the clock to tick'' Analogue computers, sub-threshold domain. ``a human brain contains about 86 billion neurons, each is within two or three connections of all the others via myriad potential routes''. ``The neocortex, where most neurons reside and which accounts for three-quarters of the brain's volume, is made up of lots of columns, each of which contains about 70,000 neurons.''},
  creationdate = {2013.10.15},
  keywords     = {neuromorphic computing},
  month        = {8},
  owner        = {ISargent},
  year         = {2013},
}

@Unpublished{Archbold2016,
  Title                    = {Personal Communication: Outlining how we are developing our strategy by understanding existing and future opportunities.},
  Author                   = {Neal Archbold},
  Note                     = {2 March 2016},
  Year                     = {2016},

  Keywords                 = {ImageLearn},
  Owner                    = {ISargent},
  creationdate                = {2016.03.03}
}

@InProceedings{ArefiEHM2008,
  author       = {Arefi, H and Engels, J and Hahn, M and Mayer, H},
  booktitle    = {XXIst ISPRS Congress},
  title        = {Levels of Detail in 3D Building Reconstruction for LiDAR Data},
  pages        = {485 - 490},
  url          = {http://www.isprs.org/proceedings/XXXVII/congress/3b_pdf/92.pdf},
  address      = {Beijing, China},
  comment      = {3D buildings reconstructed in 3 stages corresponding to the first 3 CityGML LODs: ``. In the first LOD (LOD0), the Digital Terrain Model extracted from LIDAR data is represented. For this purpose the Digital Surface Model is filtered using geodesic morphology. A prismatic model containing the major walls of the building is generated to form the LOD1. The building outlines are detected by classification of non-ground objects and the building outlines are approximated by two approaches; hierarchical fitting of Minimum Boundary rectangles (MBR) and RANSAC based straight line fitting algorithm. LOD2 is formed by including the roof structures into the model. For this purpose, a model driven approach based on the analysis of the 3D points in a 2D projection plane is proposed. A building region is divided into smaller parts according to the direction and the number of ridge lines, which are extracted using geodesic morphology. The 3D model is derived for each building part. Finally, a complete building model is formed by merging the 3D models of the building parts and adjusting the nodes after the merging process.''},
  creationdate = {2014.10.28},
  keywords     = {3D buildings},
  owner        = {ISargent},
  year         = {2008},
}

@Article{ArelRK10,
  author       = {Itamar Arel and Rose, Derek C and Karnowski, Thomas P},
  title        = {Deep Machine Learning - A New Frontier in Artificial Intelligence Research},
  journaltitle = {IEEE Computational Intelligence Magazine},
  year         = {2010},
  month        = {11},
  pages        = {13-18},
  url          = {http://web.eecs.utk.edu/~itamar/Papers/CIM2010.pdf},
  comment      = {Bought from IEEE, find in library in personal drive. A useful overview of deep (machine) learning. Covers convolutional neural networks, deep belief networks, convolutional deep belief networks and Hierarchical temporary memory. Useful references for all of these as well as neuro-science on which it is based. A short section on applications (handwriting, speech, face as well as one reference for general objects HuangL06).},
  keywords     = {Machine Learning, Representation Learning, Deep Learning, ImageLearn},
  owner        = {isargent},
  creationdate    = {2013.08.08},
}

@InProceedings{AzimN13,
  author    = {Tayyaba Azim and Mahesan Niranjan},
  title     = {Inducing Discrimination In Biologically Inspired Models Of Visual Scene Recognition},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2013},
  month     = {6},
  address   = {Providence, RI, USA},
  comment   = {University of Southampton. Quite hard to understand. Paper is about the discriminatory ability of two generative models, multivariate Gaussian and Restricted Boltzman machine. Propose to derive a Fisher kernel (kernal that enhances discrimination between observations, I think) from the RBM. Use texture and handwritten character benchmark data sets. Results don't look all that convincing but it is very interesting that this work is being undertaken at Southampton.},
  keywords  = {Machine Learning, Computer Vision, Representation Learning, ImageLearn},
  owner     = {ISargent},
  creationdate = {2013.09.30},
}

@Article{BaatzKCGP2012,
  author       = {Baatz, G and K\''oser, K and Chen, D and Grzeszczuk, R and Pollefeys, M},
  journaltitle = {International Journal of Computer Vision},
  title        = {Leveraging 3D City Models for Rotation Invariant Place-of-Interest Recognition},
  number       = {3},
  pages        = {315-334},
  url          = {http://www.inf.ethz.ch/personal/pomarc/pubs/BaatzIJCV11.pdf},
  volume       = {96},
  comment      = {Paper about recognising buildings from phone camera images. Use 3D building models to create 'orthos' of building facades and the gravity information to determine top and bottom so that 'upright SIFT' can be used to improve matching.},
  creationdate = {2014.09.30},
  keywords     = {3D buildings},
  owner        = {ISargent},
  year         = {2012},
}

@Article{BahuKKM2014,
  author       = {Bahu, Jean-Marie and Koch, Andreas and Kremers, Enrique and Murshed, Syed Monjur},
  title        = {Towards a 3D Spatial Urban Energy Modelling Approach},
  journaltitle = {Int. J. 3D Inf. Model.},
  year         = {2014},
  volume       = {3},
  number       = {3},
  month        = {7},
  pages        = {1--16},
  issn         = {2156-1710},
  doi          = {10.4018/ij3dim.2014070101},
  url          = {http://dx.doi.org/10.4018/ij3dim.2014070101},
  acmid        = {2738646},
  address      = {Hershey, PA, USA},
  comment      = {use 3D models to model energy in city - energy loss and gain through buildings. buildings modelled to LOD2 with some semantic information},
  date   = {2014},
  keywords     = {Agent-Based Modelling, Energy Systems, Heat Energy Demand, Keywords: 3D City Model, Urban Planning, 3DCharsPaper},
  numpages     = {16},
  owner        = {ISargent},
  publisher    = {IGI Global},
  creationdate    = {2015.11.09},
}

@Book{BallardB1982,
  author        = {D. H. Ballard and C. M. Brown},
  title         = {Computer Vision},
  year          = {1982},
  publisher     = {Prentice-Hall},
  address       = {Englewood Cliffs, NJ},
  comment       = {Guide to computer vision for artificial intelligence. All basic functions, template matching, edge detection, segmentation},
  creationdate    = {2016-12-13 13:58:41 +0000},
  date-modified = {2016-12-13 14:00:41 +0000},
  keywords      = {vision},
  owner         = {ISargent},
  creationdate     = {2017.04.13},
}

@InProceedings{BankoB01,
  author       = {Michele Banko and Eric Brill},
  title        = {Scaling to very very large corpora for natural language disambiguation},
  booktitle    = {ACL '01: Proceedings of the 39th Annual Meeting on Association for Computational Linguistics},
  year         = {2001},
  organization = {Association for Computational Linguistics},
  pages        = {26--33},
  url          = {https://dl.acm.org/doi/10.3115/1073012.1073017},
  abstract     = {The amount of readily available online text has reached hundreds of billions of words and continues to grow. Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less. In this paper, we evaluate the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambiguation, when trained on orders of magnitude more labeled data than has previously been used. We are fortunate that for this particular application, correctly labeled training data is free. Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost.},
  address      = {Morristown, NJ, USA},
  comment      = {Working for Microsoft on better grammar checking (part of natural language processing). instead on putting effort into improved algorithms, tested effect of applying massive data sets to problem. Found that algorithm type / method less important than size of training data set. Noting that it can be expensive to obtain very large training data sets, they go on suggest solutions. Active learning is initiated by training a classifier, or committee of classifiers, using a seed data set and then applying an unlabelled data set. Those samples for which the classifier shows greatest uncertainty (for example, little relative difference weights at the different labels or a lot of difference between the classifiers in the committee) are then manually annotated before retraining. In this work, they used the uncertainty to select half the set of the manual annotation and the other half was chosen manually. This reducing the amount of annotation required considerably. Also try weakly-supervised learning in which a committee of classifiers is trained with a seed training data set (seed corpus). The classifiers is then used to classify unlabelled data. Those examples for which the classification agrees are expected to be correctly labelled and these are then used as training data. They found there was a increase in classification accuracy up to a point at which it is probably that the sample bias introduced by the method used to select these training data offsets the gains made by the additional data. Suggest combining active learning with unsupervised learning.},
  keywords     = {Natural Language Processing, Machine Learning, Microsoft Research},
  owner        = {ISargent},
  creationdate    = {2013.06.28},
}

@InProceedings{BarchiesiP13,
  author    = {Daniele Barchiesi and Plumbley, Mark D.},
  title     = {Learning Incoherent Subspaces For Classification Via Supervised Iterative Projections And Rotations},
  booktitle = {2013 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING},
  year      = {2013},
  month     = {9},
  address   = {SOUTHAMPTON, UK},
  comment   = {Includes brief review of feature transform methods principal component analysis (PCA) and Fisher's linear discriminant analysis (LDA). Also simple description of incoherent dictionary learning 'Dictionary learning aims at optimising a dictionary for sparse approximation given a set of training data. It is an unsupervised technique that can be thought as being a generalisation of PCA, as both methods learn linear subspaces that minimise the approximation error of the signals. Dictionary learning, however, is generally more flexible than PCA because it can be employed to learn more general non-orthogonal over-complete dictionaries'. includes useful descriptions, references and pseudo code for incoherence subspace projection. Test on iris data set and include some plots that look good.},
  keywords  = {Machine Learning, Classification, Feature TraCoding, Sparse Coding},
  owner     = {ISargent},
  creationdate = {2013.09.30},
}

@Article{BarnsleyB1997,
  author       = {Barnsley, M. J. and Barr, S. L.},
  title        = {A graph-based structural pattern recognition system to infer urban land use from fine spatial resolution land-cover data},
  journaltitle = {Computers, Environment and Urban Systems},
  year         = {1997},
  volume       = {21},
  number       = {3/4},
  pages        = {209-},
  comment      = {The paper demonstrating how different ages of housing estate are derived based on the structure of graphs used to model the relationships of building vectors in Land-Line data.},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  creationdate    = {2015.09.08},
}

@Article{BarrettS2015,
  author       = {Lisa Feldman Barrett and W. Kyle Simmons},
  title        = {Interoceptive predictions in the brain},
  journaltitle = {Nature Reviews Neuroscience},
  year         = {2015},
  volume       = {16},
  pages        = {419--429},
  doi          = {doi:10.1038/nrn3950},
  url          = {http://www.nature.com/nrn/journal/v16/n7/full/nrn3950.html},
  comment      = {Rather than being a passive 'stimulus-response' organ, neuroscience is discovering that the brain anticipate incoming stimuli and cascades these prediction through the cortex. Therefore, rather than simply waiting for events to happen, the brain prepares the cortex for sensory input from the environment. The predictions can be tested about the actual sensory input with a goal of preparing the cortex fot he input and minimising the error in the prediction. Gives detail of structure of the brain and a lot of links to both other articles and information boxes.},
  howpublished = {online},
  keywords     = {ImageLearn, Neuroscience},
  owner        = {ISargent},
  creationdate    = {2015.07.04},
}

@Article{BarryD10,
  author       = {Caswell Barry and Christian F. Doeller},
  journaltitle = {The Journal of Neuroscience},
  title        = {Conjunctive Representations in the Hippocampus: What and Where?},
  number       = {3},
  pages        = {799--801},
  url          = {http://www.jneurosci.org/content/30/3/799.full.pdf},
  volume       = {30},
  comment      = {A review of Review of KomorowskiME09 by grad students. Talks about place cells and conjunctive coding in the hippocampus. ``The findings by Komorowski et al. (2009) now provide evidence that single cells in the hippocampus represent the spatial context in conjunction with the items encountered and that this representation supports behavior. Furthermore, that the conjunctive code apparently develops from a purely spatial code. The results are entirely consistent with the theory that during first exposure to an environment a spatial map-like representation is formed in the hippocampus and items and events are then encoded onto that in their spatial context (O'Keefe and Nadel, 1978)''},
  creationdate = {2014.06.10},
  keywords     = {Neuroscience},
  owner        = {ISargent},
  year         = {2010},
}

@Article{Barsalou2008,
  author       = {Lawrence W. Barsalou},
  title        = {Grounded Cognition},
  journaltitle = {Annual Review of Psychology},
  year         = {2008},
  volume       = {59},
  pages        = {617--645},
  url          = {http://www.annualreviews.org/doi/pdf/10.1146/annurev.psych.59.103006.093639},
  comment      = {Review of findings of psychological research favouring the grounded cognition model. This model (related to the embodied cognition model - possibly the latter is just a misnomer) proposes that cognition is founded on bodily state, modal simulations and situated action. Most work on this model propose that all the perceptual, motor, and introspective states resulting from an experience of a category are retained and then reactivated when representing that category. (izzy: related to active inference in the cortex Barrett2015).},
  keywords     = {ImageLearn, psychology},
  owner        = {ISargent},
  creationdate    = {2015.07.05},
}

@InProceedings{BartieMPD2014,
  author       = {Bartie, P. and Mackaness, W. and Petrenz, P. and Dickinson, A.},
  booktitle    = {Proceedings of RefNet Workshop on Psychological and Computational Models of Reference Comprehension and Production},
  date         = {31st August},
  title        = {Clustering landmark image annotations based on tag location and content},
  comment      = {From Wong2014: ``a web-based experiment identifying the features in a number of urban scenes which could be used in forming navigational instructions (Bartie et al., 2014). Participants tagged landmarks at object level online but this only focused on 2D static photographs.''},
  creationdate = {2014.10.30},
  keywords     = {navigation, RapidDC},
  owner        = {ISargent},
  year         = {2014},
}

@TechReport{Batty2001,
  author    = {Michael Batty and David Chapman and Steve Evans and Mordechai Haklay and Stefan Kueppers and Naru Shiode and Hudson Smith, Andy and Paul M. Torrens},
  title     = {Visualising the city: Communicating urban design to planners and decision-makers. Planning Support Systems, Models and Visualisation Tools},
  year      = {2001},
  pages     = {405--443},
  url       = {http://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/II-2-W1/129/2013/isprsannals-II-2-W1-129-2013.pdf},
  comment   = {Possibly this lead to virtual london. Lots of example sof 3D city models. Contains an alternative LOD system (LODs A-G).},
  keywords  = {3D buildings},
  owner     = {ISargent},
  creationdate = {2014.11.19},
}

@Article{Bednar2012,
  author       = {James A. Bednar},
  title        = {Building a mechanistic model of the development and function of the primary visual cortex},
  journaltitle = {Journal of Physiology-Paris},
  year         = {2012},
  volume       = {106},
  number       = {5--6},
  pages        = {194--211},
  url          = {http://www.sciencedirect.com/science/article/pii/S0928425712000022},
  comment      = {Paper reviewing various mthods for understanding the functionality within the primary visual cortex. use the Topographica simulator (http://ioam.github.io/topographica/) for computational modelling or neural maps. Reprints the orientation-sensitivity maps from shrews and monkeys. Good example of using computation modelling to building understanding of cortical processing and biological information processing in general. Topographica would be good to play with one day.},
  keywords     = {ImageLearn, neuroscience},
  owner        = {ISargent},
  creationdate    = {2015.07.08},
}

@InProceedings{BekinsA2005,
  author       = {Bekins, D and Aliaga, Daniel G},
  booktitle    = {Visualization},
  title        = {Build-by-number: rearranging the real world to visualize novel architectural spaces},
  pages        = {143-150},
  url          = {https://www.cs.purdue.edu/cgvlab/papers/aliaga/vis05.pdf},
  comment      = {Build-by-number is ``a technique for quickly designing architectural structures that can be rendered photorealistically at interactive rates. We combine image-based capturing and rendering with procedural modeling techniques to allow the creation of novel structures in the style of real-world structures''.},
  creationdate = {2014.10.03},
  keywords     = {3D},
  owner        = {ISargent},
  year         = {2005},
}

@InProceedings{BellingerSJ12,
  author       = {Colin Bellinger and Shiven Sharma and Nathalie Japkowicz},
  booktitle    = {11th International Conference on Machine Learning and Applications},
  title        = {One-Class versus Binary Classification: Which and When?},
  comment      = {Clearly written paper comparing one class classification (OCC) with binary classification. Considers the effect of unbalanced data sets (in which the key class is the greater proportion). Test a variety of classifiers on a range of artificial and benchmark datasets. ``We use the Autoassociator (AA) and the Probability Density Estimator (PDEN) for one-class classification. The binary classifiers use are: Multilayer Perception (MLP), Decision Trees (DTree), Support Vector Machines (SVM), Nearest Neighbour (IBK)''. One artificial data set has a single cluster of target class whereas the other has 2 clusters. Use an index of imbalance to show how the performance of these classifiers varies with index. The OCCs tend to be much more consistent whereas the binary classifiers tend to perform less well as imbalance increases (MLP performs very badly). Unfortunately, imbalance only goes one way, that is when the target class outnumbers the other class(es). I'd be interested in the case when the target class contains many fewer examples than the other class(es).},
  creationdate = {2013.12.13},
  keywords     = {one class classification, binary classification, balanced data},
  owner        = {ISargent},
  year         = {2012},
}

@InProceedings{Bengio2014,
  author       = {Joshua Bengio},
  booktitle    = {Machine Learning Summer School, Reykjavik},
  title        = {Deep Learning},
  url          = {http://www.iro.umontreal.ca/~bengioy/talks/mlss-reykjavik.pdf},
  comment      = {''Good features essential for successful ML: 90\% of effort'' - reason to learn rather than hard code features.},
  creationdate = {2017.01.16},
  owner        = {ISargent},
  year         = {2014},
}

@Article{Bengio09,
  author       = {Yoshua Bengio},
  journaltitle = {Foundations and Trends in Machine Learning},
  title        = {Learning Deep Architectures for {AI}},
  number       = {1},
  pages        = {1-127},
  url          = {http://www.iro.umontreal.ca/~bengioy/papers/ftml_book.pdf},
  volume       = {2},
  comment      = {''If a machine captured the factors that explain the statistical variations in the data, and how they interact to generate the kind of data we observe, we would be able to say that the machine understands those aspects of the world covered by these factors of variation'' ``The focus of deep architecture learning is to automatically discover such abstractions, fromthe lowest level features to the highest level concept.. Ideally, we would like learning algorithms that enable this discovery with as little human effort as possible, i.e., without having to manually define all necessary abstractions or having to provide a huge set of relevant hand-labeled examples. If these algorithms could tap into the huge resource of text and images on the web, it would certainly help to transfer much of human knowledge into machine-interpretable form.'' ``This locality issue is directly connected to the literature on the curse of dimensionality, but the results we cite show that what matters for generalization is not dimensionality, but instead the number of ``variations'' of the function we wish to obtain after learning.'' ``requirements we perceive for learning algorithms to solve AI. * Ability to learn complex, highly-varying functions, i.e., with a number of variationsmuch greater than the number of training examples. * Ability to learn with little human input the low-level, intermediate, and high-level abstractions that would be useful to represent the kind of complex functions needed for AI tasks. * Ability to learn from a very large set of examples: computation time for training should scale well with the number of examples, i.e. close to linearly. * Ability to learn from mostly unlabeled data, i.e. to work in the semi-supervised setting, where not all the examples come with the ``right'' associated labels. * Ability to exploit the synergies present across a large number of tasks, i.e. multi-task learning. These synergies exist because all the AI tasks provide different views on the same underlying reality. * In the limit of a large number of tasks and when future tasks are not known ahead of time, strong unsupervised learning (i.e. capturing the statistical structure in the observed data) is an important element of the solution.'' Argue for deeper architectures over shallow (old style neural network architechures: ``functions that can be compactly represented by a depth k architecture might require an exponential number of computational elements to be represented by a depth k-1 architecture. Since the number of computational elements one can afford depends on the number of training examples available to tune or select them, the consequences are not just computational but also statistical: poor generalization may be expected when using an insufficiently deep architecture for representing some functions.'' Good explanations of kolmagorov complexity, occam's razor, curse of dimensionality, boltzman machine, distributed representations, multi-clustering ``Deep architectures have not been studied much in the machine learning literature, because of the difficulty in optimizing them (Bengio et al., 2007). Notable exceptions include convolutional neural networks (LeCun et al., 1989; LeCun et al., 1998b; Simard \& Platt, 2003; Ranzato et al., 2007), and Sigmoidal Belief Networks using variational approximations (Dayan, Hinton, Neal, \& Zemel, 1995; Hinton, Dayan, Frey, \& Neal, 1995; Saul, Jaakkola \& Jordan, 1996; Titov \& Henderson, 2007), andmore recently Deep Belief Networks (Hinton et al., 2006; Bengio et al., 2007).'' ``Although deep neural networks were generally found too difficult to train well, there is one notable exception: convolutional neural networks. Convolutional nets were inspired by the visual system's structure, and in particular by the models of it proposed by Hubel and Wiesel (1962)...To this day, vision systems based on convolutional neural networks are among the best performing systems.'' ``Some of the deep architectures discussed below (Deep Belief Nets and stacked autoassociators) exploit as component or monitoring device a particular type of neural network: the autoassociator, also called autoencoder, or Diabolo network (Rumelhart et al., 1986a; Bourlard \& Kamp, 1988; Hinton \& Zemel, 1994; Schwenk \& Milgram, 1995; Japkowicz, Hanson, \& Gluck, 2000)...An autoassociator is trained to encode the input in some representation so that the input can be reconstructed from that representation. Hence the target output is the input itself.'' ``Another strategy, which was found very successful (Olshausen \& Field, 1997; Doi, Balcan, \& Lewicki, 2006; Ranzato et al., 2007; Ranzato \& LeCun, 2007; Ranzato, Boureau, \& LeCun, 2008), is based on a sparsity constraint on the code.'' ``PCA and most variants of ICA seem inappropriate because they generally do not make sense in the so-called overcomplete case, where the number of outputs of the layer is is greater than the number of inputs of the layer. This suggests looking in the direction of extensions of ICA to deal with the overcomplete case (Lewicki \& Sejnowski, 1998; Hinton, Welling, Teh, \& Osindero, 2001; Teh, Welling, Osindero, \& Hinton, 2003), as well as algorithms related to PCA and ICA, such as autoassociators and Restricted Boltzmann Machines, which can be applied in the overcomplete case'' ``It is conceivable that learning a second layer based on the same principle but taking as input the features learned with the first layer could extract slightly higher-level features. In this way, one could imagine that higher-level abstractions that characterize the input could emerge. Note how in this process all learning could remain local to each layer, therefore side-stepping the issue of gradient diffusion that might be hurting gradient-based learning of deep neural networks, when we try to optimize a single global criterion. This motivates the next section, where we formalize the concepts behind RBMs.'' ``Shaping and the use of a curriculum can also be seen as continuation methods. For this purpose, consider the learning problem of modeling the data coming from a training distribution P. The idea is to reweight the probability of sampling the examples from the distribution according to a given schedule, starting from the ``easiest'' examples and moving gradually towards examples illustrating more abstract concepts.'' ``According to learning theory (Vapnik, 1995; Li \& Vitanyi, 1997), to obtain good generalization it is enough that the total number of bits needed to encode the whole training set be small, compared to the size of the training set.'' ``Unknown future tasks: if a learning agent does not know what future learning tasks it will have to deal with in the future, but it knows that the task will be defined with respect to a world (i.e. random variables) that it can observe now, it would appear very rational to collect as much information as possible about this world so as to learn what makes it tick.'' Gives pseudocode. An exellent paper however I will need to understand far better boltzman, gibbs, markov, hinton, ... to understand the latter sections of this paper.},
  creationdate = {2013.07.22},
  keywords     = {Machine Learning, Representation Learning, Deep Learning, ImageLearn, DeepLEAP},
  owner        = {ISargent},
  year         = {2009},
}

@Article{BengioCV13,
  author       = {Yoshua Bengio and Aaron Courville and Pascal Vincent},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title        = {Representation Learning: A Review and New Perspectives},
  number       = {8},
  pages        = {1798-1828},
  url          = {http://arxiv.org/pdf/1206.5538v3.pdf},
  volume       = {35},
  comment      = {''This paper is about feature learning, or representation learning , i.e., learning transformations of the data that make it easier to extract useful information when building classifiers or other predictors'' ``deep architectures promote the re-use of features, and deep architectures can potentially lead to progressively more abstract features at higher layers of representations (more removed from the data) ``...vast quantities of unlabeled examples, to learn representations that separate the various explanatory sources. Doing so should give rise to a representation significantly more robust to the complex and richly structured variations extant in natural data sources for AI-related tasks.'' ``It is important to distinguish between the related but distinct goals of learning invariant features and learning to disentangle explanatory factors'' ``Suggest it is better to ``disentangle as mnay factors as possible, discarding as little informaton about the data as practical''

Representation learning allows the learning of non-task-specific priors. The objective is to recover or at least disentangle underlying factors of variation. The representations can be used for multi-task or transfer learning. Deep archtectures allow feature re-use and progressively more abstract features. Deep architectures may be either probabilistic graphical models, in which the hidden units may be considered latent random variables, or neural networks, in which the units describe a computational model.},
  creationdate = {2013.11.19},
  keywords     = {representation learning, ImDeepLEAP, DeepLEAP, MLStrat Milestones},
  owner        = {ISargent},
  year         = {2013},
}

@InProceedings{BengioLPL2007,
  author       = {Bengio, Yoshua and Pascal Lamblin and Dan Popovici and Larochelle, Hugo},
  booktitle    = {Advances in Neural Information Processing Systems 19},
  title        = {Greedy Layer-Wise Training of Deep Networks},
  editor       = {B. Sch\''{o}lkopf and J.C. Platt and T. Hoffman},
  pages        = {153--160},
  publisher    = {MIT Press},
  url          = {http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf},
  bdsk-url-1   = {http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf},
  creationdate = {2017.04.27},
  keywords     = {deep learning, unsupervised, MLStrat Milestones},
  owner        = {ISargent},
  year         = {2007},
}

@InProceedings{BengioLCW09,
  author       = {Yoshua Bengio and Jerome Louradour and Ronan Collobert and Jason Weston},
  booktitle    = {Proceedings of the 26th International Conference on Machine Learning},
  title        = {Curriculum Learning},
  abstract     = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them \curriculum learning''. In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
  creationdate = {2014.05.22},
  keywords     = {machine Learning, curriculum learning, ImageLearn},
  owner        = {ISargent},
  year         = {2009},
}

@Article{BennerGGHL2013,
  author       = {Benner, J. and Geiger, A. and Groger, G. and Hafele, K.-h. and Lowner, M.-o.},
  journaltitle = {ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences},
  title        = {Enhanced LoD Concepts for Virtual 3D City Models},
  pages        = {51-61},
  url          = {http://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/II-2-W1/51/2013/isprsannals-II-2-W1-51-2013.pdf},
  volume       = {II-2/W1},
  comment      = {''The Level of Detail concept of City GML is an established and frequently used tool for representing city objects with varying geometric and semantic complexity, supporting the scaling of city models with respect to the user's needs. Nevertheless, it has been shown that the current concept is deficient in relation to applications and verifiability. Main shortcomings are the lack of metadata, the missing distinction between interior and exterior representation and the arbitrary assignment of one LoD concept for almost all CityGML modules.'' This is the paper that shows 12 different legal variants of the LoD2 concept.},
  creationdate = {2014.10.30},
  keywords     = {3D buildings, CityGML},
  owner        = {ISargent},
  year         = {2013},
}

@Article{BergstraB2012,
  author       = {James Bergstra and Yoshua Bengio},
  journaltitle = {Journal of Machine Learning Research},
  title        = {Random Search for Hyper-Parameter Optimization},
  pages        = {281-305},
  url          = {http://jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf},
  volume       = {13},
  comment      = {Paper showing that hyperparameters can be optimised more efficiently and more effectively using random rather than manual or grid search. Optimatised against a verfication data set. ``Our analysis of the hyper-parameter response surface (\psy) suggests that random experiments are more efficient because not all hyperparameters are equally important to tune. Grid search experiments allocate too many trials to the exploration of dimensions that do not matter and suffer from poor coverage in dimensions that are important.''},
  creationdate = {2014.10.03},
  keywords     = {machine learning, hyperperamaters, ImageLearn},
  owner        = {ISargent},
  year         = {2012},
}

@InProceedings{BergstraBBK2011,
  Title                    = {Algorithms for Hyper-Parameter Optimization},
  Author                   = {James S. Bergstra and R\'{e}mi Bardenet and Yoshua Bengio and Bal\'{a}zs K\'{e}gl},
  Booktitle                = {Advances in Neural Information Processing Systems},
  Year                     = {2011},
  Editor                   = {J. Shawe-Taylor and R.S. Zemel and P.L. Bartlett and F. Pereira and K.Q. Weinberger},
  Volume                   = {24},

  Keywords                 = {ImageLearn},
  Owner                    = {ISargent},
  creationdate                = {2016.06.21}
}

@Article{BernknopfS2015,
  author       = {Richard Bernknopf and Carl Shapiro},
  journaltitle = {ISPRS International Journal of Geo-Information},
  title        = {Economic Assessment of the Use Value of Geospatial Information},
  doi          = {doi:10.3390/ijgi4031142},
  number       = {3},
  pages        = {1142--1165},
  url          = {http://www.mdpi.com/2220-9964/4/3/1142/pdf},
  volume       = {4},
  comment      = {Considers the benefit of GI given three examples: (1) a retrospective model about environmental regulation of agrochemicals; (2) a prospective model about the impact and mitigation of earthquakes in urban areas; and (3) a prospective model about developing private-public geospatial information for an ecosystem services market. Each example demonstrates the potential value of geospatial information in a decision with uncertain information.},
  keywords     = {DeepLEAP, geospatial},
  owner        = {ISargent},
  creationdate    = {2016.07.28},
  year         = {2015},
}

@InProceedings{BiljeckiLS2014,
  author       = {Biljecki, Filip and Ledoux, Hugo and Stoter, Jantien},
  booktitle    = {Proceedings of the 9th 3DGeoInfo Conference 2014},
  title        = {{Height references of CityGML LOD1 buildings and their influence on applications}},
  doi          = {http://doi.org/10.4233/uuid:09d030b5-67d3-467b-babb-5e5ec10f1b38},
  editor       = {Breunig, Martin and Mulhim, Al-Doori and Butwilowski, Edgar and Kuper, Paul Vincent and Benner, Joachim and H{\''a}fele, Karl-Heinz},
  address      = {Dubai, UAE},
  comment      = {Really valuable paper looking at the effect of different building heights on analysis using block models. includes useful references for different uses of block models. Relates block models to CityGML and INSPIRE, both of which seem to have several options for the location of the height measure. identifies 7 different heights which are a mix of real-world features (eave, top) and technical heights (third, half roof). Create a synthetic city with synthetic LOD3 models as 'ground truth' from which the various height block models area created. Find H3, which is one half the roof height, results in the least RMSE when calculating volumes. ``We have shown that, while LOD1 is the simplest volumetric form in CityGML, it is surrounded by potential ambiguities because its geometry may be modelled in many variants...we have shown with experiments the potentially drastic difference between the variants with respect to a GIS operation (volume computation)''},
  creationdate = {2015.11.09},
  month        = {11},
  owner        = {ISargent},
  year         = {2014},
}

@Article{BiljeckiLSZ2014,
  author       = {Biljecki, F. and Ledoux, H. and Stoter, J. and Zhao, J.},
  journaltitle = {Computers, Environment and Urban Systems},
  title        = {Formalisation of the level of detail in 3D city modelling},
  pages        = {1 - 15},
  volume       = {48},
  comment      = {From Elsivier website: ``Highlights * Formalisation of the concept of level of detail as the degree of its adherence to its corresponding subset of reality. *Every geo-dataset has a level of detail, which can be specified with 6 metrics. *Level of detail is a concept separated from quality. *The approach is of continuous nature and it enables an arbitrary number of discrete LODs. *The framework has been implemented in CityGML and 10 discrete LODs are made as an example of the realization'' From Wong2014: ``While much has been done in establishing industry-wide standards such as CityGML's level of detail (LoD) classification, the concept is incoherent and inconsistent between practitioners, standards and institutions''},
  creationdate = {2014.10.30},
  keywords     = {3D buildings, CityGML, 3DCharsPaper},
  owner        = {ISargent},
  year         = {2014},
}

@Book{COSTTU08012014,
  author       = {R. Billen and A.-F. Cutting-Decelle and O. Marina and de Almeida, J.-P. and M. Caglioni, G. Falquet and T. Leduc and C. M\'{e}tral and G. Moreau and J. Perret and G. Rabino and San Jose, R. and I. Yatskiv and S. Zlatanova},
  title        = {3D City Models and urban information: Current issues and perspectives. European COST Action TU0801},
  publisher    = {EDP Sciences},
  url          = {http://orbi.ulg.ac.be/handle/2268/162974},
  address      = {France},
  comment      = {Final report of the COST TU8001 action about 3D city models. Massive document, not read it all. Attempts to define terms such as 'semantic' because group comprised people from many disciplines. State-of-the-art also defined. Lots of understanding of different use cases. The working group 3 stuff is excellent - looking at utility, usability and usefulness of city models and asking why they are not used more. ``A clear understanding of the uses allowed by the models is needed in order
for a comprehensive decision to be made before producing 3D city models. More work should be done starting from potential applications and identifying the features (in terms of data and software) needed to provide the most significant added values.''},
  creationdate = {2015.11.10},
  keywords     = {3DCharsPaper, 3D usability},
  owner        = {ISargent},
  year         = {2014},
}

@Article{Bishop13,
  author       = {Christopher M Bishop},
  journaltitle = {Philosophical Transactions of the Royal Society},
  title        = {Model-based machine learning},
  volume       = {371},
  comment      = {A fantastic paper that turned round my understanding and view on Bayesian inference. Provides outline of current machine learning, comparing 'traditional' with Bayesian methods as an introduction to Infer.NET. ``The central idea of the model-based approach to machine learning is to create a custom bespoke model tailored specifically to each new application. ... In many traditional machine learning methods, the adaptive parameters of the model are assigned point values that are determine by using an optimization algorithm to minimize a suitable cost function. By contrast, in a Bayesian setting, unknown variables are described using probability distributions, and the observation of data allows these distributions to be updated through Bayes' theorem. ... process is intrinsically sequential and is therefore well suited to online learning. Parameter optimization, which is widely used in traditional machine learning, is replaced in the Bayesian setting by inference in which the distributions over quantities of interest are evaluated, conditioned on the observed data. ... Bayesian methods are at the most powerful when the supply of data is limited [statistically small in comparison to the model being considered]... Many of the new applications for machine learning arising from the data explosion are characterized by datasets that are computationally large but statistically small. ...The easiest way to understand the interpretation of the graph is to imagine generating synthetic data ancestral sampling from the graph. This is called the generative viewpoint ... A high proportion of the standard techniques used in traditional machine learning can be expressed as special cases f the graphical model framework, coupled with appropriate inference algorithms. ... In practise, there may be some uncertainty over the graph structure, for example, whether particular linkes should be presented or not, .. Running inference on the gated graph then gives posterior distributions over different structures, conditioned on the observed data. ...a probabilistic model defines a joint distribution over all the variables in our application. We can partition these variables into those that are observed, x, (the data), those whose values we wish to know, z, and the remaining latent variabels, w. ... The change in distribution in going from the prior to the posterior reflects the information gained as a result of observing the data, and represents the modern Bayesian perspective on what it means for a machine to 'learn from data'. ... We have exploited the factorization structure to exchange summation and multiplication and thereby achieve a form that is analytically equivalent but computationally more efficient. ... Thus, by using the factorization of the joint distribution, we have reduced the computation from one that is exponetial in the length of the chain to one that is linear in the length of the chain. ... One class of approximation scheme is based on sampling using techniques such as Markov chain Monte Carlo (MCMC). A very simple, though widely applicable, MCMC method is Gibbs sampling. ... In practise, however, Monte Carlo methods are computationally expensive, and typically do not scale to large datasets encountered in many technological applications, particularly those involving internet-scale datasets. ... The local messages are approximated through minimization of the Kullback-Leibler (KL) divergence ... inference and decision are interleaved, and the graphical model is being continually created. This is a far cry from the traditional machine learning paradigm in which the parameters of a model are tuned in the laboratory using a training dataset (with cross-validation to avoid overfitting), the parameters then frozen and the fixed system used to make predictions on the future test data. ... probabilistic programming has become a very active field of research.'' Topics also mentioned include: Probabilistic graphical models Directed acyclic graphs HMM forward-backward algorithm linear dynamical systems Kalman filter Autoregressive HMM Factorial HMM Switching state-space model Factor graphs Gates Message-passing scheme Junction tree algorithm Loopy belief propogation TrueSkill Traditional message passing Tree-weighted message passing Fractional belief propogation Expectation propogation Power EP Csoft Rejection sampling},
  creationdate = {2014.05.30},
  keywords     = {machine Learning, Bayesian inference, probabilistic programming, ImageLearn},
  owner        = {ISargent},
  year         = {2013},
}

@Article{Bowers2009,
  author       = {Jeffrey S Bowers},
  journaltitle = {Psychological Review},
  title        = {On the biological plausibility of grandmother cells: implications for neural network theories in psychology and neuroscience},
  number       = {1},
  pages        = {220-251},
  volume       = {116},
  comment      = {Article in favour of localist theory of cognition - that concept or grandmother cells exist},
  keywords     = {ImageLearn, neuroscience},
  owner        = {ISargent},
  creationdate    = {2015.07.15},
  year         = {2009},
}

@Article{BransonVWPB14,
  author       = {Steve Branson and Grant Van Horn and Catherine Wah and Pietro Perona and Serge Belongie},
  title        = {The Ignorant Led by the Blind: A Hybrid Human-Machine Vision System for Fine-Grained Categorization},
  journaltitle = {International Journal of Computer Vision},
  year         = {2014},
  month        = {2},
  abstract     = {We present a visual recognition system for fine-grained visual categorization. The system is composed of a human and a machine working together and combines the complementary strengths of computer vision algorithms and (non-expert) human users. The human users provide two heterogeneous forms of information object part clicks and answers to multiple choice questions. The machine intelligently selects the most informative question to pose to the user in order to identify the object class as quickly as possible. By leveraging computer vision and analyzing the user responses, the overall amount of human effort required, measured in seconds, is minimized. Our formalism shows how to incorporate many different types of computer vision algorithms into a human-in-the-loop framework, including standard multiclass methods, part-based methods, and localized multiclass and attribute methods. We explore our ideas by building a field guide for bird identification. The experimental results demonstrate the strength of combining ignorant humans with poor-sighted machines the hybrid system achieves quick and accurate bird identification on a dataset containing 200 bird species.},
  comment      = {This looks fascinating. An application that uses CV to generate the most intelligent question for the human such that a classification can be arrived at as soon as possible. Utilises the best aspects of machines and of humans.},
  keywords     = {machine learning},
  owner        = {ISargent},
  creationdate    = {2014.02.21},
}

@Article{Breiman2001,
  author       = {Leo Breiman},
  title        = {Statistical Modeling: The Two Cultures},
  journaltitle = {Statistical Science},
  year         = {2001},
  volume       = {16},
  number       = {3},
  pages        = {199--231},
  url          = {http://projecteuclid.org/download/pdf_1/euclid.ss/1009213726},
  abstract     = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated bya given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical communityhas been committed to the almost exclusive use of data models. This commit- ment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current prob- lems. Algorithmic modeling, both in theoryand practice, has developed rapidlyin fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move awayfrom exclusive dependence on data models and adopt a more diverse set of tools.},
  comment      = {Fascinating article extolling algorithmic modelling over data modelling - that is, using algorithms to model the 'black box' of nature rather than trying to statistically model the processes in the black box. One of the main reasons for this position is that the algorithmic method focuses on getting a good prediction. The data method focuses on getting a good model but if this model is wrong the conclusions may also be. Discusses the two cultures, Occam's Razor, The curse of dimensionality (Bellman), goodness-of-fit and residuals analysis, the multiplicity of good models (which Brieman refers to as Rashomon after the Japanese film). Set very much at a time when SVM were out-performing neural nets. In many ways this is a comment on the new (at the time) 'big data' paradigm that standard multivaraite statistics could not be used when so many variables are being used. Includes some interesting comments both in favour of and opposing this paper. Brieman's achievments include the development of random forests.},
  keywords     = {machine learning, data modelling, algorithms},
  owner        = {ISargent},
  creationdate    = {2014.08.05},
}

@InProceedings{Brenner2001,
  author       = {Brenner, C},
  booktitle    = {Photogrammetric Week 01},
  title        = {City Models - Automation in Research and Practice},
  pages        = {149--158},
  publisher    = {Wichmann},
  url          = {http://www.ifp.uni-stuttgart.de/publications/phowo01/Brenner.pdf},
  address      = {Hannover},
  comment      = {Discusses why methods for 3D building reconstruction aren't very successful especially given requirements of [mapping agencies]. This is largely because the parts that can be automated are [minimal] and the gain of the automation may be offset by the loss to checking of the the data. Suggests two approaches to automation, one by reconstructing from the ground plan alone and one by incorporating information from the DSM. ``Semiautomatic approaches have been reported for both image- and DSM-based systems. They can be divided into approaches which model buildings from a fixed set of volumetric primitives which are combined (e.g. G\''ulch et al. 1999, Brenner 1999) and approaches which build the topology of the surface directly (e.g. Gr\''un \& Wang 1998). `` ``There are numerous techniques to segment DSMs into meaningful regions, for example region growing, line grouping (Jiang \& Bunke 1994) or robust estimation techniques like RANSAC (Fischler \& Bolles 1981). `` As far as i know, this work remains relevant in 2011.},
  creationdate = {2014.10.28},
  keywords     = {3D buildings, 3D quality, 3DCharsPaper},
  owner        = {ISargent},
  year         = {2001},
}

@Misc{Briggs2012,
  author       = {William M. Briggs},
  title        = {It is Time to Stop Teaching Frequentism to Non-statisticians},
  howpublished = {arXiv},
  url          = {http://arxiv.org/pdf/1201.2590.pdf},
  comment      = {Excellent and very short paper saying why confidence intervals, P-values etc are not good because they are not understood and easy to cheat with. Talks about the great Bayesian Switch. ``Equipped only with their common sense and ignorant of fine distinctions of philosophy we statisticians spend years assimilating, civilians before they come to us think like Bayesians. We do our best, by repetition and by rote, to beat this out of them...''},
  creationdate = {2014.10.03},
  keywords     = {statistics},
  month        = {1},
  owner        = {ISargent},
  year         = {2012},
}

@Book{Burke1757,
  author    = {Edmund Burke},
  title     = {A Philosophical Inquiry into the Origin of Our Ideas of The Sublime and Beautiful With Several Other Additions},
  url       = {http://www.bartleby.com/24/2/},
  comment   = {A treatise on aesthetics. Discussed in Gooley2012 (p274) 'Burke found that fitness, perfection, virtue and proportion all fail to account for the beauty in plants, animals or humans'.},
  keywords  = {landscape, scene analysis, RapidDC, MLStrat},
  owner     = {ISargent},
  creationdate = {2014.06.25},
  year      = {1757},
}

@Article{Burl1998,
  author       = {Burl, MichaelC. and Asker, Lars and Smyth, Padhraic and Fayyad, Usama and Perona, Pietro and Crumpler, Larry and Aubele, Jayne},
  title        = {Learning to Recognize Volcanoes on Venus},
  doi          = {10.1023/A:1007400206189},
  issn         = {0885-6125},
  language     = {English},
  number       = {2-3},
  pages        = {165-194},
  url          = {http://dx.doi.org/10.1023/A%3A1007400206189},
  volume       = {30},
  comment      = {Domain versus ML expertise: ``We show how the development of such a system requires a completely different set of skills than are required for applying machine learning to “toy world” domains. This paper discusses important aspects of the application process not commonly encountered in the “toy world,” including obtaining labeled training data, the difficulties of working with pixel data, and the automatic extraction of higher-level features.'' Also gives evidence that it is the extraction of good features that is more important than the choice of classifier (or something like that).},
  creationdate = {2017.04.05},
  journal      = {Machine Learning},
  keywords     = {ImageLearn, domain expertise},
  owner        = {ISargent},
  publisher    = {Kluwer Academic Publishers},
  year         = {1998},
}

@Article{Cadieu2014,
  author       = {Charles F. Cadieu and Ha Hong and Daniel L. K. Yamins and Nicolas Pinto and Diego Ardila and Ethan A. Solomon and Najib J. Majaj and James J. DiCarlo},
  title        = {Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition},
  url          = {http://arxiv.org/pdf/1406.3284v1.pdf},
  abstract     = {The primate visual system achieves remarkable visual object recognition performance even in brief presentations and under changes to object exemplar, geometric transformations, and background variation (a.k.a. core visual object recognition). This remarkable performance is mediated by the representation formed in inferior temporal (IT) cortex. In parallel, recent advances in machine learning have led to ever higher performing models of object recognition using artificial deep neural networks (DNNs). It remains unclear, however, whether the representational performance of DNNs rivals that of the brain. To accurately produce such a comparison, a major difficulty has been a unifying metric that accounts for experimental limitations such as the amount of noise, the number of neural recording sites, and the number trials, and computational limitations such as the complexity of the decoding classifier and the number of classifier training examples. In this work we perform a direct comparison that corrects for these experimental limitations and computational considerations. As part of our methodology, we propose an extension of ``kernel analysis'' that measures the generalization accuracy as a function of representational complexity. Our evaluations show that, unlike previous bio-inspired models, the latest DNNs rival the representational performance of IT cortex on this visual object recognition task. Furthermore, we show that models that perform well on measures of representational performance also perform well on measures of representational similarity to IT and on measures of predicting individual IT multi-unit responses. Whether these DNNs rely on computational mechanisms similar to the primate visual system is yet to be determined, but, unlike all previous bio-inspired models, that possibility cannot be ruled out merely on representational performance grounds},
  comment      = {paper with objects imposed on natural scenes and these strange images are tested against human, monkey and different AI methods of classification. Shows deep learning very powerful.},
  creationdate = {2014.09.09},
  owner        = {ISargent},
  year         = {2014},
}

@InProceedings{Candes06,
  author       = {Emamnuel J. Cand\`{e}s},
  booktitle    = {Proceedings of the International Congress of Mathematicians},
  title        = {Compressive sampling},
  address      = {Madrid, Spain},
  comment      = {''because most signals are compressible, why spend so much effort acquiring all the data when we know that most of it will be discarded? Wouldn't it be possible to acquire the data in already compressed form so that one does not need to throw away anything? ``Compressive sampling'' also known as ``compressed sensing'' [20] shows that this is indeed possible.''},
  creationdate = {2014.01.28},
  owner        = {ISargent},
  year         = {2006},
}

@InProceedings{CarreiraPerpinanH2005,
  author    = {Carreira-Perpinan, Miguel A. and Hinton, Geoffrey E.},
  title     = {On Contrastive Divergence Learning},
  editor    = {Intelligence, Artificial and {Statistics, 2005}, Barbados},
  biburl    = {http://www.bibsonomy.org/bibtex/287686aa19cf8339d926bafc7860d78b4/prlz77},
  keywords  = {Contrastive Divergence, deep Learning},
  owner     = {ISargent},
  creationdate = {2017.04.27},
  year      = {2005},
}

@Article{CastelluccioPSV2015,
  author       = {Marco Castelluccio and Giovanni Poggi and Carlo Sansone and Luisa Verdoliva},
  journaltitle = {CoRR},
  title        = {Land Use Classification in Remote Sensing Images by Convolutional Neural Networks},
  url          = {http://arxiv.org/abs/1508.00092},
  volume       = {abs/1508.00092},
  bibsource    = {dblp computer science bibliography, http://dblp.org},
  biburl       = {http://dblp.uni-trier.de/rec/bib/journals/corr/CastelluccioPSV15},
  comment      = {Test GoogLeNet and Caffe (CaffeNet) using the UC-Merced (USGS set with 21 land-use classes) and the Brazilian Coffee Scenes (SPOT imagery with 3 classes - coffee, non-coffee and mixed) data sets. A nicely summarised overview of hand-coded feature methods and their application to imagery. A comment on spatial scale: ``Neurons...in deeper layers view (indirectly) larger portions of the image''. Consider that ``optical remote sensing images have strong low-level similarities with general-purpose optical images''. Consider 3 options for applying CNNs/ConvNets to remotely sensed imagery: training from scratch, fine tuning and feature vector. Training from scratch does not perform very well with the UC-Merced data set, possibly because there are not enough examples in the data set. It is the best method with the Brazilliant Coffee Scenes data set - their explanation is that this is probably due to the large number of samples in each class. I would also comment that the nature of these images is dissimilar to the 3-band images that the networks were training on. Fine tuning involves taking a trained data set, freezing the lower layers (those that are deemed to be generic to all optical image domains) and continuing to train the rest of the network with a final softmax layer for classification. This method works best for the UC_Merced data set. The feature vector method simply takes the trained network and replaces the final layer with a softmax that will fit the data. This method does moderately well for the UC_Merced data but not the Brazilian Coffee Scene data.},
  creationdate = {2016.05.11},
  keywords     = {ImageLearn, Spatial Scale, Remote Sensing, DeepLEAP, MLStrat DLRS},
  year         = {2015},
}

@InProceedings{ChenRH2016,
  author    = {L. Chen and F. Rottensteiner and C. Heipke},
  booktitle = {ISPRS XXIII CONGRESS},
  date      = {12-19 July},
  title     = {Invariant Descriptor Learning Using a Siamese Convolutional Neural Network},
  doi       = {https://doi.org/10.5194/isprsannals-III-3-11-2016},
  url       = {https://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/III-3/11/2016/isprs-annals-III-3-11-2016.pdf},
  comment   = {Trained network to identify matching or non-matching image pairs. Application of this would be real image matching or image retrieval. (2021 thought: or even pre-trained network for other applications). Similar, but predates, SimCLR ChenKNH2020.},
  keywords  = {representaiton learning, MLStrat Training},
  owner     = {ISargent},
  creationdate = {2016.07.05},
  year      = {2016},
}

@Article{ChenLZWG2014,
  author       = {Y. Chen and Z. Lin and X. Zhao and G. Wang and Y. Gu},
  journaltitle = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  title        = {Deep Learning-Based Classification of Hyperspectral Data},
  doi          = {10.1109/JSTARS.2014.2329330},
  issn         = {1939-1404},
  number       = {6},
  pages        = {2094-2107},
  volume       = {7},
  comment      = {''We then propose a novel deep learning framework to merge [stacked autoencoders with a new way of classifying with spatial-dominated information], from which we can get the highest classification accuracy. The framework is a hybrid of principle component analysis (PCA), deep learning architecture, and logistic regression.'' Compress the spectral dimension using PCA and then extract neighbourhood regions around each pixel that are then flattened for forward propogation through the stacked autoencoder. Interesting that they use patches with a AE rather than CNN.},
  creationdate = {2016.05.11},
  keywords     = {ImageLearn, Spatial Scale, Hyperspectral, RemoteDeepLEAP, DeepLEAP},
  month        = {6},
  owner        = {ISargent},
  year         = {2014},
}

@Article{ChengHL2017,
  author       = {Gong Cheng and Junwei Han and Xiaoqiang Lu},
  title        = {Remote Sensing Image Scene Classification: Benchmark and State of the Art},
  url          = {http://arxiv.org/abs/1703.00121},
  volume       = {abs/1703.00121},
  abstract     = {Remote sensing image scene classification plays an important role in a wide range of applications and hence has been receiving remarkable attention. During the past years, significant efforts have been made to develop various datasets or present a variety of approaches for scene classification from remote sensing images. However, a systematic review of the literature concerning datasets and methods for scene classification is still lacking. In addition, almost all existing datasets have a number of limitations, including the small scale of scene classes and the image numbers, the lack of image variations and diversity, and the saturation of accuracy. These limitations severely limit the development of new approaches especially deep learning-based methods. This paper first provides a comprehensive review of the recent progress. Then, we propose a large-scale dataset, termed ``NWPU-RESISC45'', which is a publicly available benchmark for REmote Sensing Image Scene Classification (RESISC), created by Northwestern Polytechnical University (NWPU). This dataset contains 31,500 images, covering 45 scene classes with 700 images in each class. The proposed NWPU-RESISC45 (i) is large-scale on the scene classes and the total image number, (ii) holds big variations in translation, spatial resolution, viewpoint, object pose, illumination, background, and occlusion, and (iii) has high within-class diversity and between-class similarity. The creation of this dataset will enable the community to develop and evaluate various data-driven algorithms. Finally, several representative methods are evaluated using the proposed dataset and the results are reported as a useful baseline for future research.},
  bibsource    = {dblp computer science bibliography, http://dblp.org},
  biburl       = {http://dblp.uni-trier.de/rec/bib/journals/corr/ChengHL17},
  comment      = {a good paper giving a review of classification of remote sensing data - read properly!},
  creationdate = {2017.07.05},
  journal      = {CoRR},
  keywords     = {datasets, MLStrat Data},
  owner        = {ISargent},
  year         = {2017},
}

@Misc{Chollet2015,
  Title                    = {Keras},

  Author                   = {Chollet, Fran\c{c}ois and others},
  HowPublished             = {\url{https://github.com/fchollet/keras}},
  Year                     = {2015},

  Keywords                 = {ImageLearn},
  Owner                    = {ISargent},
  Publisher                = {GitHub},
  creationdate                = {2017.05.30}
}

@InProceedings{Christmas13,
  author       = {Jacqueline Christmas},
  booktitle    = {2013 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING},
  title        = {The Effect Of Missing Data On Robust Bayesian Spectral Analysis},
  address      = {SOUTHAMPTON, UK},
  comment      = {University of Exeter. Actual work is on predicting sea state within a 3-4 second window (for use in optimising wave power generation). Previous paper introduced robust bayesian spectral analysis and this paper looks at the effect of missing data on the model. Uses precipitation data because wave data not available. Finds that if missing data are in a block, up to 50\% of the data can be missing. Much less is data are randomly missing.},
  creationdate = {2013.09.30},
  keywords     = {Machine Learning, Bayesian Methods},
  month        = {9},
  owner        = {ISargent},
  year         = {2013},
}

@Article{CimpoiMKV2016,
  author       = {Mircea Cimpoi and Subhransu Maji and Iasonas Kokkinos and Andrea Vedaldi},
  journaltitle = {International Journal of Computer Vision},
  title        = {Deep Filter Banks for Texture Recognition, Description, and Segmentation},
  url          = {http://link.springer.com/article/10.1007%2Fs11263-015-0872-3},
  comment      = {''propose a human-interpretable vocabulary of texture attributes to describe common texture patterns, complemented by a new describable texture dataset for benchmarking''. Vocabularly includes honeycombed, polkadotted, bubbly, knitted. Dataset is call the describable textures dataset (DTD). Useful review of hand-crafted local descriptors. ``bag-of-visual-words and the Fisher vectors ... show that these have excellent efficiency and generalization properties if the convolutional layers of a deep model are used as filter banks''. ``While texture representations were extensively used in most areas of image understanding, since the breakthrough work of Krizhevsky et al. (2012) they have been replaced by deep Convolutional Neural Networks (CNNs)'' ``Our work is motivated by that of Rao and Lohse (1996) and Bhushan et al. (1997). Their experiments suggest that there is a strong correlation between the structure of the lexical space and perceptual properties of texture. While they studied the psychological aspects of texture perception, the focus of this paper is the challenge of estimating such properties from images automatically. Their work Bhushan et al. (1997), in particular, identified a set of words sufficient to describe a wide variety of texture patterns; the same set of words was used to bootstrap DTD.'' Evaluate texture representations on texture recognition, and object and scene recognition. Look into the effect of transferring pretrained network application from one domain to another. Results indicate that convolutional features learned are more generic to different data sets than the features in the fully connected layers. ``The main finding is that orderless pooling of convolutional neural network features is a remarkably good texture descriptor''.},
  creationdate = {2016.04.26},
  keywords     = {texture measures, convolutional neural networks, ImageLearn},
  owner        = {ISargent},
  year         = {2016},
}

@Article{Ciresan2012,
  author       = {Dan Cire\c{s}an and Ueli Meier and Jonathan Masci and J\''{u}rgen Schmidhuber},
  journaltitle = {Neural Networks},
  title        = {Multi-Column Deep Neural Network for Traffic Sign Classification},
  url          = {http://www.idsia.ch/~juergen/nn2012traffic.pdf},
  comment      = {Road sign recognition},
  creationdate = {2014.09.09},
  owner        = {ISargent},
  year         = {2012},
}

@InProceedings{CiresanGGS2012,
  author       = {Ciresan, Dan Claudiu and Alessandro Giusti and Gambardella, Luca Maria and J{\''u}rgen Schmidhuber},
  booktitle    = {NIPS},
  title        = {Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images},
  pages        = {2852--2860},
  creationdate = {2017.03.09},
  owner        = {ISargent},
  year         = {2012},
}

@Book{ClarkDF2004,
  author       = {Clark, Jo and Darlington, John and Fairclough, Graham},
  title        = {Using Historic Landscape Characterisation: English Heritage's review of HLC Applications 2002 - 03},
  isbn         = {1 8 99907 77 7},
  publisher    = {English Heritage},
  comment      = {Main reference for Historic England's History Landscape Characterisation. From Rouse2008: ``HLC works at a landscape scale. It recognises that the notion of present day landscape is a human construction. The fabric of the land that individuals and groups use to create their own notion of landscape is the product of thousands of years of human activity, although what remains to be seen today may be very recent, and has undergone successive periods of change and modification. Landscape, therefore, can only be understood if its dynamic nature is taken into account.'' From Langlands2015a: [Historic Landscape Characterisation] has become the standard means by which we break rural and urban areas down into contiguous blocks of broadly homogeneous development, primarily for the purposes of planning and conservation ...HCL work adopts the aerial perspective on 'character' which results in abstractions of space that can do little to relate to the human experience of character''.},
  creationdate = {2017.04.04},
  keywords     = {Landscape, Characterisation, ImageLearn},
  owner        = {ISargent},
  year         = {2004},
}

@InProceedings{CoatesHN11,
  author    = {Coates, Adam and Lee, Honglak and Ng, Andrew Y.},
  title     = {An analysis of single-layer networks in unsupervised feature learning},
  booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year      = {2011},
  url       = {http://ai.stanford.edu/~ang/papers/nipsdlufl10-AnalysisSingleLayerUnsupervisedFeatureLearning.pdf},
  comment   = {Create single-layer networks using different algorithms: Sparse auto-encoder, Sparse RBM and K-means.},
  keywords  = {Machine Learning, Representation learning, ImageLearn, RoofShape},
  owner     = {ISargent},
  creationdate = {2013.12.19},
}

@InBook{CoatesN2012,
  author       = {Adam Coates and Andrew Y. Ng},
  title        = {In Neural Networks: Tricks of the Trade},
  chapter      = {Learning Feature Representations with K-means},
  editor       = {G. Montavon, G. B. and Orr, K.-R.},
  publisher    = {Springer LNCS 7700},
  url          = {http://www.cs.stanford.edu/~acoates/papers/coatesng_nntot2012.pdf},
  comment      = {excellent reference paper for using k-means in general and especially with learning representations from image patches. Implements spherical k-means and discussing preprocessing including normalisation and whitening as well as initialisaton (recommends randomly initialising from a normal distribution and then normalising to bring centroids back to the sphere), damped updates, . Notes that k-means needs very large data sets to get results from high dimensional data, compared to sparse coding. K-means is much faster than other methods and notes that the choice of method would therefore depend on the dimensionality of the data: ``For modest dimensionalities (e.g., hundreds of inputs), this tradeoff can be advantageous because the additional data requirements do not outweigh the very large constant-factor speedup that is gained by training with K-means. For very high dimensionalities, however, it may well be the case that another algorithm like sparse coding works better or even faster''. A useful list of recommendations at the end:
''1. Mean and contrast normalize inputs.
2. Use whitening to ``sphere'' the data, taking care to set the \epsilon parameter appropriately. If whitening cannot be performed due to input dimensionality, one should split up the input variables.
3. Initialize K-means centroids randomly from Gaussian noise and normalize.
4. Use damped updates to help avoid empty clusters and improve stability.
5. Be mindful of the impact of dimensionality and sparsity on K-means. K-means tends to find sparse projections of the input data by seeking out ``heavy-tailed'' directions. Yet when the data is not properly whitened, the input dimensionality is very high, or there is insuffcient data, it may perform poorly.
6. With higher dimensionalities, K-means will require significantly increased amounts of data, possibly negating its speed advantage.
7. Exogenous parameters in the system (pooling, encoding methods, etc.) can have a bigger impact on final performance than the learning algorithm itself. Consider spending compute resources on more cross-validation for parameters before concluding that a more expensive learning scheme is required.
8. Using more centroids almost always helps when using the image recognition pipeline described in this chapter, provided we have enough training data. Indeed, whenever more compute resources become available, this is the first thing to try.
9. When labeled data is abundant, find a cheap encoder and let a supervised learning system do most of the work. If labeled data is limited (e.g., hundreds of examples per class), an expensive encoder may work better.
10. Use local receptive fields wherever possible. Input data dimensionality is the main bottleneck to the success of K-means and should be kept as low as possible. If local receptive fields cannot be chosen by hand, try an automated dependency test to help cut up your data into (overlapping) groups of inputs with lower dimensionality. This is likely a necessity for deep networks!''},
  creationdate = {2015.06.29},
  keywords     = {ImageLearn, RoofShape},
  owner        = {ISargent},
  year         = {2012},
}

@Article{Cohen1994,
  author       = {Jacob Cohen},
  journaltitle = {American Psychologist},
  title        = {The earth is round (p < .05)},
  pages        = {997--1003},
  comment      = {Paper cited by Nate Silver as one of many rejecting Fisherian (frequentist) statistics. Cohen recommends researchers report effect sizes in the form of confidence limits. ``...we have a considerably arrange of statistical techniques ... but they must be used sensibly ... Even null hypothesis testing and power analysis can be useful if we abandon the rejection of point nil hypothesis and use instead ``good-enough'' range null hypothesis... we can also find use for likelihood ratios and Bayesian methods.''},
  creationdate = {2014.10.03},
  keywords     = {statistics},
  owner        = {ISargent},
  year         = {1994},
}

@Article{CorcoranW2006,
  author       = {Padraig Corcoran and A Winstanley},
  title        = {Using texture to tackle the problem of scale in land-cover classification},
  pages        = {113--132},
  url          = {http://www.isprs.org/proceedings/XXXVI/4-C42/Papers/02_Potential%20and%20problems%20of%20multiscale%20representation/OBIA2006_Corcoran_Winstanley.pdf},
  abstract     = {This paper discusses the problem of scale in current approaches to Object Based Image Analysis (OBIA), and proposes how it may be overcome using theories of texture. It is obvious that aerial images contain land-cover that is textured and thus any features used to derive a land-cover classification must model texture information and as well as intensity. Previous research in the area of OBIA has attempted to derive land-cover classification using intensity features only, ignoring the presence of texture. This has led to a number of issues with the current theory of OBIA. Using only intensity it is impossible to perform segmentation of textured land-cover. In an attempt to tackle this problem it has become practice in OBIA to run segmentation at a number of different scales in the hope that each textured region will appear correctly segmented at some scale. This process of performing segmentation at multiple scales is not in line with current theories of visual perception. Julesz (Julesz 1983) states that when we view an object our aperture is adjusted to view that object in its true form. Also in theories of visual object recognition each object or feature is represented only once in its true form. The result of integrating segmentation at multiple scales is the generation of a land-cover hierarchy in a bottom-up manner but this is not how our visual system generates such hierarchies. This process in the visual system is conversely very top-down, with the aggregation of objects not only being driven by their relative intensity or texture features but also our knowledge, desires and expectations. Quantitative evaluation is also made increasingly difficult due to the lack of ground truth for each scale; it is impossible to predict the appropriate appearance of ground truth at each scale. Given the fact that each land-cover is represented at a number of scales, the number of context relationships between objects which must be managed is exponentially large. This makes the task of deriving land-use from land-cover increasingly difficult. If a robust set of intensity and texture features can be extracted and integrated correctly it would be possible to represent each land-cover in its true form within the one segmentation. Using a non-linear diffusion process and a geostatistical feature extraction algorithm we extract a set of intensity and texture feature respectively. Theses features are then integrated in such a manner to perform discriminate land-cover based on intensity where possible and texture where not. The motivation being that intensity features do not suffer from the uncertainty principle unlike texture thus giving more accurate boundary localization},
  comment      = {OBIA - object based image analysis. Claim that previous methods have only used intensity values. This proposes using texture as well. ``For texture feature extraction a robust estimate of spatial autocorrelation or variogram known as the mean square-root pair difference (SRPD) is used (Cressie and Hawkins 1980) ``. Boundaries are then identified by integrated intensity and texture to define 'objects'.},
  creationdate = {2016.11.22},
  journal      = {Object-based image analysis},
  keywords     = {DeepLEAP},
  owner        = {ISargent},
  year         = {2008},
}

@Book{Crawford1953,
  author       = {Crawford, O. G. S.},
  title        = {Archaeology in the field},
  publisher    = {Frederick A. Praeger},
  comment      = {From Lucas2012 ``Crawford...was the first to use the concept of palimpsest about the landscape in a systematic way''},
  creationdate = {2015.07.30},
  keywords     = {landscape, ImageLearn},
  owner        = {ISargent},
  year         = {1953},
}

@Article{Cross1988,
  author    = {A. M. Cross},
  title     = {Detection of circular geological features using the Hough transform},
  doi       = {10.1080/01431168808954956},
  eprint    = {http://dx.doi.org/10.1080/01431168808954956},
  number    = {9},
  pages     = {1519-1528},
  url       = {http://dx.doi.org/10.1080/01431168808954956},
  volume    = {9},
  comment   = {reference for Hough transform in remote sensing},
  journal   = {International Journal of Remote Sensing},
  keywords  = {ImageLearn, Feature Extraction},
  owner     = {ISargent},
  creationdate = {2017.05.29},
  year      = {1988},
}

@InProceedings{CsurkaDFWB2004,
  author    = {Gabriella Csurka and Christopher R. Dance and Lixin Fan and Jutta Willamowski and C{'{e}}dric Bray},
  title     = {Visual categorization with bags of keypoints},
  booktitle = {In Workshop on Statistical Learning in Computer Vision, ECCV},
  year      = {2004},
  pages     = {1--22},
  comment   = {Extracts a load of features (Harris, SIFT) and uses a bag-of-words approach to image categorisation.},
  owner     = {ISargent},
  creationdate = {2017.04.13},
}

@InProceedings{DalalT2005,
  author    = {Navneet Dalal and Bill Triggs},
  title     = {Histograms of Oriented Gradients for Human Detection},
  booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2005},
  editor    = {C. Schmid and S. Soatto and C. Tomas},
  volume    = {1},
  month     = {6},
  pages     = {886-893},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1467360},
  address   = {San Diego, USA},
  comment   = {key paper for HOG. Used for detecting people in images.},
  owner     = {ISargent},
  creationdate = {2015.06.12},
}

@Article{Dao11,
  author       = {Huyen Tue Dao},
  title        = {Getting to Know Machine Learning},
  journaltitle = {UX Magazine},
  year         = {2011},
  date        = {2011-12-14},
  pages        = {Article No. 773},
  url          = {http://uxmag.com/articles/getting-to-know-machine-learning},
  comment      = {Getting to Know Machine Learning},
  owner        = {ISargent},
  creationdate    = {2013.10.25},
}

@InProceedings{Dasgupta00,
  author    = {Sanjoy Dasgupta},
  title     = {Experiments with Random Projection},
  booktitle = {Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence (UAI'00)},
  year      = {2000},
  editor    = {Craig Boutilier and MoisÃƒÂ©s Goldszmidt},
  publisher = {Morgan Kaufmann Publishers Inc., San Francisco, CA, USA},
  pages     = {143-151},
  url       = {http://cseweb.ucsd.edu/users/dasgupta/papers/randomf.pdf},
  comment   = {Article about using random projections for mixture of gaussians. Discusses use w.r.t. pca. Well written. Seems that any random projection can maintain separation between clusters...?},
  keywords  = {Machine learning},
  owner     = {ISargent},
  creationdate = {2014.05.15},
}

@InProceedings{Datta2016,
  author    = {Megha Datta},
  title     = {Impact of Geospatial Information on Economy and Society},
  booktitle = {Geospatial World Forum},
  year      = {2016},
  month     = {5},
  comment   = {Slides show the economic impact created by geospatial information on national economies of Australia, New Zealand, USA, Canada, Ireland, England and Wales.},
  owner     = {ISargent},
  creationdate = {2016.09.21},
}

@Article{DaviesHGHD2009,
  Title                    = {User needs and implications for modelling vague named places},
  Author                   = {Davies, C. and Holt, I. and Green, J. and Harding, J. and Diamond, L.},
  Year                     = {2009},
  Number                   = {3},
  Pages                    = {174--94},
  Volume                   = {9},

  Journaltitle             = {Spatial Cognition \& Computation},
  Keywords                 = {3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2015.11.11}
}

@Article{DaviesTDGC2006,
  author       = {C. Davies and W. Tompkinson and N. Donnelly and L. Gordon and K. Cave},
  journaltitle = {Computers in Human Behavior},
  title        = {Visual saliency as an aid to updating digital maps},
  number       = {4},
  pages        = {672--684},
  volume       = {22},
  comment      = {Find submitted copy in \\os2k17\Research\ResearchProjects\MachineLearning\SubProjects\ImageLearn\Documents\Davies_etal_finalsubmission.docx. Uses model of Itti and Koch to define visual saliency and works with assumption that attention on an image is result of bottom-up low-level visually salient features. Also discusses role of top-down processes by which prior knowledge influence attention. Test this using basic eye tracking and compare group of expert aerial photogrammetrists with a group of novices. ``It may be that experts are better able to 'turn on and off' the deeper semantic interpretation, depending on the task at hand, and this may help them to deal more efficiently with each new image'' ``The results suggest, unexpectedly, that experience with aerial imagery leads experts to be more responsive to visual saliency than novices. One possible explanation for this is that remotely sensed images are complex. ``},
  creationdate = {2014.11.20},
  keywords     = {image saliency, ImageLearn, DeepLEAP1},
  owner        = {ISargent},
  year         = {2006},
}

@Book{DayanA05,
  author       = {Peter Dayan and Abbott, L. F.},
  title        = {Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems},
  publisher    = {MIT Press},
  url          = {http://cns-classes.bu.edu/cn510/Papers/Theoretical%20Neuroscience%20Computational%20and%20Mathematical%20Modeling%20of%20Neural%20Systems%20-%20%20Peter%20Dayan,%20L.%20F.%20Abbott.pdf},
  comment      = {Course text for Computational Neuroscience course on Coursera},
  creationdate = {2014.03.18},
  keywords     = {Neuroscience},
  owner        = {ISargent},
  year         = {2005},
}

@InProceedings{DeCubberV2010,
  author       = {De Cubber, Ine and Van Orshoven, Jos},
  booktitle    = {5th International Conference on 3D GeoInformation},
  title        = {{3D}-capabilities required by users of the 2D-large scale topographic reference database in {F}landers, {B}elgium},
  editor       = {Thomas H. Kolbe and Gerhard K\''{o}nig and Claus Nagel},
  address      = {Berlin, Germany},
  comment      = {Surveys and workshops current users of 2D data to understand who they are, what they do and if 3D would improve what they do. Indication are that existing applications would benefore from 3D data. visualisation would be the a use common to different applications but other requirements tend to be more application-specific. It would not be possible to design thedata model to suit all uses so suggest an interative approach to developing 3D data, cycling through the requirements, the data models, the implented large-scale topographic database and the applications. Data model would start by focussing on the common requirement, visualisation, and with each iterations be increasing detailed or extended. Gives a table showing what object clasess (buildings, trees, road infrastructure, water/river, terrain modela dn cables/pipelines) are important to each application. I have PDF.},
  creationdate = {2015.11.10},
  keywords     = {3DCharsPaper},
  month        = {11},
  owner        = {ISargent},
  year         = {2010},
}

@InProceedings{DeMaziereV13,
  author       = {De Mazi\`{e}re, Patrick and Van Hulle, Marc M.},
  booktitle    = {2013 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING},
  title        = {Inter-Document Reference Detection As An Alternative To Full Text Semantic Analysis In Document Clustering},
  address      = {SOUTHAMPTON, UK},
  comment      = {''As an alternative to a semantic analysis, we searched for inter-document references or direct references. Direct references are defined as terms that explicitly refer to other documents present in the inventory. We show that the grouping based on references is largely similar to the one based on semantics, but with considerably less computational efforts. `` Linking documents in an inventory of 6,919 useful documents and over 330,000 pages. Full text semantic search method took around 6 months for whole inventory whereas search for direct references took only an hour. At poster, author said the reason for not continuing this work is that it is ``too technical and we are scientists''. The work has been transfered to a commercial organisation to continue.},
  creationdate = {2013.09.30},
  keywords     = {Machine Learning, Natural Language Processing},
  month        = {9},
  owner        = {ISargent},
  year         = {2013},
}

@Article{DeSouzaRH12,
  author       = {De Souza, M A and S Robson and J C Hebden},
  title        = {A photogrammetric technique for acquiring accurate head surfaces of newborn infants for optical tomography under clinical conditions},
  journaltitle = {The Photogrammetric Record},
  year         = {2012},
  volume       = {27},
  number       = {139},
  pages        = {253-271},
  comment      = {measuring babies heads in NNU using different methods},
  keywords     = {3D, modelling},
  owner        = {Izzy},
  creationdate    = {2013.04.05},
}

@Misc{DeepLearningTutorial,
  author           = {{Deep Learning Tutorial}},
  title            = {Convolutional Neural Networks (LeNet)},
  url              = {http://deeplearning.net/tutorial/lenet.html},
  urldate          = {2016.05.11},
  comment          = {''stacking many such layers leads to (non-linear) ``filters'' that become increasingly ``global'' (i.e. responsive to a larger region of pixel space).''},
  creationdate     = {2016.05.11},
  keywords         = {ImageLearn, CNN, Spatial Scale},
  modificationdate = {2023-12-06T08:00:49},
  owner            = {ISargent},
  year             = {2016},
}

@InProceedings{Deng2014,
  author    = {Jia Deng and Nan Ding and Yangqing Jia and Andrea Frome and Kevin Murphy and Samy Bengio and Yuan Li and Hartmut Neven and Hartwig Adam},
  title     = {Large-Scale Object Classification Using Label Relation Graphs},
  booktitle = {European Conference on Computer Vision (ECCV)},
  year      = {2014},
  url       = {http://web.eecs.umich.edu/~jiadeng/paper/deng2014large.pdf},
  comment   = {Collaboration between Google and University of Michigan. Graphs to model knowledge. Hierarchy and exclusion (HEX) graphs have edges to demonstrate parent/child classes (e.g. husky is subclass of dog) as well as mutual exclustion (e.g. dog cannot be cat). Where there are no edges there is overlap between classes (probably). Use graphs for probabilistic classification. I don't know how the graphs are built - haven't read paper in detail.},
  owner     = {ISargent},
  creationdate = {2014.10.03},
}

@InProceedings{Deng2011,
  author       = {Li Deng},
  booktitle    = {Proceedings of Asian-Pacific Signal \& Information Processing Annual Summit and Conference (APSIPA-ASC)},
  title        = {An overview of deep-structured learning for information processing},
  url          = {http://131.107.65.14/pubs/155609/DENG-APSIPA.pdf},
  comment      = {Generally well-written (except for one or two sections) review of deep learning for information processing, written to accompany a tutorial. Divides deep networks into 3 categories: generative, discriminative and hybrid (which includes discriminative networks pre-trained by generative methods) Bias towards applications in speech recognition and I htink this is from the microsoft team that produced the lauded English-Mandarin translator. ``Human information processing mechanisms (e.g., vision and speech), however, suggest the need of deep architectures for extracting complex structure and building internal representation from rich sensory inputs. For example, human speech production and perception systems are both equipped with clearly layered hierarchical structures in transforming the information from the waveform level to the linguistic level (Baker et al., 2009; Deng, 1999, 2003)''.},
  creationdate = {2015.06.24},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  year         = {2011},
}

@InCollection{DengD2013,
  author       = {Li Deng and Dong Yu},
  booktitle    = {Foundations and Trends in Signal Processing},
  title        = {Deep Learning:Methods and Applications},
  number       = {3-4},
  pages        = {197-387},
  url          = {http://research.microsoft.com/pubs/209355/DeepLearning-NowPublishing-Vol7-SIG-039.pdf},
  volume       = {7},
  comment      = {'Book' reviewing deep learning mainly for text and speech tasks but also includes a computer vision/object recognition chapter. Masses of references. Haven't read but could be useful. `` In the most recent literature, deep learning is embodied also as representation learning, which involves a hierarchy of features or concepts where higher-level representations of them are defined from lower-level ones and where the same lower-level representations help to define higher-level ones. ``},
  creationdate = {2014.10.09},
  keywords     = {deep learning},
  owner        = {ISargent},
  year         = {2013},
}

@Article{DhillonM2001,
  author       = {Inderjit S. Dhillon and Dharmendra S. Modha},
  title        = {Concept Decompositions for Large Sparse Text Data Using Clustering},
  journaltitle = {Machine Learning},
  year         = {2001},
  volume       = {42},
  number       = {1},
  pages        = {143-175},
  comment      = {Introduce spherical k-means, consine similarity. Nicely puts concepts into words after defining them mathematically.},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  creationdate    = {2015.06.08},
}

@Article{DoerschSGSE2012,
  author       = {Doersch, C and Singh, S and Gupta, A and Sivic, J and Efros, A. A},
  title        = {What makes Paris look like Paris?},
  journaltitle = {ACM Transactions on Graphics},
  year         = {2012},
  volume       = {31},
  number       = {4},
  pages        = {101},
  url          = {http://graphics.cs.cmu.edu/projects/whatMakesParis/paris_sigg_reduced.pdf},
  comment      = {Fascinating paper using Google streetview photos to identify architectural and other characteristics of different cities. Take patches of street-level photos and use weak geographical clustering to find discriminative features. Worth understanding in detail some time.},
  keywords     = {Characterisation},
  owner        = {ISargent},
  creationdate    = {2014.09.30},
}

@InProceedings{DollarTPB2009,
  author    = {Piotr Doll\'{a}r and Zhuowen Tu and Pietro Perona and Serge Belongie},
  title     = {Integral channel features},
  booktitle = {British Machine Vision Conference},
  year      = {2009},
  url       = {http://pages.ucsd.edu/~ztu/publication/dollarBMVC09ChnFtrs_0.pdf},
  comment   = {Seem to create a massive set of features form the images by applying various transforms. Resulting images are called channels. Examples are convolutions, Haar-like features.},
  keywords  = {ImageLearn},
  owner     = {ISargent},
  creationdate = {2015.06.05},
}

@InProceedings{DonahueJVHZTD2014,
  author       = {Jeff Donahue and Yangqing Jia and Oriol Vinyals and Judy Hoffman and Ning Zhang and Eric Tzeng and Trevor Darrell},
  booktitle    = {Proceedings of the 31st International Conference on Machine Learning},
  title        = {DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition},
  editor       = {Eric P. Xing and Tony Jebara},
  number       = {1},
  pages        = {647--655},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  url          = {http://proceedings.mlr.press/v32/donahue14.html},
  volume       = {32},
  abstract     = {We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.},
  address      = {Bejing, China},
  comment      = {Extract features from a pre-trained network to use generic tasks. Evaluate the features learned at different layers in the network and visualise using T-SNE. ``Our model can either be considered as a deep architecture for transfer learning  based  on  a  supervised  pre-training  phase,  or  simply DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition as  a  new  visual  feature DeCAF defined  by  the  convolutional network weights learned on a set of pre-defined object recognition tasks.'' ``We visualize features in the following way:  we run the t-SNE algorithm (van der Maaten \& Hinton, 2008) to find a 2-dimensional embedding of the high-dimensional feature space, and plot them as points colored depending on their semantic category in a particular hierarchy. ``

Also extract features from networks pretrained with ImageNet, train different classifiers (logistic regression, SVM) and compare ability to transfer against CalTexh-101 (in both cases dropout improved results) and on the Office domain adaptation test dataset. Also tested capacity of pretraining on subcategory recognition using the Caltech-UCSD birds dataset and on scene recognition using the e SUN-397 large-scale scene recognition database.},
  creationdate = {2017.09.08},
  keywords     = {deep learning, pretraining, image classification, object detection, MLStrat Object},
  month        = {6},
  owner        = {ISargent},
  year         = {2014},
}

@Article{Doneus2013,
  author       = {Michael Doneus},
  journaltitle = {Remote Sensing},
  title        = {Openness as Visualization Technique for Interpretative Mapping of Airborne Lidar Derived Digital Terrain Models},
  doi          = {10.3390/rs5126427},
  pages        = {6427--6442},
  url          = {https://www.mdpi.com/2072-4292/5/12/6427},
  volume       = {5},
  comment      = {Suggest measure of openness instead of hillshade etc for visualising for lidar data.},
  owner        = {ISargent},
  creationdate    = {2015.12.14},
  year         = {2013},
}

@Article{DorningerP2008,
  author       = {Dorninger, P. and Pfeifer, N},
  journaltitle = {Sensors},
  title        = {A Comprehensive Automated 3D Approach for Building Extraction, Reconstruction, and Regularization from Airborne Laser Scanning Point Clouds},
  pages        = {7323-7343},
  url          = {http://www.igp.ethz.ch/photogrammetry/education/lehrveranstaltungen/PCV_HS11/content_folder/PCV-HS2011-script-objectextr-dorninger.pdf},
  volume       = {8},
  comment      = {''In this article, we propose a comprehensive approach for automated determination of 3D city models from airborne acquired point cloud data. It is based on the assumption that individual buildings can be modeled properly by a composition of a set of planar faces. Hence, it is based on a reliable 3D segmentation algorithm, detecting planar faces in a point cloud''.},
  creationdate = {2014.10.28},
  keywords     = {3D buildings},
  num          = {11},
  owner        = {ISargent},
  year         = {2008},
}

@Article{DosovitskiyB2015,
  author       = {Alexey Dosovitskiy and Thomas Brox},
  journaltitle = {arXiv:1506.02753},
  title        = {Inverting Convolutional Networks with Convolutional Networks},
  url          = {http://arxiv.org/abs/1506.02753},
  comment      = {Invert CNNs to study the learned representations. Mostly, the higher level representations seem to be blurred versions of the input image. showed that higher level representations contained much of the original information from the input image.},
  keywords     = {ImageLearn, visualisation, MLStrat Discovery},
  owner        = {ISargent},
  creationdate    = {2015.07.16},
  year         = {2015},
}

@Misc{Driscoll2014,
  author       = {M. E. Driscoll},
  title        = {The Data Science Debate: Domain Expertise or Machine Learning? Summary of debate at Strata Conference Santa Clara 2012.},
  url          = {http://medriscoll.com/post/18784448854/the-data-science-debate-domain-expertise-or-machine},
  comment      = {''Choosing the right features requires domain expertise'' (from video) ``Like any good debate topic, there is merit on both sides of the domain expertise versus machine learning proposition. As Hal Varian said when we asked him before the panel: ``it depends on the structure of the problem.'' And in fairness to the debate panelists, they did not choose their positions: we assigned teams fifteen minutes before we went on stage. One of the conclusions reached was that, when a problem is well-structured (or to Drew Conway's point, when a good question is posed), it is much easier for machine learning to succeed. Kaggle's strength as a contest platform is that domain experts have already framed the problem: they choose the features of the data to use (feature engineering or ``feature creation'', as Monica Rogati calls it) as well as the criteria for success. This is the first, hardest step in any data science project. After this, machine learners can step in and develop the best algorithms for classifying and predicting new data (or, less usefully, explaining old data). Thus who you decide to hire as your first data scientist - a domain expert or a machine learner - might be as simple as this: could you currently prepare your data for a Kaggle competition? If so, then hire a machine learner. If not, hire a data scientist who has the domain expertise and the data hacking skills to get you there.''},
  creationdate = {2014.11.07},
  keywords     = {ImageLearn, domain expertise, MLStrat},
  owner        = {ISargent},
  year         = {2014},
}

@Book{DudaH1973,
  author    = {Richard O. Duda and Peter E. Hart},
  title     = {Pattern Classification and Scene Analysis},
  year      = {1973},
  comment   = {The original.},
  keywords  = {feature extraction},
  owner     = {ISargent},
  creationdate = {2017.04.12},
}

@Book{DudaHS,
  author       = {Richard O. Duda and Peter E. Hart and David G. Stork},
  title        = {Pattern Classification},
  edition      = {Second Edition},
  comment      = {''an ideal feature extractor would yield a representation that makes the job ofthe classifier trivial; conversely, an omnipotent classifier would not need the help of a sophisticated feature extractor. The distinction is forced upon us for practical, rather than theoretical reasons.''},
  creationdate = {2017.04.12},
  keywords     = {feature extraction},
  owner        = {ISargent},
  year         = {2000},
}

@Article{EfrosT2016,
  author       = {Alyosha Efros and Antonio Torralba},
  journaltitle = {International Journal of Computer Vision},
  title        = {Guest Editorial: Big Data},
  url          = {http://link.springer.com/article/10.1007%2Fs11263-016-0914-5},
  comment      = {Differentiates between the objectives of ``vision as measurement'' (objective, well-posed, problems) and ``vision as understanding'' (subjective, philosphical problems). Natural phenomena (and human interpretations of them - Izzy) are difficult to describe in terms of concise models but big data allows them to described in terms of data-centric models.},
  creationdate = {2016.08.31},
  keywords     = {Machine Learning, ImageLearn},
  owner        = {ISargent},
  year         = {2016},
}

@Article{EinhauserSP2008,
  author           = {Einh\''{a}user, W. and Spain, M. and Perona, P.},
  journaltitle     = {Journal of Vision},
  title            = {Objects predict fixations better than early saliency},
  number           = {14},
  pages            = {18},
  url              = {http://dx.doi.org/10.1167/8.14.18},
  volume           = {8},
  comment          = {''In some images, saliency is an excellent predictor of fixated locations (for details, see Methods), while in other images prediction is poor'' ``If objects are known, early saliency contributes little to fixation prediction''},
  creationdate     = {2015.07.08},
  keywords         = {ImageLearn, vision},
  modificationdate = {2022-05-08T15:39:27},
  owner            = {ISargent},
  year             = {2008},
}

@InProceedings{EllulA13,
  author    = {C. Ellul and J. Altenbuchner},
  title     = {LOD 1 Vs. LOD 2 - Preliminary Investigations into Differences in Mobile Rendering Performance},
  booktitle = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  year      = {2013},
  volume    = {II-2/W1},
  note      = {ISPRS 8th 3DGeoInfo Conference \& WG II/2 Workshop},
  month     = {11},
  url       = {http://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/II-2-W1/129/2013/isprsannals-II-2-W1-129-2013.pdf},
  address   = {Istanbul, Turkey},
  comment   = {Testing differences in rendering 3D data between block models and models with roof shapes. There is a significant time difference. Excellent paper for references on applications of 3D data as well as their creation.},
  keywords  = {3D buildings, visualisation, applications},
  owner     = {ISargent},
  creationdate = {2013.10.22},
}

@TechReport{EllulTXSR13,
  author      = {Claire Ellul and Nart Tamash and Feng Xian and John Stuiver and Patrick Rickles},
  title       = {Using Free and Open Source GIS to Automatically Create Standards-Based Spatial Metadata in Academia - First Investigations},
  institution = {University College London},
  year        = {2013},
  url         = {http://tinyurl.com/kom3wn8},
  comment     = {Paper on the potential to automate metadata, esp with ref to Inspire and other standards.},
  keywords    = {quality},
  owner       = {ISargent},
  creationdate   = {2013.10.22},
}

@Article{ErhanBCMVB2010,
  author        = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Manzagol, Pierre-Antoine and Vincent, Pascal and Bengio, Samy},
  title         = {Why Does Unsupervised Pre-training Help Deep Learning?},
  journal       = {J. Mach. Learn. Res.},
  year          = {2010},
  volume        = {11},
  month         = mar,
  pages         = {625--660},
  issn          = {1532-4435},
  url           = {http://dl.acm.org/citation.cfm?id=1756006.1756025},
  acmid         = {1756025},
  bdsk-url-1    = {http://dl.acm.org/citation.cfm?id=1756006.1756025},
  comment       = {unsupervised learning may be the best strategy for most layers of a deep network, with supervised learning either confined to a relatively shallow output section, or used for fine-tuning the network},
  creationdate    = {2016-12-13 13:58:41 +0000},
  date-modified = {2016-12-13 14:00:41 +0000},
  issue_date    = {3/1/2010},
  keywords      = {ImageLearn},
  numpages      = {36},
  owner         = {ISargent},
  publisher     = {JMLR.org},
  creationdate     = {2017.05.30},
}

@TechReport{ErhanBCV2009,
  Title                    = {Visualizing higher-layer features of a deep network},
  Author                   = {D. Erhan and Y. Bengio and A. Courville and P. Vincent},
  Institution              = {University of Montreal},
  Year                     = {2009},
  Number                   = {1341},

  Date                     = {6},
  Keywords                 = {ImageLearn, visualisation},
  Owner                    = {ISargent},
  creationdate                = {2017.05.30}
}

@TechReport{ErhanDC2010,
  author       = {Erhan, Dumitru and Courville, Aaron and Bengio, Yoshua},
  title        = {Understanding representations learned in deep architectures},
  url          = {http://www.dumitru.ca/files/publications/invariances_techreport.pdf},
  comment      = {Paper about visualising the representations learned in deep networks. ``we contrast and compare several techniques for finding such interpretations. We applied our techniques on Stacked Denoising Auto-Encoders and Deep Belief Networks, trained on several vision datasets''. ``To better understand what models learn, we set as an aim the exploration of ways to visualize what a unit activates in an arbitrary layer of a deep network. The goal is to have this visualization in the input space (of images), while remaining computationally efficient, and to make it as general as possible (in the sense of it being applicable to a large class of neural-network-like models).'' Discusses others' work visualising representations. A single layer method can just involve a transform back to input space. HintonOT06 incorporated a generative procedure and this paper make this more generally applicable to any layer. Effectively this method samples from the inputs ``by performing ancestral top-down sampling'' by fixing the target hidden unit to 1 and the result is a distribution for the representation.This paper introduces a further method ``inspired by the idea of maximizing the response of a given unit'' which effectively identifies this inputs that maximise the activation of the unit. These inputs then need to be somehow combined to show what they have in common. LeeEN2008 used linear combinations of lower level representations. Paper also looks at the rotation invariance of learned representations.},
  creationdate = {2014.07.11},
  keywords     = {ImageLearn, deep learning, machine learning, rotation invariance, feature extraction, toponet metrics, visualisation},
  owner        = {ISargent},
  year         = {2010},
}

@InProceedings{EslamiHW12,
  author       = {S. M. Eslami and N. Heess and J. Winn},
  booktitle    = {Proceedings of the Conference on Computer Vision and Pattern Recognition},
  title        = {The Shape Boltzmann Machine: a Strong Model of Object Shape},
  url          = {http://research.microsoft.com/pubs/162693/cvpr_12_eslami_shapebm.pdf},
  comment      = {Shapes are binary object (1) and background (0) images. ``This paper addresses the question of how to build a strong probabilistic model of binary object shapes. We define a strong model as one which meets two requirements: 1. Realism - samples from the model look realistic; 2. Generalization - the model can generate samples that differ from training examples.'' ``RBMs can, in principle, approximate any binary distribution [10], but this can require an exponential number of hidden units and a similarly large amount of training data. The DBM provides a richer model by introducing additional layers of latent variables as shown in Fig. 2(c). The additional layers capture high-order dependencies between the hidden variables of previous layers and so can learn about complex structure in the data using relatively few hidden units.''},
  creationdate = {2013.10.16},
  keywords     = {Machine Learning, ComputeLearning, Deep Learning, ImageLearn},
  owner        = {ISargent},
  year         = {2012},
}

@TechReport{EthicalAI2019,
  author           = {{The IEEE Global Initiative}},
  date             = {2019},
  institution      = {IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems},
  title            = {Ethically Aligned Design},
  eprint           = {https://ethicsinaction.ieee.org/wp-content/uploads/ead1e.pdf},
  subtitle         = {A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems},
  url              = {https://ethicsinaction.ieee.org/},
  comment          = {Use A/IS instead of AI: Autonomous and Intelligent Systems.

General principles:
1. Human Rights
2. Well-being
3. Data Agency
4. Effectiveness
5. Transparency
6. Accountability
7. Awareness of Misuse
8. Competence

Each of these can be mapped to one or more of the following pillars.

The Three Pillars of the Ethically Aligned Design Conceptual Framework:
1. Universal Human Values
2. Political Self-Determination and Data Agency
3. Technical Dependability

Areas of Impact:
A/IS for Sustainable Development
Personal Data Rights and Agency Over Digital Identity
Legal Frameworks for Accountability
Policies for Education and Awareness

Implementation:
Well-being Metrics
Embedding Values into Autonomous and Intelligent Systems
Methods to Guide Ethical Research and Design
Affective Computing

Really interesting chapter on classical ethics which identifies a number of issues and makes recommendations. Ethics includes vertue, duty, utilitarian and care. Includes ethics from different traditional including Buddism, Shinto and Ubuntu.

Under Corporate Practices on A/IS, recommendations include:
- top-down leadership, bottom-up empowerment, ownership, and responsibility, along with the need to consider system deployment contexts and/or ecosystems. Corporations should identify stages in their processes in which ethical considerations, “ethics filters”, are in place before products are further developed and deployed
- Companies should create roles for senior-level marketers, engineers, and lawyers who can collectively and pragmatically implement ethically aligned design. There is also a need for more in-house ethicists, or positions that fulfill similar roles
- Employees should be empowered and encouraged to raise ethical concerns
- Organizations should clarify the relationship between professional ethics and applied A/IS ethics by helping or enabling designers, engineers, and other company representatives to discern the differences between these kinds of ethics and where they complement each other
- Corporate ethical review boards, or comparable mechanisms, should be formed to address ethical and behavioral concerns in relation to A/IS design, development and deployment 
- To ensure representation of stakeholders, organizations should enact a planned and controlled set of activities
- Companies should study design processes to identify situations where engineers and researchers can be encouraged to raise and resolve questions of ethics and foster a proactive environment to realize ethically aligned design
- Organizations should identify points for formal review during product development. These reviews can focus on “red flags” that have been identified in advance as indicators of risk


Old notes (2017):
''A Vision for Prioritizing Human Wellbeing with Artificial Intelligence and Autonomous Systems''
Sections:
General Principles
 We are motivated by a desire to create ethical principles for AI/AS that:
 1.Embody the highest ideals of human rights.
 2.Prioritize the maximum benefit to humanity and the natural environment.
 3.Mitigate risks and negative impacts as AI/AS evolve as socio-technical systems.
Embedding Values into Autonomous Intelligence Systems
Methodologies To Guide Ethical Research and Design
Safety and Beneficence of Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI) 
Personal Data and Individual Access Control 
Reframing Autonomous Weapons Systems
Economics/Humanitarian Issues 
Law},
  creationdate     = {2017.01.05},
  keywords         = {AI, Ethics, EthicsWS},
  modificationdate = {2023-12-11T08:01:14},
  organisation     = {IEEE},
  owner            = {ISargent},
  year             = {2019},
}

@InProceedings{EvansLS2014,
  author    = {Stephen Evans and Rob Liddiard and Philip Steadman},
  title     = {A 3D geometrical model of the non-domestic building stock of England and Wales},
  booktitle = {Building Simulation and Optimisation conference},
  year      = {2014},
  month     = {6},
  url       = {http://www.bartlett.ucl.ac.uk/energy/news/documents/BSO14_Paper_026.pdf},
  address   = {University College London},
  comment   = {Work produced using AddressBase Premium and Sites layer data for GB and a small area of Topo data with Building Height Attribute using geospatial data to understand energy use in buildings better.},
  keywords  = {3D usage},
  owner     = {ISargent},
  creationdate = {2014.11.07},
}

@InProceedings{EveleighJBBC2014,
  Title                    = {Designing for Dabblers and Deterring Drop - Outs in Citizen Science},
  Author                   = {Alexandra Eveleigh and Charlene Jennett and Ann Blandford and Philip Brohan and Anna L. Cox},
  Booktitle                = {ACM CHI Conference on Human Factors in Computing Systems},
  Year                     = {2014},

  Abstract                 = {In most online citizen science projects, a large proportion of participants contribute in small quantities. To investigate how low contributors differ from committed volunteers, we distributed a survey to members of the Old Weather project, followed by interviews with respondents selected according to a range of contribution levels. The studies reveal a complex relationship between motivations and contribution. Whilst high contributors were deeply engaged by social or competitive features, low contributors described a solitary experience of 'dabbling' in projects for short periods. Since the majority of participants exhibit this small -scale contribution pattern, there is great potential value in designing interfaces to tempt lone workers to complete 'just another page', or to lure early drop-outs back into participation. This includes breaking the work into components which can be tackled without a major commitment of time and effort, and providing feedback on the quality and value of these contributions.},
  Keywords                 = {RapidDC},
  Owner                    = {ISargent},
  creationdate                = {2015.07.20},
  Url                      = {http://discovery.ucl.ac.uk/1418573/1/p2985-eveleigh.pdf}
}

@Book{FaircloughLN1999,
  Title                    = {Yesterday's World, Tomorrow's Landscape},
  Author                   = {Fairclough, Graham J and Lambrick, G and McNab, A},
  Publisher                = {English Heritage},
  Year                     = {1999},

  Keywords                 = {Landscape, Characterisation, ImageLearn},
  Owner                    = {ISargent},
  creationdate                = {2017.04.04}
}

@Article{FellemanV1991,
  author       = {Felleman, D.J. and Van Essen, D.C.},
  journaltitle = {Cerebral Cortex},
  title        = {Distributed hierarchical processing in the primate cerebral cortex},
  pages        = {1--47},
  volume       = {1},
  comment      = {From Friston2002 ``The organisation of the visual cortices can be considered as a hierarchy''},
  creationdate = {2015.07.16},
  keywords     = {ImageLearn, neuroscience},
  owner        = {ISargent},
  year         = {1991},
}

@InProceedings{FengSB2016,
  author    = {Y. Feng and A. Schlichting and C. Brenner},
  title     = {3{D} Feature Point Extraction from Lidar Data Using a Neural Network},
  booktitle = {ISPRS XXIII CONGRESS},
  year      = {2016},
  date      = {2016},
  comment   = {Detect features for matching between images using neural network.},
  owner     = {ISargent},
  creationdate = {2016.07.05},
}

@Misc{Fergus2013,
  author       = {Rob Fergus},
  title        = {Deep Learning for Computer Vision},
  year         = {2013},
  howpublished = {Tutorial at NIPS 2013},
  month        = {12},
  url          = {http://research.microsoft.com/apps/video/default.aspx?id=206976&l=i},
  comment      = {Overview of the state of the art of deep learning introduced by Chris Bishop. Log book 7/8/14.},
  owner        = {ISargent},
  creationdate    = {2014.09.09},
}

@Article{Field87,
  author       = {Field, David J.},
  journaltitle = {Journal of the Optical Society of America A},
  title        = {Relations between the statistics of natural images and the response properties of cortical cells},
  number       = {12},
  url          = {https://www.osapublishing.org/josaa/fulltext.cfm?uri=josaa-4-12-2379&id=2980f},
  volume       = {4},
  abstract     = {The relative efficiency of any particular image-coding scheme should be defined only in relation to the class of images that the code is likely to encounter. To understand the representation of images by the mammalian visual system, it might therefore be useful to consider the statistics of images from the natural environment (i.e., images with trees, rocks, bushes, etc). In this study, various coding schemes are compared in relation to how they represent the information in such natural images. The coefficients of such codes are represented by arrays of mechanisms that respond to local regions of space, spatial frequency, and orientation (Gabor-like transforms). For many classes of image, such codes will not be an efficient means of representing information. However, the results obtained with six natural images suggest that the orientation and the spatial-frequency tuning of mammalian simple cells are well suited for coding the information in such images if the goal of the code is to convert higher-order redundancy (e.g., correlation between the intensities of neighboring pixels) into first-order redundancy (i.e., the response distribution of the coefficients). Such coding produces a relatively high signal-to-noise ratio and permits information to be transmitted with only a subset of the total number of cells. These results support Barlow's theory that the goal of natural vision is to represent the information in the natural environment with minimal redundancy.},
  comment      = {Fundamental paper and referenced in SimoncelliO01},
  keywords     = {Neuroscience, ImageLearn},
  owner        = {isargent},
  creationdate    = {2014.01.14},
  year         = {1987},
}

@InProceedings{FiratCV2014,
  author    = {Orhan Firat and Gulcan Can and Fatos T. Yarman Vural},
  title     = {Representation Learning for Contextual Object and Region Detection in Remote Sensing},
  booktitle = {22nd International Conference on Pattern Recognition (ICPR)},
  year      = {2014},
  month     = {8},
  pages     = {3708-3713},
  doi       = {10.1109/ICPR.2014.637},
  address   = {Stockholm, Sweden},
  comment   = {Use sparse autoencoders to extract representations from remotely sensed data. When using an ordinary sparse autoencoder (with nromalised and whitened imagery) the test images were convolved with the learned 'factors' for detecting airfields. Conditional Random Fields are then used to identify the class label of the central pixel. Also build convolutional sparse autoencoders (CSA) for object and region detection.},
  issn      = {1051-4651},
  keywords  = {ImageLearn, CNN, Autoencoder, Remote Sensing, DeepLEAP},
  owner     = {ISargent},
  creationdate = {2016.05.11},
}

@Article{FogelS1989,
  Title                    = {Gabor filters as texture discriminator},
  Author                   = {Fogel, I. and Sagi, D.},
  Journal                  = {Biol. Cybernetics},
  Year                     = {1989},
  Number                   = {2},
  Pages                    = {103-113},
  Volume                   = {61},

  Bdsk-url-1               = {http://dx.doi.org/10.1007/BF00204594},
  creationdate               = {2016-12-13 13:58:41 +0000},
  Date-modified            = {2016-12-13 14:00:41 +0000},
  Doi                      = {10.1007/BF00204594},
  ISSN                     = {0340-1200},
  Language                 = {English},
  Owner                    = {ISargent},
  Publisher                = {Springer-Verlag},
  creationdate                = {2017.04.13},
  Url                      = {http://dx.doi.org/10.1007/BF00204594}
}

@Article{FogelS89,
  author       = {Fogel, I. and Sagi, D},
  title        = {Gabor filters as texture discriminator},
  journaltitle = {Biological Cybernetics},
  year         = {1989},
  volume       = {61},
  number       = {2},
  comment      = {Reference for Gabor filters},
  keywords     = {feature extraction, DeepLEAP},
  owner        = {ISargent},
  creationdate    = {2013.12.18},
}

@Article{FormanG81,
  author       = {Forman, R.T.T. and Godron, M.},
  title        = {Patches and structural components for a landscape ecology},
  journaltitle = {Bioscience},
  year         = {1981},
  volume       = {31},
  number       = {10},
  pages        = {733-740},
  url          = {http://www.jstor.org/discover/10.2307/1308780?uid=3738032&uid=2129&uid=2&uid=70&uid=4&sid=21103146551517},
  comment      = {An early paper considering that landscapes are a recognisable and useful unity in ecology.},
  keywords     = {ImageLearn, landscape regionalization, ecology},
  owner        = {ISargent},
  creationdate    = {2013.12.19},
}

@Article{FriedmanF01,
  author       = {Friedman, Ronald S and F\''{o}rster, Jens},
  journaltitle = {Journal of personality and social psychology},
  title        = {The effects of promotion and prevention cues on creativity},
  number       = {6},
  pages        = {1001-1013},
  url          = {http://www.socolab.de/content/files/Jens%20pubs/friedman_foerster2001.pdf},
  volume       = {81},
  comment      = {Research referenced in Williams and Penman Mindfullness book. Participants completed maze puzzles (as if to help a mouse). Apparently incidental to the puzzle was either that the maze picture contained cheese at the exit or an owl at the start. Both sets completed the puzzle well but a further test then guaged creative problem solving and it was found that those who had completed the owl maze were 50 percent worse than those who'd competed the cheese maze. Paper cites much previous work in the area.},
  creationdate = {2014.01.31},
  keywords     = {RapidDC},
  owner        = {isargent},
  year         = {2001},
}

@Article{Friston2005,
  author       = {Friston, K},
  journaltitle = {Philosophical Transactions of the Royal Society of London B: Biological Science},
  title        = {A theory of cortical responses},
  pages        = {815--836},
  url          = {http://www.fil.ion.ucl.ac.uk/~karl/A%20theory%20of%20cortical%20responses.pdf},
  volume       = {360},
  comment      = {theory of cortical responses as a hierarchical generative model implemented as a predictive model where the error of prediction is used to adjust the state of the model.},
  creationdate = {2015.07.16},
  keywords     = {ImageLearn, neuroscience},
  owner        = {ISargent},
  year         = {2005},
}

@Article{Friston2002,
  author       = {Friston, K. J.},
  journaltitle = {Progress in Neurobiology},
  title        = {Functional integration and inference in the brain},
  pages        = {113--143},
  url          = {http://www.fil.ion.ucl.ac.uk/~karl/Functional%20integration%20and%20inference%20in%20the%20brain.pdf},
  volume       = {68},
  comment      = {Presents a model of how the brain represents and categorises the causes of sensory inputs. Shows that feedforward architectures alone are no sufficient. Generative models require backward connections. These connections must be modulatory: estimated causes at higher levels interact with predicted responses at lower levels. Fantastic reference resource. Discussion of representational learning in the brain. Emprical Bayes shown to be biologically plausible model of how the brain infers and learns causes.},
  creationdate = {2015.07.16},
  keywords     = {ImageLearn, neuroscience},
  owner        = {ISargent},
  year         = {2002},
}

@InProceedings{FromeCSBDRM2013,
  Title                    = {DeViSE: A Deep Visual-Semantic Embedding Model},
  Author                   = {Andrea Frome and Greg Corrado and Jon Shlens and Samy Bengio and Jeffrey Dean and Marc'Aurelio Ranzato and Tomas Mikolov},
  Booktitle                = {Advances In Neural Information Processing Systems, {NIPS}},
  Year                     = {2013},

  Owner                    = {ISargent},
  creationdate                = {2015.04.24}
}

@Article{Fukushima1980,
  author       = {Kunihiko Fukushima},
  title        = {Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},
  journaltitle = {Biological Cybernetics},
  year         = {1980},
  volume       = {36},
  number       = {4},
  pages        = {193-202},
  comment      = {The Neocognitron is neural network model that was inspired by Hubel and Wiesel's cascading model of the visual nervous system. It was an unsupervised algorithm. First proposal of convolutional neural networks (according to CastelluccioPSV2015).},
  keywords     = {ImageLearn, Vision, Neuroscience, CNN},
  owner        = {ISargent},
  creationdate    = {2015.06.30},
}

@Article{Gabor46,
  author       = {D. Gabor},
  journaltitle = {Journal of the Institute of Electrical Engineers},
  title        = {Theory of communication},
  pages        = {429-457},
  url          = {http://bigwww.epfl.ch/chaudhury/gabor.pdf},
  volume       = {93},
  comment      = {From Field87: ``Gabor showed how to represent time-varying signals in terms of functions that are localized in both time and frequency (the functions in time are represented by the product of a Gaussian and a sinusoid).''},
  creationdate = {2014.01.14},
  keywords     = {information theory},
  owner        = {isargent},
  year         = {1946},
}

@Article{Gill1999,
  author       = {Jeff Gill},
  title        = {The Insignificance of Null Hypothesis Significance Testing},
  journaltitle = {Political Research Quarterly},
  year         = {1999},
  volume       = {52},
  number       = {3},
  pages        = {647-674},
  url          = {http://www.nyu.edu/classes/nbeck/q2/gill.pdf},
  comment      = {Paper cited by Nate Silver as one of many rejecting Fisherian (frequentist) statistics.},
  keywords     = {statistics},
  owner        = {ISargent},
  creationdate    = {2014.10.03},
}

@Book{Gilpin1786,
  author    = {Gilpin, William},
  title     = {Observations relative chiefly to Picturesque Beauty, Made in the year 1772 ..... Cumberland \& Westmoreland},
  year      = {1786},
  publisher = {R.Blamire},
  address   = {London},
  comment   = {One of several books by William Gilpin in which he attempts to describe the components that make a scene 'picturesque' (his term). This is discussed in Gooley2012 (p273-275), saying that Gilpin is followin in the steps of Burke1757.},
  keywords  = {landscape, scene analysis, RapidDC},
  owner     = {ISargent},
  creationdate = {2014.06.25},
}

@InProceedings{GimenezRSZ2015,
  author    = {Lucile Gimenez and Sylvain Robert and Fr\'{e}d\'{e}ric Suard and Khaldoun Zreik},
  title     = {Cost-effective reconstruction of BIM from 2{D} scanned plan: experiments on existing buildings},
  booktitle = {Sustainable Places},
  year      = {2015},
  month     = {9},
  address   = {Savona, Italy},
  comment   = {Paper describing method of creating 3D building models from 2D plans. Divide model into geometry, topology and semantics. Employs methods of vectorisation and pattern recognition to idetnify walls and other elements and automatic recognition of text elements and human intervention to reconstruct the building.},
  keywords  = {3D extraction},
  owner     = {ISargent},
  creationdate = {2015.11.17},
}

@InProceedings{GladstoneGH2012,
  author       = {C. S. Gladstone and A. Gardiner and D. Holland},
  booktitle    = {Proceedings of the 4th {GEOBIA}},
  date         = {May 7-9},
  title        = {A Semi-Automatic Method for Detecting Changes to Ordnance Survey Topographic Data in Rural Environments},
  editor       = {Queiroz Feitosa, Raul and da Costa, Gilson Alexandre Ostwald Pedro and de Almeida, Cl\'{a}udia Maria and Garcia Fonseca, Leila Maria and Kux, Hermann Johann Heinrich},
  pages        = {396--401},
  url          = {http://mtc-m16c.sid.inpe.br/col/sid.inpe.br/mtc-m18/2012/05.14.17.56/doc/110.pdf},
  abstract     = {The detection of changes from aerial imagery is an essential task for Ordnance Survey in order to maintain its topographic database. This paper describes the work of the Research department to create a semi-automatic method for change detection in rural environments. The proposed method uses 4-band aerial imagery and a Digital Surface Model(DSM) generated from the corresponding panchromatic imagery to automatically classify and identify changes between the imagery and the topographic database using eCognition software. These automatically generated `change candidates' are then manually checked and any necessary map updates are carried out by a photogrammetrist. A trial of the method found 81.7\% of the genuine changes which require a map update (completeness), while 25.8\% of the `change candidates' were a genuine change (correctness). These results suggest that the method could provide significant efficiency savings if deployed in production. Other potential uses for the method have also been explored, particularly whether the automatic image classification could be used to filter DSMs to DTMs and whether it could contribute to a new land cover product for the Ordnance Survey.},
  address      = {Rio de Janeiro, Brazil},
  creationdate = {2016.11.22},
  keywords     = {DeepLEAP1},
  month        = {5},
  owner        = {ISargent},
  year         = {2012},
}

@InProceedings{GlorotB2010,
  author    = {Xavier Glorot and Yoshua Bengio},
  title     = {Understanding the difficulty of training deep feedforward neural networks},
  booktitle = {Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year      = {2010},
  volume    = {Volume 9 of JMLR},
  url       = {http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf},
  address   = {Chia La-guna Resort, Sardinia, Italy},
  comment   = {From the perspective of now being able to train deep networks (post Hinton 2006) this paper looks at some different aspects of training and the effect they have: activation function, weight initialisation, cost function. Uses various methods for looking at weight update as well as test error to understand better what is happening, although isn't able to explain everything. Finds sigmoid activation is poor due to not being symmetric around 0. Tanh is better but suggests Softsign (x/(1+|x|)) is better due to its smoother asymptotes. Shame relu hasn't been studied. For initialisation, unsupervised methods are probably prefered but this paper looks at initialising from a uniform distribution. Finds (especialy for Tanh activation) better results with their 'normalized initialization', which accounts for the weights in subsequent layers (since bradley 2009 it found that 'back-propagated gradients were smaller as one moves from the output layer towards the input layer, just after initialization'. Also 'found that the logistic regresssion or conditional log-likelihood cost function (-logP(y|x) couple with softmax outputs) worked much better (for classification problems) than the quadratic cost which was traditionally used to train feedforward neural networks'.},
  keywords  = {ImageLearn},
  owner     = {ISargent},
  creationdate = {2016.03.03},
}

@InProceedings{GlorotBB11,
  author       = {Xavier Glorot and Antoine Bordes and Yoshua Bengio},
  booktitle    = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS 2011)},
  title        = {Deep Sparse Rectifier Neural Networks},
  comment      = {Proposes rectifier activation function, which is closer to neural leaky integrate-and-fire activation function. Show that this produces sparseness. Image and sentiment analysis. From Bengio's MLSS14 slides: ``sparse rectified denoising autoencoders trained on bags of words for sentiment analysis - different features specialize on different aspects (domain, sentiment)''},
  creationdate = {2014.05.22},
  keywords     = {deep learning},
  owner        = {ISargent},
  year         = {2011},
}

@Article{GoldluckeAKC14,
  author       = {Bastian Goldl\''{u}cke and Mathieu Aubry and Kalin Kolev and Daniel Cremers},
  title        = {A Super-Resolution Framework for High-Accuracy Multiview Reconstruction},
  number       = {2},
  pages        = {172-191},
  url          = {http://link.springer.com/article/10.1007%2Fs11263-013-0654-8},
  volume       = {106},
  comment      = {Probably a more recent version of GoldLuckeC09.},
  creationdate = {2014.01.17},
  keywords     = {3D, super resolution},
  month        = {1},
  owner        = {ISargent},
  year         = {2014},
}

@InProceedings{GoldLuckeC09,
  author       = {Bastian Goldl\''{u}cke and Daniel Cremers},
  booktitle    = {Pattern Recognition (Proc. DAGM)},
  title        = {A Superresolution Framework for High-Accuracy Multiview Reconstruction},
  url          = {http://vision.in.tum.de/_media/spezial/bib/gc09_std.pdf},
  abstract     = {We present a variational approach to jointly estimate a displacement map and a superresolution texture for a 3D model from multiple calibrated views. The superresolution image formation model leads to an energy functional defined in terms of an integral over the object surface. This functional can be minimized by alternately solving a deblurring PDE and a total variation minimization on the surface, leading to increasingly accurate estimates of photometry and geometry, respectively. The resulting equations can be discretized and solved on texture space with the help of a conformal atlas. The superresolution approach to texture reconstruction allows to obtain fine details in the texture map which surpass individual input image resolution.},
  creationdate = {2014.01.17},
  keywords     = {3D, super resolution},
  owner        = {ISargent},
  year         = {2009},
}

@TechReport{GoodfellowBIAS14,
  author       = {Goodfellow, Ian J. and Yaroslav Bulatov and Julian Ibarz and Sacha Arnoud and Vinay Shet},
  institution  = {Google Inc.},
  title        = {Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks},
  url          = {http://arxiv.org/abs/1312.6082},
  comment      = {Google paper on ``recognizing arbitrary multi-digit numbers from Street View imagery''. Use deep convolutional neural network. Try out various architectures and find 11 layers is best but also suggest that deeper may be better. `` In this work we assume that the street numbers have already been roughly localized, so that the input image contains only one street number, and the street number itself is usually at least one third as wide as the image itself.'' ``We can for example transcribe all the views we have of street numbers in France in less than an hour using our Google infrastructure. Most of the cost actually comes from the detection stage that locates the street numbers in the large Street View images.'' ``One caveat to our results with this architecture is that they rest heavily on the assumption that the sequence is of bounded length, with a reasonably small maximum length N. For unbounded N, our method is not directly applicable, and for large N our method is unlikely to scale well...One possible solution could be to train a model that outputs one ``word'' (N character sequence) at a time and then slide it over the entire image followed by a simple decoding.''},
  creationdate = {2014.01.14},
  keywords     = {deep learning, machine learning, text recognition, ImageLearn},
  owner        = {isargent},
  year         = {2014},
}

@InProceedings{GoodfellowCB13,
  author       = {Ian J Goodfellow and Aaron Courville and Yoshua Bengio},
  booktitle    = {International Conference on Learning Representations},
  title        = {Joint training deep Boltzmann machines for classification},
  url          = {https://arxiv.org/abs/1301.3568},
  comment      = {Currently, deep models are trained for classification by first training deep machine to produce representations and then using these as features to another net and performing supervised training for classification. This work aims to do this two training session at the same time. Use multi-prediction training. There was a similar thing at AISTATS around 2011 that did a similar thing paper by Stoinov? (Jason Eisner's lab). From Bengio's MLSS14 slides ``sparse auto-encoders trained on images *some higher-level features more invariant to geometric factors of variaton''.},
  creationdate = {2014.05.22},
  keywords     = {Machine Learning, deep learning, ImageLearn},
  owner        = {ISargent},
  year         = {2013},
}

@InProceedings{GoodfellowLSLN09,
  author       = {Goodfellow, Ian J. and Le, Quoc V. and Saxe, Andrew M. and Honglak Lee and Ng, Andrew Y.},
  booktitle    = {Advances in Neural Information Processing Systems (NIPS)},
  title        = {Measuring invariances in deep networks},
  number       = {22},
  url          = {http://web.eecs.umich.edu/~honglak/nips09-MeasuringInvariancesDeepNetworks.pdf},
  comment      = {''we show that with increasing depth, the representations learned can also enjoy an increased degree of invariance''. Test stacked autoencoder and convolutional deep belief network. ``A surprising finding in our experiments with visual data is that stacked autoencoders yield only modest improvements in invariance as depth increases. This suggests that while depth is valuable, mere stacking of shallow architectures may not be sufficient to exploit the full potential of deep architectures to learn invariant features. Another interesting finding is that by incorporating sparsity, networks can become more invariant. This suggests that, in the future, a variety of mechanisms should be explored in order to learn better features...We also document that explicit approaches to achieving invariance such as max-pooling and weight-sharing in CDBNs are currently successful strategies for achieving invariance. ``},
  creationdate = {2014.05.22},
  keywords     = {Machine Learning, Deep learning, ImageLearn},
  owner        = {ISargent},
  year         = {2009},
}

@InProceedings{GoodfellowSS2015,
  author    = {Goodfellow, Ian J. and Jonathon Shlens and Christian Szegedy},
  booktitle = {ICLR 2015},
  title     = {Explaining and Harnessing Adversarial Examples},
  url       = {https://arxiv.org/abs/1412.6572v3},
  comment   = {Adversarial examples are examples of data with a possibly imperceptible difference from some real example but that ellicits and entirely different, erroneous, classification. It is a problem in many neural net as well as linear methods. This paper considers that the issue is due to the linear nature of even deep models (rather than, as other suggest, their non-linearity). Proposes a method for generating and then training with adversarial examples which improves the success of the network (and apparantly the representations learned). Also discusses 'rubbish examples' which to a human are meaningless but 'fool' the network. For example by perturbing noise in the direction if a given class this can be classified as that class. it seems that RBF networks, are not vulnerable to adversarial examples or rubbish examples. Mainly using MNIST.},
  keywords  = {ImageLearn, Visualisation},
  owner     = {ISargent},
  creationdate = {2016.01.29},
  year      = {2015},
}

@Unpublished{GoodwynM2015,
  author    = {Nicola Goodwyn and Diana Moraru},
  title     = {3D DATA GENERATION PROJECT PROVING REPORT - DTM Generation Software Evaluation},
  note      = {GEN/15/3576},
  comment   = {Report into software for creating synchronous DTM and DSM. Finds SimActive Correlator 3D to be the best of the 5 options (the others being LAStools, Inpho, Terrasolid and SOCET GXP.},
  keywords  = {DTM},
  owner     = {ISargent},
  creationdate = {2015.03.17},
}

@Book{Gooley2012,
  author    = {Tristan Gooley},
  title     = {The natural Explorer. Understanding your landscape.},
  year      = {2012},
  publisher = {Sceptre},
  comment   = {Somewhat essay-ey book about the components of our land, how they come to be there and how they are understood. Some good parts, including short biographies of previous authors and pholosophers who have done similar things. Discusses Gilpin1786 and Burke1757. See also page 278-279 for further discussion of what constitutes beauty.},
  keywords  = {landscape, ImageLearn},
  owner     = {ISargent},
  creationdate = {2014.06.25},
}

@TechReport{GrogerKC06,
  author       = {Gerhard Gr\''{o}ger and Kolbe, Thomas H and Angela Czerwinski},
  institution  = {Open Geospatial Consortium Inc.},
  title        = {Candidate OpenGIS^\circledR CitCity Implementation Specification (City Geography Markup Language)},
  number       = {OGC 06-057r1},
  type         = {Candidate OpenGIS Implementation Specification},
  comment      = {Includes descriptions of the 5 LODs levels of detail},
  creationdate = {2014.04.01},
  keywords     = {3D buildings},
  owner        = {ISargent},
  year         = {2006},
}

@Article{GraceSDZE2017,
  author           = {Katja Grace and John Salvatier and Allan Dafoe and Baobao Zhang and Owain Evans},
  date             = {2017-05-24},
  journaltitle     = {CoRR},
  title            = {When Will AI Exceed Human Performance? Evidence from AI Experts},
  eprint           = {1705.08807},
  eprinttype       = {arXiv},
  url              = {https://arxiv.org/abs/1705.08807},
  volume           = {abs/1705.08807},
  abstract         = {Advances in artificial intelligence (AI) will transform modern life by reshaping transportation, health, science, finance, and the military [1, 2, 3]. To adapt public policy, we need to better anticipate these advances [4, 5]. Here we report the results from a large survey of machine learning researchers on their beliefs about progress in AI. Researchers predict AI will outperform humans in many activities in the next ten years, such as translating languages (by 2024), writing high-school essays (by 2026), driving a truck (by 2027), working in retail (by 2031), writing a bestselling book (by 2049), and working as a surgeon (by 2053). Researchers believe there is a 50\% chance of AI outperforming humans in all tasks in 45 years and of automating all human jobs in 120 years, with Asian respondents expecting these dates much sooner than North Americans. These results will inform discussion amongst researchers and policymakers about anticipating and managing trends in AI.},
  bibsource        = {dblp computer science bibliography, \url{https://dblp.org}},
  biburl           = {https://dblp.org/rec/journals/corr/GraceSDZE17.bib},
  comment          = {Surveyed AI experts to get predictions of when AI will surpass human ability in different areas.

Survey of 352 `top' AI researchers. Lots of disagreement.

When will machines achieve high level machine intelligence - wide range of views and depends on how the question was asked

Also discusses AI safety and how experts seem to underestimate the need to address this (despite `5\% chance of human extinction')

Summarised brilliantly here:  \url{https://www.youtube.com/watch?v=HOJ1NVtlnyQ}},
  creationdate     = {2017.06.19},
  keywords         = {AI, Transition, MLStrat, trust, ethics, AISummit},
  modificationdate = {2023-12-11T07:59:16},
  owner            = {ISargent},
  year             = {2017},
}

@Article{GrogerP2012,
  author       = {GrÃƒÂ¶ger, G. and Pl\''umer, L.},
  journaltitle = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title        = {CityGML--Interoperable semantic 3D city models},
  pages        = {12-33},
  volume       = {71},
  comment      = {''This paper gives an overview of CityGML, its underlying concepts, its Levels-of-Detail, how to extend it, its applications, its likely future development, and the role it plays in scientific research. Furthermore, its relationship to other standards from the fields of computer graphics and computer-aided architectural design and to the prospective INSPIRE model are discussed, as well as the impact CityGML has and is having on the software industry, on applications of 3D city models, and on science generally.''},
  creationdate = {2014.10.30},
  owner        = {ISargent},
  year         = {2012},
}

@Misc{Microsoft09,
  author       = {John Guiver and John Winn},
  title        = {Infer.NET: Building Software with Intelligence},
  howpublished = {Presented at PDC 2009},
  url          = {http://www.microsoftpdc.com/2009/VTL03},
  comment      = {Call the signal to desired value problem ``reasoning backwards''. Infer.NET, like other probabilistic programming tools, works with probability distributions (rather than running problems many times). E.g. use inference engine to infer new distribution given observed values. Can have forwards and backwards messages. Have seen up to 25 minutes in - real examples after this point. variable.if() statement has variable.ifnot() statement, the latter being a bit like else. However, in probabilistic programming both statements are executed. Gives examples of clicking on web search results and TrueSkill for judging Halo players. 100's of lines of code for TrueSkill and several months to develop but with Infer.NET only a few lines and an hour to develop. Can use if() and ifnot() to create variables with different distributions if don't know what distribution to use.},
  creationdate = {2014.06.03},
  month        = {11},
  owner        = {ISargent},
  year         = {2009},
}

@Article{GuoA2016,
  author       = {Wenzhangzhi Guo and Parham Aarabi},
  title        = {Hair Segmentation Using Heuristically-Trained Neural Networks},
  comment      = {PDF in library folder. Key thing here is the heuristic training. I believe this is done by first defining a set of rules to identify hair e.g. based on colour and texture (sadly they don't say what the rules are but there are other papers that have worked on this in the introduction ``Aarabi [10] first built a hair color model and a hair gradient model based on plausible hair regions in the input image''). These are then used to give a probability that regions of images are hair. The high confidence regions (hair and non-hair) are then used to train the NN. The classification is performed on images patches and there is a chunk of post-processing e.g. to remove eyes and find the largest connected component. Using the heuristic component apparently means fewer training examples are required. This method out-performs all other methods tested.},
  creationdate = {2016.11.29},
  journal      = {IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS},
  owner        = {ISargent},
  year         = {2016},
}

@Article{GuoBSWK2016,
  author       = {Yulan Guo and Mohammed Bennamoun and Ferdous Sohel and Min Lu and Jianwei Wan and Ngai Ming Kwok},
  title        = {A Comprehensive Performance Evaluation of 3D Local Feature Descriptors},
  journaltitle = {international Journal of Computer Vision},
  year         = {2016},
  comment      = {Describe and compare 10 different 3D descriptors using a range of data sets. Useful for understanding these and other descriptors. For example, how (and in what) to compute them. For some they use Matlab, the rest PCL. The length of the resulting descriptors varies hugely, from 32 to 1980. Evaluate for their performance w.r.t. descriptiveness (using precision-recall curve), Compactness (using the average area under the precision-recall curve divided by the length of the descriptor), robustness (using the variation of the area under the precision-recall curve again a range of disturbances such as noise), scalability (by assessing the change of the area under the precision-recall curve as the data set increases) and efficiency (by assessing how long it takes to compute the descriptors against the number of point isn the model). Makes recommendations based on type of data set and task. Notes that none of the descriptors performs particularly well with low-cost low-resolution sensors. An annoying aspect of this paper is that the order of different evaluations, descriptors is not consistent. In Library.},
  file         = {GuoBSWK2016.pdf:3DDescriptors\\GuoBSWK2016.pdf:PDF},
  keywords     = {3D descriptors},
  owner        = {ISargent},
  creationdate    = {2016.04.26},
}

@InProceedings{Haala2013,
  Title                    = {Landscape of Dense Image Matching Algorithms},
  Author                   = {Norbert Haala},
  Booktitle                = {Report on the 2nd EuroSDR workshop on 'High Density Image Matching for DSM Computation' June 13th to 14th 2013},
  Year                     = {2013},
  Series                   = {EuroSDR},

  Keywords                 = {DSM},
  Owner                    = {ISargent},
  creationdate                = {2014.11.14},
  Url                      = {http://www.ifp.uni-stuttgart.de/publications/phowo13/240Haala-new.pdf}
}

@Article{HaalaK2010,
  author       = {Norbert Haala and Martin Kada},
  title        = {An update on automatic 3D building reconstruction},
  journaltitle = {ISPRS Journal of Photogrammetry and Remote Sensing},
  year         = {2010},
  volume       = {65},
  pages        = {570--580},
  comment      = {A very easy to read paper reviewing 3D building reconstruction. Read this again for rereferences to roof shape detection and extraction.},
  keywords     = {3D building},
  owner        = {ISargent},
  creationdate    = {2015.03.22},
}

@InBook{Haber1980,
  author       = {R.N. Haber},
  title        = {The Perception Pictures},
  chapter      = {Perceiving space from pictures: A theoretical analysis},
  editor       = {M.A. Hagen},
  pages        = {3--31},
  publisher    = {Academic Press},
  volume       = {1},
  address      = {New York},
  comment      = {Walters1987 references this paper in relation to: ``it is unlikley that the human visual system has developed separate visual processes to deal with line drawings''},
  creationdate = {2015.06.18},
  owner        = {ISargent},
  year         = {1980},
}

@Article{HainesYoungC96,
  author       = {Haines-Young, R. and M. Chopping},
  journaltitle = {Progress in Physical Geography},
  title        = {Quantifying landscape structure: a review of landscape indices and their application to forested landscapes},
  number       = {4},
  pages        = {418-445},
  url          = {http://www.nottingham.ac.uk/cem/pdf/Haines-Young_Chopping_1996.pdf},
  volume       = {20},
  comment      = {Review of landscape metrics specifically for forest management. ``This study... examines the way in which quantitative measures, or indices, can be used ... to assess the spatial implications of the various design guidelines... concludes progress has been made in the dvelopment of a range of landscape pattern measures...''. An area I was unaware of but these metrics could be more widely applicable. Quantifying the landscape is difficult and different studies have identified many different metrics. The metrics can be broadly grouped into area metrics, edge metrics, shape metrics, core area metrics, nearest neighbour metrics, diversity metrics and contagion and interspersion. More information about landscape metrics can be found in McGarigal2015.},
  creationdate = {2013.11.06},
  keywords     = {landscape, ImageLearn, regionalisation},
  owner        = {ISargent},
  year         = {1996},
}

@Book{HallPW2016,
  author    = {Patrick Hall and Wen Phan and Katie Whitson},
  date      = {5},
  title     = {The Evolution of Analytics: Opportunities and Challenges for Machine Learning in Business},
  comment   = {A short free eBook written well without sensation. Puts machine learning into a wider context of analytics as well as artificial intelligence and gives a short background to ML. A series of sections then describe three modern applications of machine learning: recommendation systems, streaming analytics, deep learning and cognitive computing. The third section considers 5 areas where the business will face challenges to the adoption of ML: organisational, data, infrastructure, modelling, and, operational and production. Organisational challenges include talent scarcity and lack of engagement - solutions include creating a centre of excellence that will then efficiently use analytic talent within the organisation; making analytics more approachable by providing approachable interfaces (and I would argue: data); employing a chief analytics officer who can champion analytics both in IT (usu realm of CIO) and business (usually the realm of CMO) and; having processes in place to enable a data-driven culture. Data challenges include various data quality problems, which can be fixed but the truth is that the majority of time on algorithm development is spent on preparing data and dealing with quality issues. Data security and governance is also important but there don't seem to be any clear approaches apart from addressing these are the start of a ML exercise. There are challenges around integrating different types of data, and feature extraction, selection and engineering can help this. And data need to be easy to explore so that data scientists can make effective decisions on which data to appropriately use. Infrastructure challenges are largely around the best way to store data and files, and compute power. These need to allow the data scientist to work efficiently and can be highly dynamic so designing in 'elasticity' will allow optimal use of resources. Machine learning models can be complex and difficult to interpret and this is a challenge to their adoption in some areas. Operational and production challenges arise when developed models needs to be deployed and consideration of where the data are stored and the programming language of the final model can make a significant difference when scaling up to production. Additionally, means to regularly or continually upgrade models are essential if accuracy of the outcome is to be maintained (or improved) over time and this can be acheived using either a 'challenger model' approach (whereby an alternative model is developed alongside the existing one) or online learning which is constantly updating with new data. Because operational systems are critical there is a need for tighter controls and governance but this must be balanced by the need to innovate and quickly evaluate new algorithms, libraries and tools. The final sections give a couple of case studies, firstly of a healthcare information company that analyses patient data consumer analysis company that enables decision making about customers for marketing, credit, etc. Useful things from these that I identified were: putting in feedback screens within the systems allowed operators/patients to update the system if the models had made inaccurate predictions, thereby improving the model; Equifax is a company that analyses billions of social media posts and relates this to buying behaviour; using Neural Decision technology Equifax is able to produce reason codes that explain the logic behind their model's (neural net) 'decision'; trying new things takes more time and its managements' responsibility to create space for this research and development to occur; the importance of preserving the experimental graduate schell mindset in the workplace.},
  keywords  = {Machine learning, AI, operational, MLStrat Programme},
  owner     = {ISargent},
  creationdate = {2016.10.19},
  year      = {2016},
}

@InProceedings{HarchaouiDPDM12,
  author       = {Zaid Harchaoui and Matthijs Douze and Mattis Paulin and Miroslav Dudik and Jerome Malick},
  booktitle    = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR-12)},
  title        = {Large-scale image classification with trace-norm regularization},
  url          = {http://hal.inria.fr/docs/00/72/83/88/PDF/hdpdm_2012_ultrarod.pdf},
  abstract     = {With the advent of larger image classification datasets such as ImageNet, designing scalable and efficient multi-class classification algorithms is now an important challenge. We introduce a new scalable learning algorithm for large-scale multi-class image classification, based on the multinomial logistic loss and the trace-norm regularization penalty. Reframing the challenging non-smooth optimization problem into a surrogate infinite-dimensional optimization problem with a regular l1 -regularization penalty, we propose a simple and provably efficient accelerated coordinate descent algorithm. Furthermore, we show how to perform efficient matrix computations in the compressed domain for quantized dense visual features, scaling up to 100,000s examples, 1,000s-dimensional features, and 100s of categories. Promising experimental results on the ``Fungus'', ``Ungulate'', and ``Vehicles'' subsets of ImageNet are presented, where we show that our approach performs significantly better than state-of-the-art approaches for Fisher vectors with 16 Gaussians.},
  comment      = {I came across this paper through the Microsoft Research website although it doesn't seem to credit MSR anywhere. ``Popular and efficient visual features include the low dimensional bag-of-visualwords (BOV) [5], Fisher vectors [22, 23], local coordinate coding [30] and supervector coding [31].'' About improving the efficiency of algorithms that classify images (not pixels or regions in images). A highly technical paper that i would struggle to follow.},
  creationdate = {2013.10.16},
  keywords     = {Machine Learning},
  owner        = {ISargent},
  year         = {2012},
}

@Article{HareSL2014,
  author       = {Jonathon S. Hare and Sina Samangooei and Paul H. Lewis},
  journaltitle = {Multimedia Tools and Applications},
  title        = {Practical scalable image analysis and indexing using Hadoop},
  number       = {3},
  pages        = {1215-1248},
  volume       = {71},
  comment      = {http://link.springer.com/article/10.1007%2Fs11042-012-1256-0#/page-1},
  creationdate = {2016.04.26},
  owner        = {ISargent},
  year         = {2014},
}

@InProceedings{Harley2015,
  author    = {Adam W Harley},
  title     = {An Interactive Node-Link Visualization of Convolutional Neural Networks},
  booktitle = {ISVC},
  year      = {2015},
  pages     = {867--877},
  comment   = {Visualising the structure of the network and the representation at each node for MNIST data. For teaching purposes I think.},
  owner     = {ISargent},
  creationdate = {2016.04.19},
}

@Article{HeZRS2015,
  author       = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  journaltitle = {arXiv:1502.01852v1},
  title        = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
  url          = {http://arxiv.org/pdf/1502.01852v1.pdf},
  comment      = {''Rectified activation units (rectifiers) are essential for state-of-the-art neural networks.'' ``the rectifier neuron e.g., Rectified Linear Unit (ReLU), is one of several keys to the recent success of deep networks It expedites convergence of the training procedure and leads to better solutions than conventional sigmoid-like units.'' Rectifiers are activation functions. These can be trained using the chain rule. Doesn't work with regularisation though because this tends to set gradient of negative part of activiation function to zero.},
  creationdate = {2015.02.27},
  keywords     = {deep learning, activation functions, MLStrat Milestones},
  owner        = {ISargent},
  year         = {2015},
}

@Article{HeZRS2015resnet,
  author       = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  title        = {Deep Residual Learning for Image Recognition},
  comment      = {this is the resnet paper. ``This reformulation is motivated by the counterintuitive phenomena about the degradation problem (Fig. 1, left). As
we discussed in the introduction, if the added layers can be constructed as identity mappings, a deeper model should have training error no greater than its shallower counterpart. The degradation problem suggests that the solvers might have difficulties in approximating identity mappings by multiple nonlinear layers.''},
  creationdate = {2017.05.28},
  journal      = {arXiv:1512.03385v1},
  keywords     = {deep learning, skip connections, MLStrat Milestones},
  owner        = {ISargent},
  year         = {2015},
}

@Article{Heimann2007,
  Title                    = {Three-dimensional linearised Euler model simulations of sound propagation in idealised urban situations with wind effects},
  Author                   = {Dietrich Heimann},
  Year                     = {2007},
  Pages                    = {217--237},
  Volume                   = {68},

  Abstract                 = {A three-dimensional numerical time-domain model based on the linearised Euler equation is applied to idealised urban situations with elongated, isolated buildings beside a straight street with sound emissions. The paper aims at the investigation of principle relationships between the source-receiver geometry (street and building facades) and sound propagation under the consideration of ground and wind. By applying cyclic lateral boundary conditions for either one or both horizontal co-ordinates, two different idealised urban environments were considered: a single street and parallel streets. Numerical experiments were performed to elaborate the effects of different roof types, ground properties, wind flow, and turbulence in both urban environments with the focus on the back facades (quiet sides) of the buildings. As a result it was found that the back facades of flat-roof buildings are quieter than those of hip roof buildings despite equal cross-cut areas. The wind effect (resulting in quieter upwind and louder downwind facades) is more pronounced for hip-roof buildings. In the case of parallel streets upwind facades are slightly louder than downwind facades because they are simultaneously exposed to downwind propagating sound from the next parallel street.},
  Journaltitle             = {Applied Acoustics},
  Keywords                 = {3D usage, 3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2015.01.28},
  Url                      = {http://elib.dlr.de/46076/1/sdarticle.pdf}
}

@Article{Hinton07,
  author       = {Hinton, Geoffrey. E},
  journaltitle = {Trends in Cognitive Sciences},
  title        = {Learning Multiple Layers of Representation},
  pages        = {428-434},
  url          = {http://www.cs.toronto.edu/~hinton/absps/tics.pdf},
  volume       = {11},
  comment      = {One of the key papers in deep learning but contains a lots I need to understand better. ``The hope is that the active features in the higher layers will be a much better guide to appropriate actions that the raw sensory data or the lower-level features''. Studies of the neocortex (gives refs) ``suggest a hierarchy of progressively more complex features in which each layer can influence the layer below''. Much of this is about generative models, which I understand to be models of the distribution of the observed data but I think i have a simplistic view. The key point is to learn a model that generates sensory data rather than classifying it, eliminating the need for large amounts of labelled data. This 'top down' process is complemented by a bottom up pass of recognition (e.g. discrimination). Generative models include factor analysis, independant components analysis and mixture models. I need to understand all 3 better. Describes how restricted Bolzmann machines can be composed to create a composite generative model and that each layer of representation is learned in turn (one layer at a time). this review focuses on images of handwritten digits but suggests applying RBMs to high-dimensional sequential data. I've finally started to understand the 'hidden variables', the variables that we are actually interested in but which we can only measure indirectly using sensory data. ``It seems most appropriate when hidden variables generate richly structured sensory data that provide plentiful information about the states of the hidden variables. If the hidden variables also generate a label that contains little information or is only occasionally observed, it is a bad idea to try to learn the mapping from sensory data to labels using discriminative learning methods. It is much more sensible first to learn a generative model that infers the hidden variables from the sensory data and then to learn the simpler mapping from the hidden variables to the labels''.},
  creationdate = {2013.10.09},
  keywords     = {Machine Learning, Deep Learning, representation learning, ImDeepLEAP, DeepLEAP},
  owner        = {ISargent},
  year         = {2007},
}

@Misc{HintonGoogleTechTalk07,
  author       = {Geoffrey E. Hinton},
  title        = {The Next Generation of Neural Networks},
  year         = {2007},
  howpublished = {Google Tech Talk, Video, YouTube},
  url          = {http://www.youtube.com/watch?v=AyzOUbkUf3M&noredirect=1},
  comment      = {Starts with background to NN, perceptrons (no adaptation), backprop, kernal methods (SVM) in which training example becomes a feature (optimisation throws away some of features and retains others and so its 'just a perceptron' and works better than back propogation. Backpropogations requires labelled data which doesn't match up to how the brain seems to be working. Instead of trying to learn the probability of the label given an image, learn the probabiliyt of the image. A generative model. Binary stochastic neurons are used in this model. Join these up into restricted boltzman machine which can only learn one layer of features. Networks are governed by an energy function.Weights determine energies linearly the probabilities are an exponential function of the energies (log-probability is a linear function of the weights). Simple algorithm for training RBM: maximum likelihood learning algorithm uses alternating Gibbs sampling: input activates feature detector on or off. Binary state of feature detectors then creates fantasy (reconstruct input) when in generative mode. Repeating the passing of input data then creating fantasies is called a Markov chain. Need update rule that says believe in the data rather than the fantasies. 'Measure how often a pixel i and a feature detector j are on together when I'm showing you the data vector v and then measure how often they're on together when the model is just fantasising and raise the weights by how often they're on together when its seeing data and lower the weights by how often they're on together when its fantasising. What that'll do will make it happier with the data (lower energy) and less happy with its fantasies and so its fantasies will gradually move towards the data. If its fantasies are just like the data, then these correlations (the correlation of pixel i and feature detector j being on together) in the fantasies will be the same as in the data and so it will stop learning'. ML learning is slow, needs to be run ~100 steps. Hinton made it run 100,000 times faster using greedy learning. Run for only 1 step rather than 100. Now the change in the weight is a learning rate times the difference between statistics measured with the data and statistics measured with reconstructions with the data. Its not ML learning but it works well. Deep network is trained by taking activations of features and making them data and performing the training again. It can be proved that with each layer we can a better model of the training data. Weights in RBM define prob of visible vector given hidden vector. Weights also define whole Markov chain in that they define probability distribution over hidden units i.e. prior over patterns of hidden activities. Generative model can be improved by fine-tuning. Discriminiative fine-tuning gives even better results i.e. label a few examples and use back propogation to to train to descriminate. Compression of data done using deep autoencoders which have RBMs each learned in turn and each with fewer neurons than previously. At e.g. 4th hidden layer transpose previous weights/layers (resulting in increasing number of neurons from now on) then use backpropogation which will slightly alter the transposed weights. Used highly non-linear transform to compress data. PCA would be linear version and results are much better than PCA. Gives interesting document analysis into which documents are transposed into 2D space - semantic hashing. Learn hash-functions to perform approximate matching and search for similar items. Works better than locality sensitive hashing because faster and more accurate. Next would like to perform make machine recognise objects in images (analogous to words in documents) so that we can then apply semantic hashing. Introduce lateral interaction between visible units (semi-restricted boltzman machine). Lateral interactions enable the representation of real imagery in which there tend to be not much happening then suddenly structure.},
  keywords     = {deep learning, Representation learning, ImageLearn},
  owner        = {ISargent},
  creationdate    = {2013.11.13},
}

@InBook{HintonKW2011,
  author    = {Geoffrey E. Hinton and Alex Krizhevsky and Sida D. Wang},
  title     = {Artificial Neural Networks and Machine Learning -- ICANN 2011},
  year      = {2011},
  editor    = {Timo Honkela and Wlodzislaw Duch and Mark Girolami and Samuel Kaski},
  volume    = {6791},
  series    = {Lecture Notes in Computer Science},
  publisher = {Springer},
  chapter   = {Transforming Auto-Encoders},
  pages     = {44-51},
  url       = {http://www.cs.toronto.edu/~fritz/absps/transauto6.pdf},
  comment   = {Paper argues that convolution neural nets are misguided because they throw away information about pose (pooling creates translation invariance). Offer instead a 'capsule' model in which the pose information is explicit as 'instantiation parameters' and these are learned alongwith a probability of the presence of the visual entity as 'recognition units'. Observes that biological cognitive systems have access to non-visual access to transformation such as saccade. If image is translated in a known way this known translation should be part of the input. Capsules are apparently easy to learn, however, each capsule can represent only one instance of a visual entity at one time. Suggest that this model would make 3D visual understanding easier to learn. Test with MNIST data (30 capsules) as well as computer generated stereo pair images of cars (900 capsules). Instantiation parameters can incorporate translation, rotation, illumination, deformation, ...},
  keywords  = {ImageLearn},
  owner     = {ISargent},
  creationdate = {2015.06.24},
}

@Article{HintonOT06,
  author       = {Hinton, G. E. and Osindero, S. and Teh, Y.},
  journaltitle = {Neural Computation},
  title        = {A fast learning algorithm for deep belief nets},
  pages        = {1527--1554},
  url          = {http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf},
  volume       = {18},
  abstract     = {We show how to use ``complementary priors'' to eliminate the explaining away effects that make inference difficult in densely-connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory . The fast, greedy algorithm is used to initialize as lower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modelled by long ravines in the free-energy landscape of the top-level associative memory and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind},
  comment      = {Possibly the first paper of training deep networks one layer at a time. Useful hints from google+ deep learning group on understanding this paper: \url{https://plus.google.com/u/0/108611294554440482097/posts/KmsA6182SQQ?cfem=1}},
  creationdate = {2013.11.20},
  keywords     = {Representation learning, deep learning, ImageLearn, DeepLEAP, unsupervised, MLStrat Milestones},
  owner        = {ISargent},
  year         = {2006},
}

@Article{HintonS06,
  author       = {Hinton, G. E. and Salakhutdinov, R. R.},
  title        = {Reducing the Dimensionality of Data with Neural Networks},
  journaltitle = {Science},
  year         = {2006},
  volume       = {313},
  month        = {7},
  pages        = {504-507},
  url          = {http://www.cs.toronto.edu/~hinton/science.pdf},
  comment      = {Paper on building a deep(ish) autoencoder using RBMs. Lovely figures showing the separation of classes achieved with these autoencoders.},
  keywords     = {Deep Learning, Machine Learning, autoencoding, feature extraction, ImageLearn},
  owner        = {ISargent},
  creationdate    = {2014.02.10},
}

@InBook{HintonS1986,
  author       = {G. E. Hinton and T. J. Sejnowski},
  title        = {Parallel distributed processing: explorations in the microstructure of cognition},
  chapter      = {Learning and relearning in Boltzmann machines},
  pages        = {282-317},
  volume       = {1},
  comment      = {''Every robust high-level property must be implemented by the combined effect of many local components, and no single component must be crucial for the realization of the high-level property. This makes distributed representations (see Chapter 3) a natural choice when designing a damage-resistant system.''},
  creationdate = {2015.07.08},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  year         = {1986},
}

@Online{HistoricEnglandLandscapeCharacterisationWebpage,
  author           = {{Historic England}},
  title            = {Historic Landscape Characterisation},
  url              = {https://www.historicengland.org.uk/research/methods/characterisation-2/},
  comment          = {''Historic Characterisation involves applying to aspects of landscape a long-established archaeological and historical method, the classifying and interpreting of material through identifying and describing essential or distinguishing patterns, features and qualities, or attributes. The sources used when doing this are comprehensive and systematic, like modern and historic maps or aerial photographs.''},
  creationdate     = {2017.04.04},
  keywords         = {Landscape, Characterisation, ImageLearn},
  modificationdate = {2023-12-06T08:02:23},
  owner            = {ISargent},
  year             = {2017},
}

@InCollection{HochreiterBFS2001,
  author               = {Hochreiter, S. and Bengio, Y. and Frasconi, P. and Schmidhuber, J.},
  title                = {Gradient flow in recurrent nets: the difficulty of learning long-term dependencies},
  booktitle            = {A Field Guide to Dynamical Recurrent Neural Networks},
  year                 = {2001},
  editor               = {Kremer, S. C. and Kolen, J. F.},
  publisher            = {IEEE Press},
  added-at             = {2008-02-26T12:05:08.000+0100},
  biburl               = {http://www.bibsonomy.org/bibtex/279df6721c014a00bfac62abd7d5a9968/schaul},
  citeulike-article-id = {2374777},
  comment              = {the vanishing gradient problem in backpropagation},
  creationdate           = {2016-12-13 13:58:41 +0000},
  date-modified        = {2016-12-13 14:00:41 +0000},
  description          = {idsia},
  interhash            = {485c1bd6a99186c9414c6b9ddaed42c9},
  intrahash            = {79df6721c014a00bfac62abd7d5a9968},
  keywords             = {daanbib},
  owner                = {ISargent},
  creationdate            = {2008-02-26T12:07:01.000+0100},
}

@InProceedings{HoffmanRDDS13,
  author    = {Judy Hoffman and Erik Rodner and Jeff Donahue and Trevor Darrell and Kate Saenko},
  title     = {Efficient Learning of Domain-invariant Image Representations},
  booktitle = {International Conference on Learning Representations},
  year      = {2013},
  comment   = {In case where there is a transform between the training data and test data (e.g. images used later on have different lighting, are from different camera or different context to earlier data). field is call Domain Adaptation and includes feature transformation, manifold distance and parameter adaptation.},
  keywords  = {Machine learning, ImageLearn},
  owner     = {ISargent},
  creationdate = {2014.05.22},
}

@Article{HollandPCH2016,
  author       = {Holland, David A. and Pook, Claire and Capstick, Dave and Hemmings, Angus},
  title        = {The Topographic Data Deluge - Collecting and Maintaining Data in a 21ST Century Mapping Agency},
  pages        = {727-731},
  abstract     = {In the last few years, the number of sensors and data collection systems available to a mapping agency has grown considerably. In the field, in addition to total stations measuring position, angles and distances, the surveyor can choose from hand-held GPS devices, multi-lens imaging systems or laser scanners, which may be integrated with a laptop or tablet to capture topographic data directly in the field. These systems are joined by mobile mapping solutions, mounted on large or small vehicles, or sometimes even on a backpack carried by a surveyor walking around a site. Such systems allow the raw data to be collected rapidly in the field, while the interpretation of the data can be performed back in the office at a later date. In the air, large format digital cameras and airborne lidar sensors are being augmented with oblique camera systems, taking multiple views at each camera position and being used to create more realistic 3D city models. Lower down in the atmosphere, Unmanned Aerial Vehicles (or Remotely Piloted Aircraft Systems) have suddenly become ubiquitous. Hundreds of small companies have sprung up, providing images from UAVs using ever more capable consumer cameras. It is now easy to buy a 42 megapixel camera off the shelf at the local camera shop, and Canon recently announced that they are developing a 250 megapixel sensor for the consumer market. While these sensors may not yet rival the metric cameras used by today's photogrammetrists, the rapid developments in sensor technology could eventually lead to the commoditization of high-resolution camera systems. With data streaming in from so many sources, the main issue for a mapping agency is how to interpret, store and update the data in such a way as to enable the creation and maintenance of the end product. This might be a topographic map, ortho-image or a digital surface model today, but soon it is just as likely to be a 3D point cloud, textured 3D mesh, 3D city model, or Building Information Model (BIM) with all the data interpretation and modelling that entails. In this paper, we describe research/investigations into the developing technologies and outline the findings for a National Mapping Agency (NMA). We also look at the challenges that these new data collection systems will bring to an NMA, and suggest ways that we may work to meet these challenges and deliver the products desired by our users.},
  creationdate = {2016.11.22},
  journal      = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  keywords     = {DeepLEAP, MLStrat Data},
  month        = {6},
  owner        = {ISargent},
  year         = {2016},
}

@Article{HollandGSHGF2012,
  author           = {David Holland and Catherine Gladstone and Isabel Sargent and Jon Horgan and Andrew Gardiner and Mark Freeman},
  date             = {2012},
  title            = {Automating the Photogrammetric Workflow in a National Mapping Agency},
  url              = {http://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/I-4/83/2012/isprsannals-I-4-83-2012.pdf},
  volume           = {I-4},
  abstract         = {The goal of automating the process of identifying changes to topographic features in aerial photography, extracting the geometry of these features and recording the changes in a database, is yet to be fully realised. At Ordnance Survey, Britain's national mapping agency, research into the automation of these processes has been underway for several years, and is now beginning to be implemented in production systems. 
At the start of the processing chain is the identification of change - new buildings and roads being constructed, old structures demolished, alterations to field and vegetation boundaries and changes to inland water features. Using eCognition object-based image analysis techniques, a system has been developed to detect the changes to features. This uses four-band digital imagery (red, green, blue and near infra-red), together with a digital surface model derived by image matching, to identify all the topographic features of interest to a mapping agency. Once identified, these features are compared with those in the National Geographic Database and any potential changes are highlighted. These changes will be presented to photogrammetrists in the production area, who will rapidly assess whether or not the changes are real. If the change is accepted, they will manually capture the geometry and attributes of the feature concerned. The change detection process, although not fully automatic, cuts down the amount of time required to update the database, enabling a more efficient data collection workflow. Initial results, on the detection of changes to buildings only, showed a completeness value (proportion of the real changes that were found) of 92\% and a correctness value (proportion of the changes found that were real changes) of 22\%, with a time saving of around 50\% when compared with the equivalent manual process. The completeness value is similar to those obtained by the manual process. Further work on the process has added vegetation, water and many other rural features to the list of features that can be detected, and the system is currently being evaluated in a production environment.
In addition to this work, the research team at Ordnance Survey are working with the remote sensing (data collection) department to develop more efficient methods of DSM and DTM creation; more automated seamline-generation for the creation of orthoimage mosaics; and methods to automatically record simple building heights on buildings in the database. These are all methods that have been proven in a research environment - the challenge is to implement them within the working environment of the existing data collection process.},
  address          = {Melbourne, Australia},
  booktitle        = {XXII ISPRS Congress},
  creationdate     = {2016.11.22},
  journal          = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  keywords         = {DeepLEAP},
  modificationdate = {2023-12-05T13:10:12},
  owner            = {ISargent},
  year             = {2012},
}

@Article{HornikFKB2012,
  author       = {Kurt Hornik and Ingo Feinerer and Martin Kober and Christian Buchta},
  title        = {Spherical k-Means Clustering},
  journaltitle = {Journal of Statistical Software},
  year         = {2012},
  url          = {http://bach-s49.wu-wien.ac.at/4000/1/paper.pdf},
  comment      = {Makes spherical k-means much simpler to understand - its just normalising the data before starting and then normalising the clusters at each iteration.},
  owner        = {ISargent},
  creationdate    = {2015.06.08},
}

@Book{Hoskins2013,
  author    = {W. G. Hoskins},
  title     = {The Making of the English Landscape},
  year      = {2014},
  note      = {First published 1955},
  publisher = {Little Toller Books},
  comment   = {A detailed description of the English landscape from the perspective of the societies and their processes that formed it.},
  keywords  = {landscape, ImageLearn},
  owner     = {ISargent},
  creationdate = {2015.07.30},
}

@InProceedings{HowarthR2004,
  author       = {Peter Howarth and Stefan R\''{u}ger},
  booktitle    = {International Conference on Image and Video Retrieval, CIVR},
  title        = {Evaluation of Texture Features for Content-Based Image Retrieval},
  pages        = {326--334},
  creationdate = {2017.05.30},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  year         = {2004},
}

@Article{HuXHZ2015,
  author       = {Fan Hu and Gui-Song Xia and Jingwen Hu and Liangpei Zhang},
  journaltitle = {Remote Sensing},
  title        = {Transferring Deep Convolutional Neural Networks for the Scene Classification of High-Resolution Remote Sensing Imagery},
  doi          = {10.3390/rs71114680},
  number       = {11},
  pages        = {14680--14707},
  url          = {http://www.mdpi.com/2072-4292/7/11/14680},
  volume       = {7},
  comment      = {Deep networks and remote sensing. Considers not only the features/representations extracted at the final fully connected layers but also the convolutional layers. Also input multiple scales of the original image. The input images are subsets of remotely sensed imagery that have a single label e.g. beach, debnse residential, football field. There are two data sets of these (UC Merced and WHU-RS). Thus unlike ImageLearn the images are still foreground/background type. Discuss a number of deep networks, most of which are based on AlexNet. Use pre-trained networks. ``For HRRS (hi res remote sensing) scene datasets, due to their more generic ability, CNN features form the first FC layes consistently work better than those from the second FC layers that are widely used in many works''.},
  creationdate = {2016.05.09},
  keywords     = {ImageLearn, Spatial Scale, Remote Sensing, DeepLEAP, MLStrat DLRS},
  owner        = {ISargent},
  year         = {2015},
}

@InProceedings{HuangL06,
  author    = {Huang, Fu Jie and Yann LeCun},
  title     = {Large-scale Learning with SVM and Convolutional Nets for Generic Object Categorization},
  booktitle = {Proceedings of Computer Vision and Pattern Recognition Conference},
  year      = {2006},
  comment   = {A paper that investigates the good and bad points of Support Vector Machines and Convolution Neural Networks for object classification and combines the two. Effectively, I think, the CNN acts as a feature extractor for the SVM, an improved kernal for the SVM. The results are much better than for SVM alone and a little better than for CNN alone. Data set used is of 50 'toys' which are centrally placed in training/testing images. In the jittered-cluttered set the object part of the images were altered and placed into a background taken from the real-world. Useful for explanations of SVM and CNN.},
  keywords  = {Machine Learning, Classification, ImageLearn},
  owner     = {isargent},
  creationdate = {2013.08.08},
}

@InProceedings{HuangBS2011,
  author    = {Hai Huang and Claus Brenner and Monika Sester},
  booktitle = {GIS '11 Proceedings of the 19th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
  title     = {3D building roof reconstruction from point clouds via generative models},
  pages     = {16--24},
  abstract  = {This paper presents a generative statistical approach to 3D building roof reconstruction from airborne laser scanning point clouds. In previous works bottom-up methods, e.g., points clustering, plane detection, and contour extraction, are widely used. Since the laser scanning data of urban scenes often contain extra structures and artefacts due to tree clutter, reflection from windows, water features, etc., bottom-up reconstructions may result in a number of incomplete or irregular roof parts. We propose a new top-down statistical method for roof reconstruction, in which the bottom-up efforts mentioned above are no more required. Based on a predefined primitive library we conduct a generative modeling to construct the target roof that fit the data. Allowing overlapping, primitives are assembled and, if necessary, merged to present the entire roof. The selection of roof primitives, as well as the sampling of their parameters, is driven by the Reversible Jump Markov Chain Monte Carlo technique. Experiments are performed on both low-resolution (1m) and high-resolution (0.18m) data-sets. For high-resolution data we also show the possibility to reconstruct smaller roof features, such as chimneys and dormers. The results show robustness despite the clutter and flaws in the data points and plausibility in reconstruction.},
  keywords  = {3DCharsPaper},
  owner     = {ISargent},
  creationdate = {2014.10.28},
  year      = {2011},
}

@Article{HubelW59,
  author       = {Hubel, D. H. and Wiesel, T. N.},
  journaltitle = {Journal of Physiology},
  title        = {Receptive Fields of Single Neurones in the Cat's Striate Cortex},
  number       = {148},
  pages        = {574-591},
  comment      = {Key paper reporting findings about function of cells in cat visual cortex. From wikipedia: ``A simple cell in the primary visual cortex is a cell that responds primarily to oriented edges and gratings (bars of particular orientations). These cells were discovered by Torsten Wiesel and David Hubel in the late 1950s.'' Simple cells (S-Cells) repond predictably to given visual stimuli. Other cells, that were termed complex cells or C-Cells do not respond predictably to visual stimuli. Suggested that C-cells are of a higher order and gain input from a number rof S-cells. (Wikipedia: These S-cells extract local features and C-Cells add tolerance to deformation. )},
  creationdate = {2014.01.14},
  keywords     = {Neuroscience, ImageLearn},
  owner        = {isargent},
  year         = {1959},
}

@Article{HungXS2014,
  author       = {Hung, Calvin and Xu, Zhe and Sukkarieh, Salah},
  journaltitle = {Remote Sensing},
  title        = {Feature Learning Based Approach for Weed Classification Using High Resolution Aerial Images from a Digital Camera Mounted on a UAV},
  doi          = {10.3390/rs61212037},
  issn         = {2072-4292},
  number       = {12},
  pages        = {12037},
  url          = {http://www.mdpi.com/2072-4292/6/12/12037},
  volume       = {6},
  abstract     = {The development of low-cost unmanned aerial vehicles (UAVs) and light weight imaging sensors has resulted in significant interest in their use for remote sensing applications. While significant attention has been paid to the collection, calibration, registration and mosaicking of data collected from small UAVs, the interpretation of these data into semantically meaningful information can still be a laborious task. A standard data collection and classification work-flow requires significant manual effort for segment size tuning, feature selection and rule-based classifier design. In this paper, we propose an alternative learning-based approach using feature learning to minimise the manual effort required. We apply this system to the classification of invasive weed species. Small UAVs are suited to this application, as they can collect data at high spatial resolutions, which is essential for the classification of small or localised weed outbreaks. In this paper, we apply feature learning to generate a bank of image filters that allows for the extraction of features that discriminate between the weeds of interest and background objects. These features are pooled to summarise the image statistics and form the input to a texton-based linear classifier that classifies an image patch as weed or background. We evaluated our approach to weed classification on three weeds of significance in Australia: water hyacinth, tropical soda apple and serrated tussock. Our results showed that collecting images at 5-10 m resulted in the highest classifier accuracy, indicated by F1 scores of up to 94%.},
  comment      = {Apply Convnets to imagery from UAVs to detect invasive weeds.},
  creationdate = {2016.09.21},
  keywords     = {ImageLearn, Remote Sensing, DeepLEAP},
  year         = {2014},
}

@TechReport{InspireBuildingsDraft,
  Title                    = {Data Specification on Buildings -- Draft Technical Guidelines},
  Author                   = {{INSPIRE Thematic Working Group buildings}},
  Institution              = {INSPIRE Infrastructure for Spatial Information in Europe, European Commission},
  Year                     = {2013},
  Note                     = {http://inspire.ec.europa.eu/documents/Data_Specifications/INSPIRE_DataSpecification_BU_v3.0rc3.pdf},
  Type                     = {Draft Technical Guidelines Annex II \& III},

  Keywords                 = {3DCharsPaper},
  Owner                    = {isargent},
  creationdate                = {2013.11.07},
  Url                      = {http://inspire.ec.europa.eu/documents/Data_Specifications/INSPIRE_DataSpecification_BU_v3.0rc3.pdf}
}

@TechReport{INSPIREBuildings2012,
  author      = {{INSPIRE Thematic Working Group Buildings}},
  title       = {D2.8.{III}.2 {INSPIRE} Data Specification on Buildings -- Draft Guidelines},
  institution = {Directive 2007/2/EC of the European Parliament and of the Council of 14 March 2007 establishing an Infrastructure for Spatial Information in the European Community (INSPIRE)},
  year        = {2012},
  url         = {http://inspire.ec.europa.eu/documents/Data_Specifications/INSPIRE_DataSpecification_BU_v3.0rc2.pdf},
  comment     = {Buildings - there are core 2D and 3D specifications and extended 2D and 3D specifications. The intention is that mapping agencies work towards the extended spec.Different heights for buildings are possible, see \url{http://inspire.ec.europa.eu/codeList/ElevationReferenceValue/} for the ElevationreferenceValues.},
  keywords    = {3DCharsPaper},
  owner       = {ISargent},
  publisher   = {{INSPIRE} Thematic Working Group Buildings},
  series      = {{INSPIRE} Infrastructure for Spatial Information in Europe},
  creationdate   = {2015.11.07},
}

@Article{IttiK2000,
  author       = {Laurent Itti and Christof Koch},
  title        = {A saliency-based search mechanism for overt and covert shifts of visual attention},
  journaltitle = {Vision Research},
  year         = {2000},
  volume       = {40},
  pages        = {1489--1506},
  comment      = {The model of attention based on visual saliency.},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  creationdate    = {2015.07.16},
}

@InProceedings{JarrettKRL09,
  author       = {Jarrett, K. and Kavukcuoglu, K. and Ranzato, M.A. and LeCun, Y},
  booktitle    = {ICCV},
  title        = {What is the best multi-stage architecture for object recognition?},
  url          = {http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf},
  comment      = {An interesting paper although its quite hard to follow, containing a lot of 'jargon'. Seems to be comparing 'hard-wired' filters with learned representations (using sparse coding). Also comparing one layer of feature extraction with two layers in which the second layer extracts features from the first. `` Also comparing supervised with unsupervised training. Local Contrast Normalization Layer - This module performs local subtractive and divisive normalizations, enforcing a sort of local competition between adjacent features in a feature map, and between features at the same spatial location in different feature maps''},
  creationdate = {2013.09.16},
  keywords     = {deep learning, ImageLearn},
  owner        = {ISargent},
  year         = {2009},
}

@Article{JasiewiczaS2013,
  author       = {Jarek Jasiewicza and Tomasz F. Stepinski},
  title        = {Geomorphons - a pattern recognition approach to classification and mapping of landforms},
  journaltitle = {Geomorphology},
  year         = {2013},
  volume       = {182},
  pages        = {147--156},
  url          = {http://geomorphometry.org/system/files/StepinskiJasiewicz2011geomorphometry.pdf},
  comment      = {This is the key paper about geomorphons, how they are calculated and what they can do.},
  owner        = {ISargent},
  creationdate    = {2014.08.01},
}

@Article{JazayeriRK2014,
  author       = {Jazayeri, I. and Rajabifard, A. and Kalantari, M.},
  title        = {A geometric and semantic evaluation of 3D data sourcing methods for land and property information},
  journaltitle = {Land Use Policy},
  year         = {2014},
  volume       = {36},
  pages        = {219--230},
  comment      = {Paper looking at methods of creating/capturing 3D data. Suggests that data sourcing is 5-dimensional: legal information, land-use, temporal information, geometric information and semantic information. Focuses on the last 2. Details many different methods that have been applied in other works to build 3D models and tables these, separating range-based (laser scanning) and image-based (terrestrial and aerial photogrammetry, satelling, UAV and mobile mapping). A useful resource for examples of these different methods.},
  keywords     = {3DCharsPaper},
  owner        = {ISargent},
  creationdate    = {2015.11.05},
}

@InProceedings{Jiang2009,
  author       = {X. Jiang},
  booktitle    = {2009 2nd IEEE International Conference on Computer Science and Information Technology},
  title        = {Feature extraction for image recognition and computer vision},
  doi          = {10.1109/ICCSIT.2009.5235014},
  pages        = {1-15},
  comment      = {''The goal of feature extraction is to yield a pattern representation that makes the classification trivial. Conversely, a good classification design would not need the help of a sophisticated feature extraction. In fact, there is no conceptually clear boundary between feature extraction and classification.'' (DudaHS2000 say somehting very similar to this). Seems to consider FE as a dimensionality reduction operation, but this is to achieve the primary objective of ``facilitat[ing] a more reliable and more accurate classification''. ``This paper explores and studies some advanced feature extraction approaches that are based on: the human expert knowledge; the image local and global structures; and the machine learning from image database.'' Doesn't go beyond PCA and discriminant analysis for the machine learning approaches.},
  creationdate = {2017.04.12},
  keywords     = {feature extraction},
  month        = {8},
  owner        = {ISargent},
  year         = {2009},
}

@InBook{JiangYY2012,
  author       = {Jiang, Yuning and Yuan, Junsong and Yu, Gang},
  title        = {Computer Vision -- ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part II},
  chapter      = {Randomized Spatial Partition for Scene Recognition},
  doi          = {10.1007/978-3-642-33709-3_52},
  editor       = {Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
  isbn         = {978-3-642-33709-3},
  pages        = {730--743},
  publisher    = {Springer Berlin Heidelberg},
  url          = {http://dx.doi.org/10.1007/978-3-642-33709-3_52},
  address      = {Berlin, Heidelberg},
  comment      = {From CastelluccioPSV2015: ``perform a randomized spatial partition (RSP), aiming at a better characterization of the spatial layout of the images''.},
  creationdate = {2016.05.11},
  keywords     = {ImageLearn, Spatial Scale},
  owner        = {ISargent},
  year         = {2012},
}

@Article{JochemHRP2009,
  author       = {Andreas Jochem and Bernhard HÃƒÂ¶fle and Martin Rutzinger and Norbert Pfeifer},
  title        = {Automatic Roof Plane Detection and Analysis in Airborne Lidar Point Clouds for Solar Potential Assessment},
  journaltitle = {Sensors},
  year         = {2009},
  volume       = {9},
  number       = {7},
  pages        = {5241-5262},
  url          = {http://www.mdpi.com/1424-8220/9/7/5241/htm},
  comment      = {Classic fitting of roof planes to lidar data.},
  keywords     = {3D buildings},
  owner        = {ISargent},
  creationdate    = {2014.10.28},
}

@Misc{WinGamma09,
  author       = {Antonia Jones},
  title        = {Gamma Test References to January 2009},
  howpublished = {WWW},
  url          = {http://users.cs.cf.ac.uk/O.F.Rana/Antonia.J.Jones/GammaArchive/IndexPage.htm},
  comment      = {WinGamma generates GammaTest results for input-output data. These data must be continuous. It assumes there is a relationship between the inputs and outputs and determines the noise, even if the relationship is complex. As far as i can understand, this software can test for different models of relationships and indicate whether the data fit a model with low noise. It probably only useful if the user knows that somehow the outputs can be predicted from the inputs (example project was predicting downstream water levels from variable rainfall and outflow events in thames basin). ``This software is particularly useful for time series prediction and dynamic system control applications, but also has a wide range of other applications.''},
  creationdate = {2013.12.11},
  owner        = {ISargent},
  year         = {2009},
}

@InProceedings{jones-rosin-slade:2014:VL,
  author    = {Jones, Chris and Rosin, Paul and Slade, Jonathan},
  title     = {Semantic and geometric enrichment of 3D geo-spatial models with captioned photos and labelled illustrations},
  booktitle = {Proceedings of the Third Workshop on Vision and Language},
  year      = {2014},
  publisher = {Dublin City University and the Association for Computational Linguistics},
  month     = {8},
  pages     = {62--67},
  url       = {http://www.aclweb.org/anthology/W14-5409},
  address   = {Dublin, Ireland},
  comment   = {Cardiff's paper introducing Jon's work.},
  keywords  = {3D buildings, semantic, 3DCharsPaper},
  owner     = {ISargent},
  creationdate = {2014.11.04},
}

@Article{JonesDBSXX,
  author       = {Krista Jones and Rodolphe Devillers and Yvan BÃƒÂ©dard and Olaf Schroth},
  title        = {Visualizing perceived spatial data quality of 3D objects within virtual globes},
  journaltitle = {International Journal of Digital Earth},
  year         = {in prep?},
  doi          = {10.1080/17538947.2013.783128},
  comment      = {useful paper for review of quality measures of 3D data. Purpose is to provide reviews of models in 'virtual globes' (e.g. google Earth) where formal QA is not available. Trial a range of ways of visualising votes and find that users prefer the 'number in star' visualisation in which a star has the number (out of ?5) that the model is rated at.},
  owner        = {ISargent},
  creationdate    = {2015.03.17},
}

@Article{KabolizadeEM2012,
  author       = {Mostafa Kabolizade and Hamid Ebadi and Ali Mohammadzadeh},
  journaltitle = {International Journal of Applied Earth Observation and Geoinformation},
  title        = {Design and implementation of an algorithm for automatic 3D reconstruction of building models using genetic algorithm},
  pages        = {104--114},
  volume       = {19},
  comment      = {'' In this paper, a reconstruction method based on genetic algorithms (GA) is presented by optimizing height and slopes of gable roof of building models. The proposed algorithm consists of three steps; initial building boundaries are detected in the first step. Then, in extraction step, in order to improve the accuracy of detection step, initial building contours are generalized and buildings are extracted. Finally and in reconstruction step, a GA-based method is used for reconstructing the building models. Also, the method has proved to be computationally efficient, and the reconstructed models have an acceptable accuracy. Examination of the results shows that the reconstructed buildings from complex study areas that uses the proposed method have root mean square error (RMSE) of 0.1 m.''},
  creationdate = {2014.10.28},
  month        = {10},
  owner        = {ISargent},
  year         = {2012},
}

@Article{KanazawaSJ14,
  author       = {Angjoo Kanazawa and Abhishek Sharma and David W. Jacobs},
  journaltitle = {CoRR},
  title        = {Locally Scale-Invariant Convolutional Neural Networks},
  url          = {http://arxiv.org/abs/1412.5104},
  volume       = {abs/1412.5104},
  bibsource    = {dblp computer science bibliography, \url{http://dblp.org}},
  biburl       = {http://dblp.uni-trier.de/rec/bib/journals/corr/KanazawaSJ14},
  comment      = {''we present a simple model that allows ConvNets to learn features in a locally scale-invariant manner without increasing the number of model parameters. ``},
  creationdate = {2015.06.01},
  keywords     = {ImageLearn, Spatial Scale, CNN},
  owner        = {ISargent},
  year         = {2014},
}

@Article{KapoorCLK14,
  author       = {Ashish Kapoor and Caicedo, Juan C. and Dani Lischinski and Sing Bing Kang},
  journaltitle = {International Journal of Computer Vision},
  title        = {Collaborative Personalization of Image Enhancement},
  url          = {http://link.springer.com/article/10.1007%2Fs11263-013-0675-3},
  abstract     = {This paper presents methods for personalization of image enhancement, which could be deployed in photo editing software and also in cloud-based image sharing services. We observe that users do have different preferences for enhancing images and that there are groups of people that share similarities in preferences. Our goal is to predict enhancements for novel images belonging to a particular user based on her specific taste, to facilitate the retouching process on large image collections. To that end, we describe an enhancement framework that can learn user preferences in an individual or collaborative way. The proposed system is based on a novel interactive application that allows to collect user's enhancement preferences. We propose algorithms to predict personalized enhancements by learning a preference model from the provided information. Furthermore, the algorithm improves prediction performance as more enhancement examples are progressively added. We conducted experiments via Amazon Mechanical Turk to collect preferences from a large group of people. Results show that the proposed framework can suggest image enhancements more targeted to individual users than commercial tools with global auto-enhancement functionalities.},
  creationdate = {2014.01.17},
  file         = {KapoorCLK14.pdf:MachineLearning\\KapoorCLK14.pdf:PDF},
  keywords     = {machine learning, clustering},
  month        = {12},
  owner        = {ISargent},
  year         = {2013},
}

@InProceedings{KaramshukNSNM13,
  author    = {Karamshuk, Dmytro and Noulas, Anastasios and Scellato, Salvatore and Nicosia, Vincenzo and Mascolo, Cecilia},
  title     = {Geo-spotting: mining online location-based services for optimal retail store placement.},
  booktitle = {KDD},
  year      = {2013},
  editor    = {Dhillon, Inderjit S. and Koren, Yehuda and Ghani, Rayid and Senator, Ted E. and Bradley, Paul and Parekh, Rajesh and He, Jingrui and Grossman, Robert L. and Uthurusamy, Ramasamy},
  publisher = {ACM},
  isbn      = {978-1-4503-2174-7},
  pages     = {793-801},
  url       = {http://arxiv.org/pdf/1306.1704v1.pdf},
  comment   = {Used foursquare check ins to determine optimum locations for retail outlets. See slides on \url{http://datasciencelondon.org/geo-spotting-mining-online-location-based-services-for-optimal-retail-store-placement/}.},
  keywords  = {dblp},
  owner     = {ISargent},
  creationdate = {2014.02.07},
}

@Article{KarantzalosP2010,
  author       = {Konstantinos Karantzalos and Nikos Paragios},
  title        = {Large-Scale Building Reconstruction Through Information Fusion and 3-D Priors},
  journaltitle = {IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING},
  year         = {2010},
  url          = {http://vision.mas.ecp.fr/papers/karantzalos-tgars-10.pdf},
  comment      = {References SargentHF07 - when referring to the quality assessment of 3D data.},
  keywords     = {3D},
  owner        = {ISargent},
  creationdate    = {2015.03.17},
}

@Misc{Karpathy2014,
  author       = {Andrej Karpathy},
  title        = {Feature Learning Escapades},
  howpublished = {Andrej Karpathy blog},
  url          = {http://karpathy.github.io/2014/07/03/feature-learning-escapades/},
  comment      = {Interesting blog detailing authors experiences working at Google and Stanford on a range of learning projects including Google Brain, AlexNet and 3D object extraction. Seems to conclude that the focus should be on learning with labelled data rather than using unsupervised approaches (c.f. Hinton2007). Me to ImageLearn team:''yes 3D is definitely a way to go, but adds complication. I did have a play with the PCL a couple of years ago and discovered our automatically generated point clouds were very noisy. They are apparently better now. Should you have a student looking for a project, applying the object segmentation techniques to our pointclouds would be very interesting.

My opinion: ``Interesting view about focussing on labels. What about unexpected and rare objects? Things trained on ImageNet don't detect people. You are a washing machine, David. The analogy to flashing random images at a toddler (actually, it would need to be a baby) is a good one. Although this may be more appropriate to efforts like ImageNet, cifar than to e.g. MNIST or perhaps our work because the subject of these latter two is more constrained.''},
  creationdate = {2015.06.30},
  keywords     = {ImageLearn, Representation Learning},
  owner        = {ISargent},
  year         = {2014},
}

@Article{KarpathyJL2015,
  author    = {Andrej Karpathy and Justin Johnson and Fei{-}Fei Li},
  title     = {Visualizing and Understanding Recurrent Networks},
  journal   = {CoRR},
  year      = {2015},
  volume    = {abs/1506.02078},
  url       = {http://arxiv.org/abs/1506.02078},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/KarpathyJL15},
  comment   = {Use T-SNE to visualise their Recurrent Neural Networks},
  keywords  = {ImageLearn, visualisation, TopoNet Metrics},
  owner     = {ISargent},
  creationdate = {2017.05.30},
}

@PhdThesis{Klckner11,
  author    = {Stefan Kluckner},
  title     = {Semantic Interpretation of Digital Aerial Images Utilizing Redundancy, Appearance and 3D Information},
  year      = {2011},
  abstract  = {One of the fundamental problems in the area of digital image processing is the automated and detailed understanding of image contents. This task of automated image understanding covers the explanation of images through assigning a semantic class label to mapped objects, as well as the determination of their extents, locations and relationships. Occlusions, changes of illumination, viewpoint and scale, natural or man-made structures, shape-defined objects or formless, homogeneous and highly-textured materials complicate the task of semantic image interpretation. While purely appearance-driven approaches obtain reliable interpretation results on benchmark datasets, there is a trend toward a compact integration of visual cues and 3D scene geometry. Accurately estimating 3D structure from images has already reached a state of maturity, mainly due to the cheap acquisition of highly redundant data and parallel hardware. Particularly, scene information taken from multiple viewpoints results in dense 3D structures that describe the 3D shapes of the mapped objects. This thesis therefore presents methods that utilize available appearance and 3D information extensively. In the context of highly-redundant digital aerial imagery we derive a holistic description for urban environments from color and available range images. A novel statistical feature representation and efficient multiclass learners offer the mapping of compactly combined congruent cues - composed of color, texture and elevation measurements - to probabilistic object class assignments for every pixel. On the basis of the derived holistic scene description, variational fusion steps integrate highly redundant observations to form high-quality results for each modality in an orthographic view. We finally demonstrate that the holistic description, which combines color, surface models and semantic classification, can be used to construct interpreted large-scale building models that are described by only a few parameters. In the experimental evalutation we examine the results with respect to available redundancy, correctly assigned object classes, as well as to obtained noise suppression. For the evaluation we use real-world aerial imagery, showing urban environments of Dallas, Graz and San Francisco, besides standard benchmark datasets.},
  comment   = {See also KlucknerB09 and KlucknerMRB09. PhD thesis, I've not read. 3 interesting chapters: From Appearance and 3D to Interpreted Image Pixels, From 3D to the Fusion of Redundant Pixel Observations and From Interpreted Regions to 3D Models.},
  keywords  = {machine learning, aerial imagery, image interpretation},
  owner     = {ISargent},
  school    = {Graz University of Technology, Institute for Computer Graphics and Vision},
  creationdate = {2014.06.11},
}

@InProceedings{KlucknerB09,
  author    = {Stefan Kluckner and Horst Bischof},
  title     = {Semantic Classification by Covariance Descriptors Within a Randomized Forest},
  booktitle = {Computer Vision Workshops (ICCV Workshops), 2009 IEEE 12th International Conference on Computer Vision},
  year      = {2009},
  url       = {http://www.icg.tu-graz.ac.at/Members/kluckner/pub_kluckner/3Drr-09Kluckner.pdf},
  comment   = {Covariance descriptors are introduced in TuzelPM06. Use this to generate features for randomized forest classification applied to MSRC dataset and with ultracam aerial photography. Incorporate derivative ('texture') data and, where available, height (nDSM) data. Results on MRSC are 'reliable'. Classification of aerial imagery is to building, streetlay, grass, tree, waterbody and results in conherent classes. I would have liked to see comparison with another method of feature extraction and/or another classifier. By classifying all images in a block, they have redundant labelling for all regions. From this they compute a fused orthoprojection of the label data. height information is using image matching, must be slightly annoying maths to transform this back into the geometry of each image? I think one of the advantages of the covariance descriptors approach is that different sized regions can be directly compared but this study uses standard region sizes that area based on the GSD.},
  keywords  = {aerial imagery, image classification, machine learning, ImageLearn},
  owner     = {ISargent},
  creationdate = {2014.03.19},
}

@InProceedings{KlucknerMRB09,
  author       = {Kluckner, Stefan and Mauthner, Thomas and Roth, Peter M. and Bischof, Horst},
  booktitle    = {Proc. Asian Conference on Computer Vision (ACCV)},
  title        = {Semantic Classification in Aerial Imagery by Integrating Appearance and Height Information},
  url          = {http://www.icg.tugraz.at/Members/kluckner/pub_kluckner/klucknerACCV09.pdf},
  comment      = {Use UltraCam data as well as the MSRC-9 dataset. ``Due to high variability in aerial imagery, automatic classification and semantic description still pose an unsolved task in computer vision. We aim to use appearance cues, such as color, edge responses, and height information for accurate semantic classification into five classes.'' Useful review of appearance-driven supervised classification of images. Used Randomized Forests and Conditional Random Fields (which account for contextual information). ``a novel feature representation based on covariance matrices and Sigma Points ... that can be directly applied to multi-class [randomized forest] classifiers.'' ``our work has three main contributions: To allow an efficient semantic classification, we first introduce a novel technique to obtain a powerful feature representation, derived from compact covariance descriptors [17] which is directly applicable to [randomized forest] classifiers. Covariance matrices [17] can be efficiently computed and provide an intuitive integration of various feature channels. Since the space of covariance matrices does not form a Euclidean vector space [17], this representation can not be directly used for most machine learning techniques. To overcome this drawback, manifolds [18, 17, 19] are typically utilized, which, however, is computationally expensive. In contrast to calculating similarity between covariance matrices on Riemannian manifolds [18], we present a simple concept for mapping individual covariance descriptors to Euclidean vector space. The derived representation enables a compact integration of appearance, filter responses, height information etc. while the RF efficiently performs a multi-class classification task on the pixel level. Second, we introduce semantic knowledge by applying an efficient conditional random field (CRF) stage incorporating again several feature cues and co-occurrence information. To demonstrate the state-of-the-art performance we present quantitative results on the Microsoft Research Cambridge dataset MSRC-9 [15] by integrating visual appearance cues, such as color and edge information. Third, we apply our proposed method to real world aerial imagery, performing large scale semantic classification. We extend the novel feature representation with available height data as an additional cue and investigate the classification accuracy in terms of correctly classified pixels. Labeled training data, representing five annotated classes (building, tree, waterbody, green area and streetlayer), provides the input for the training process.'' Probably worth reading Kluckner's PhD thesis.},
  creationdate = {2014.02.25},
  keywords     = {aerial Imagery, machine Learning, object detection, ImageLearn},
  owner        = {ISargent},
  year         = {2009},
}

@InCollection{Kolbe2009,
  author    = {Kolbe, Thomas H.},
  title     = {Representing and Exchanging 3D City Models with CityGML},
  booktitle = {3D Geo-Information Sciences},
  year      = {2009},
  editor    = {Lee, Jiyeong and Zlatanova, Sisi},
  language  = {English},
  series    = {Lecture Notes in Geoinformation and Cartography},
  publisher = {Springer Berlin Heidelberg},
  isbn      = {978-3-540-87394-5},
  pages     = {15-31},
  doi       = {10.1007/978-3-540-87395-2_2},
  url       = {http://dx.doi.org/10.1007/978-3-540-87395-2_2},
  comment   = {Detail of the CityGML model. LODs allow the understanding of data quality, in terms of precision, different themes of feature possible (building, transportation, relief, water, vegetation) and buildings can have building parts (walls, doors). Logical consistency of model is specified and co-ordinate systems are supported. Local co-ordinate systems can be used to define library objects and their instantiation needs to include transform details. surfaces can have an appearance which is how surface responds to different factors e.g. natural light in different conditions, sound, earthquakes, etc. BIM integration to come and CityGML is complementary to other computer graphics standards like X3D, VRML and COLLADA. Can be extended for particular applications by creating an application domain environment (ADE) or 'on-the-fly' for more ad hoc extension.},
  keywords  = {3DCharsPaper, 3D modelling},
  owner     = {ISargent},
  creationdate = {2015.11.07},
}

@Misc{Koller13,
  author       = {Daphne Koller},
  title        = {Probabilistic Graphical Models},
  year         = {2013},
  howpublished = {MOOC},
  month        = {4},
  url          = {https://www.coursera.org/course/pgm},
  comment      = {Excellent introduction to PGMs.},
  owner        = {ISargent},
  creationdate    = {2014.06.03},
}

@Article{KomorowskiME09,
  author       = {Komorowski, R W and Manns, J R and Eichenbaum, H},
  title        = {Robust conjunctive item-place coding by hippocampal neurons parallels learnin what happens where},
  journaltitle = {The Journal of Neuroscience},
  year         = {2009},
  volume       = {29},
  pages        = {9918--9929},
  comment      = {See BarryD10},
  keywords     = {Neuroscience},
  owner        = {ISargent},
  creationdate    = {2014.06.10},
}

@InProceedings{KramerDRK2011,
  author       = {Michel Kr\''{a}mer and Martin Dummer and Tobias Ruppert,and J\''{o}rn Kohlhammer},
  booktitle    = {GeoViz Hamburg 2011 Workshop},
  title        = {Tackling uncertainty in combined visualizations of underground information and 3D city models},
  url          = {https://www.researchgate.net/publication/235763031_Tackling_Uncertainty_in_Combined_Visualizations_of_Underground_Information_and_3D_City_Models},
  comment      = {Worth reading. Good table showing different categories of quality and example applications.},
  creationdate = {2015.02.06},
  keywords     = {3D Quality},
  owner        = {ISargent},
  year         = {2011},
}

@InProceedings{KramerHR2007,
  author       = {M Kr\''{a}mer and J Haist and T Reitz},
  booktitle    = {5th Italian Chapter Conference},
  title        = {Methods for spatial data quality of 3D city models},
  pages        = {167--172},
  url          = {https://www.researchgate.net/publication/221210379_Methods_for_Spatial_Data_Quality_of_3D_City_Models?ev=pub_cit},
  comment      = {Uses international standards for spatial data quality which has 6 elements: 1. Positional Accuracy: The 3D coordinates of all objects have to be as exact as possible (close the ones in the conceptual reality) 2. Completeness: Objects and attributes must be complete 3. Semantic Accuracy: Classification of objects must be correct and object attributes must have valid values 4. Correctness: Object attributes must have correct values 5. Temporal Conformance: Objects must be within defined time constraints 6. Logical Consistency: Logical rules (e.g. all object faces must be oriented clock-wise) have to be consistent for all objects.},
  creationdate = {2015.02.06},
  keywords     = {3D quality},
  owner        = {ISargent},
  year         = {2007},
}

@Misc{Krebs2010,
  author    = {Waldemar Krebs},
  title     = {Trimble's eCognition Product Suite},
  url       = {http://www.gisat.cz/images/upload/6132a_trimble-ecognition-intergeo-101005.pdf},
  comment   = {mention of using eCognition to classify roof shape (apparently to flat, ridged and green).},
  keywords  = {Roof Shape},
  owner     = {ISargent},
  creationdate = {2015.06.04},
}

@MastersThesis{Krizhevsky2009,
  author    = {Krizhevsky, Alex},
  title     = {Learning Multiple Layers of Features from Tiny Images},
  year      = {2009},
  url       = {http://www.cs.toronto.edu/\~{}kriz/learning-features-2009-TR.pdf},
  comment   = {whitening transform to the image patches, to increase their statistical independence},
  keywords  = {learning, sparse, ImageLearn},
  owner     = {ISargent},
  posted-at = {2010-07-15 10:17:28},
  creationdate = {2017.05.30},
}

@InProceedings{KrizhevskySH12,
  author       = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle    = {Advances in Neural Information Processing Systems 25},
  title        = {ImageNet classification with deep convolutional neural networks},
  editor       = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
  pages        = {1097--1105},
  publisher    = {Curran Associates, Inc.},
  url          = {http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf},
  abstract     = {The AlexNet paper. e trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called ``dropout'' that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2\% achieved by the second-best entry. See also Colah2014.},
  address      = {Lake Tahoe, USA},
  comment      = {AlexNet paper.},
  creationdate = {2013.11.20},
  keywords     = {deep learning, ImageLearn, DeepLEAP, MLStrat Milesstones},
  owner        = {ISargent},
  year         = {2012},
}

@Article{KruegerD2009,
  author       = {Kai A. Krueger and Peter Dayan},
  journaltitle = {Cognition},
  title        = {Flexible shaping: How learning in small steps helps},
  pages        = {380--394},
  url          = {http://www.gatsby.ucl.ac.uk/~dayan/papers/kruegerdayan09.pdf},
  volume       = {110},
  comment      = {''Humans and animals can perform much more complex tasks than they can acquire using pure trialand error learning. This gap is filled by teaching. One important method of instruction is shaping, in which a teacher decomposes a complete task into sub-components, thereby providing an easier path to learning.''},
  creationdate = {2015.07.15},
  keywords     = {ImageLearn, psychology, cognition},
  owner        = {ISargent},
  year         = {2009},
}

@InBook{Kulschewski1997,
  author       = {Kulschewski, K},
  booktitle    = {Semantic Modeling for the Acquisition of Topographic Information from Images},
  title        = {Building Recognition with Bayesian Networks},
  editor       = {Wolfgang F\''{o}rstner and Lutz Pl\''{u}mer},
  url          = {http://books.google.co.uk/books?hl=en&lr=&id=u9g3GtWaF7UC&oi=fnd&pg=PA196&dq=%22Building+Recognition+with+Bayesian+Networks%22&ots=5OB-t8XsVm&sig=A4u16R5v1OoXpQLKHRkobZetkHs#v=onepage&q=%22Building%20Recognition%20with%20Bayesian%20Networks%22&f=false},
  comment      = {From ScholzeMV2002: ``(Kulschewski, 1997) studies the recognition of buildings from a single view using a dynamic Bayesian network. The Bayesian network approach allows the author to handle uncertainties in the input data, regarding accuracy and completeness. Extracted roof outlines are used to reliably classify the building type, yet modelling entire building types imposes a limitation to the system. `` A great paper for references for use of Bayesian nets in image interpretation. The network itself starts with segementing the image into faces, edges and points. At a higher level objects are interpreted. However there is error and so the Bayesian net is used to handle uncertainties.},
  creationdate = {2014.10.28},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  year         = {1997},
}

@Article{LopezSastreRGML13,
  author       = {L\''{o}pez-Sastre, Roberto. J. and Javier Renes-Olalla and Pedro Gil-Jim\''{e}nez and Saturnino Maldonado-Basc\''{o}n and Sergio Lafuente-Arroyo},
  journaltitle = {IEEE Transactions on Circuits And Systems for Video Technology},
  title        = {Heterogeneous Visual Codebook Integration via Consensus Clustering for Visual Categorization},
  number       = {8},
  pages        = {1358 - 1368},
  volume       = {23},
  abstract     = {Most recent category-level object and activity recognition systems work with visual words, i.e. vector-quantized local descriptors. These visual vocabularies are usually built by using a local feature, such as SIFT, and a single clustering algorithm, such as K-means. However, very different clusterings algorithms are at our disposal, each of them discovering different structures in the data. In this paper, we explore how to combine these heterogeneous codebooks and introduce a novel approach for their integration via consensus clustering. Considering each visual vocabulary as one modal, we propose the Visual Word Aggregation (VWA) methodology, to learn a common codebook, where: the stability of the visual vocabulary construction process is increased, the size of the codebook is determined in an unsupervised integration, and more discriminative representations are obtained. With the aim of obtaining contextual visual words, we also incorporate the spatial neighboring relation between the local descriptors into the VWA process: the Contextual-VWA (C-VWA) approach. We integrate over-segmentation algorithms and spatial grids into the aggregation process to obtain a visual vocabulary that narrows the semantic gap between visual words and visual concepts. We show how the proposed codebooks perform in recognizing objects and scenes on very challenging datasets. Compared with unimodal visual codebook construction approaches, our multi-modal approach always achieves superior performances.},
  creationdate = {2013.10.03},
  keywords     = {Machine Learning, visual Codebook},
  owner        = {isargent},
  year         = {2013},
}

@Article{LandesG2012,
  Title                    = {Quality assessment of geometric fa\c{c}ade models reconstructed from TLS data},
  Author                   = {Landes, H. Boulaassal and P. Grussenmeyer},
  Year                     = {2012},
  Number                   = {138},
  Pages                    = {137--154},
  Volume                   = {27},

  Journaltitle             = {Photogrammetric Record},
  Keywords                 = {3D quality},
  Owner                    = {ISargent},
  creationdate                = {2015.03.22}
}

@Article{LandesGBM2012,
  author       = {T. Landes and P. Grussenmeyer and H. Boulaassal and M. Mohamed},
  journaltitle = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  title        = {Assessment of Three-dimensional Models Derived From Lidar and TLS Data},
  pages        = {95--100},
  url          = {http://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XXXIX-B2/95/2012/isprsarchives-XXXIX-B2-95-2012.pdf},
  volume       = {XXXIX-B2},
  comment      = {''This paper presents the assessment of 3D vector models of fa\c{c}ades, roofs and complete buildings, using several approaches. Of course, visual inspection cannot be avoided. For assessing fa\c{c}ades or roofs outlines in 2D, quality factors already suggested in the literature have been applied. For assessing 3D vectors, statistical criteria like RMSE are somewhat restrictive and that's why they have been supplemented by error maps. For assessing entire 3D building models, quality factors based on volume ratios have been considered and completed by RMSE consider ations. This approach must be further investigated regarding shape characteristics of the buildings.'' Compare automatically modelled buildings against reference data using quality factors derived from literature.},
  creationdate = {2015.03.22},
  keywords     = {3D quality, 3DCharsPaper},
  owner        = {ISargent},
  year         = {2012},
}

@Article{LangeMR2016,
  author       = {Matthias Lange and Jan Mendling and Jan Recker},
  date         = {3},
  title        = {An empirical analysis of the factors and measures of Enterprise Architecture Management success},
  number       = {1},
  url          = {https://www.researchgate.net/publication/276884522_An_empirical_analysis_of_the_factors_and_measures_of_Enterprise_Architecture_Management_success},
  volume       = {6},
  comment      = {Contains good description of what Enterprise Architecture and its Management are (page 4-5) with basic references. ``The DMSM model suggests six key success dimensions that relate to factors and measures of successful information system use (DeLone \& McLean, 1992; DeLone \& McLean, 2003) ... the application of the DMSM model as a conceptual framework is applicable not only to a wide range of technological systems (Petter et al., 2013), but also management domains and contexts, such as process modeling (Bandara \& Rosemann, 2005), e-commerce management (DeLone \& McLean, 2004), or knowledge management (Kulkarni et al., 2007)'' ``A key extension to the DMSM in the substantive context of EAM is the introduction of the additional success factor EAM organizational anchoring. In general, anchors are reference points that entities can draw upon when choosing a behaviour or making a decision (Tversky \& Kahneman, 1974). Organizational anchors describe those characteristics and conditions in an organization that work collectively to enable, drive, and influence an organization's performance (Eversole \& Barr, 2003). In analogy, we thus define organizational anchoring of Enterprise Architecture Management as the characteristics and conditions through which EAM is embedded in the organization to enable, drive, and influence an organization's performance'' Concludes with three Key contribution, their Implications for research and Implications for practice.},
  creationdate = {2017.03.30},
  journal      = {European Journal of Information Systems},
  owner        = {ISargent},
  year         = {2015},
}

@Unpublished{Langlands2015a,
  author       = {Alexander Langlands},
  title        = {Mapping the \emph{Genius Loci}: Exploring the character of space and place in Ordnance Survey topographic detail. Project Rationale, Summary of Part and recommendations for Future Research},
  note         = {Project co-funded by University of Winchester and Ordnance Survey},
  comment      = {Genius Loci, by the 18th century, 

Spatial data is being used more to understand past and present society. Suggests (without reference) that a quantitative approach risks removing humans, their perceptions, attitudes and metalities from the picture. ``How [humans] construct physical space is often determine through the institutions of society'' - credited to Soja1989. ``Phenomenologists wanted to see human action and agency as central to the understanding of past landscapes and set out to describe the character of human experience via our own comprehension of the material world''. ``An underlying philosophy in antropological studies that structured social space has the capacity to be read. 'Space Syntax' and 'morphological grammar' operate on the premise that the manner in which humans order space is embedded with a social logic over which we are either conscious or sub-conscious''. I have encountered space syntax before - some very interesting work in the early 00's. Morphological grammar, on the other hand, seems to be misappropriated. 

I see this work as the sociological/anthropological analogy to ecological regionalisation and landscape ecology. The overall thinking of the project is that space can be read and interpreted horizontally as time is read and interpreted vertically in stratigraphies. This is somewhat of a mismatch between the wordy theorising of the introductory sections (mostly this part) and the rather simple topological analysis of the latter sections. However, the premise of the work is interesting, that the character of place is partly defined by its history and archeology.},
  creationdate = {2015.07.29},
  keywords     = {landscape, ImageLearn},
  owner        = {ISargent},
  year         = {2015},
}

@Unpublished{Langlands2015b,
  author       = {Alexander Langlands},
  title        = {Mapping the \emph{Genius Loci}: Exploring the character of space and place in Ordnance Survey topographic detail. PART 1: The Historic Environment and Landscape Character},
  note         = {Project co-funded by University of Winchester and Ordnance Survey},
  comment      = {''Under the auspices of English Heritage, a Historic Landscape Characterisation (HLC) project was rolled out first across Bodmin Moor, Cornwall and then throughout the county with the approach being developed by a number of other studies and extended to other areas of Britain. A key element to HLC is the mapping of the 'whole' rathen than select elements of a landscape...'. 

From Langlands2015a: [Historic Landscape Characterisation] has become the standard means by which we break rural and uraban areas down into contiguous blocks of broadly homogeneous development, primarily for the purposes of planning and conservation ...HCL work adopts the aerial perspective on 'character' which results in abstractions of space that can do little to relate to the human experience of character''. Whilst I agree that experience of a place on the ground can be very different to that imagined from an aerial view, no evidence is presented that either are 'wrong' or indeed there is no mapping between the two. 

The method used in the part of the study extract lines ('linears' - an archeological term?) from vector data and uses buffers to determine if there are corresponding lines in maps from different time periods. From this some statistics are derived about relict features and some theories are proposed about how these can be used to characterise a place. e.g. ``The densities of polylines in these areas could be used as an indicator of historic character potential, with the argument running that the built historic environemnt of earlier periods is most likely to be perserved in these locations''.},
  creationdate = {2015.07.29},
  keywords     = {landscape, ImageLearn},
  owner        = {ISargent},
  year         = {2015},
}

@Unpublished{Langlands2015c,
  author       = {Alexander Langlands},
  title        = {Mapping the \emph{Genius Loci}: Exploring the character of space and place in Ordnance Survey topographic detail. PART 2: Reading the Map: Temporality and Topographic Definition},
  note         = {Project co-funded by University of Winchester and Ordnance Survey},
  comment      = {This part of the project treats the map as a palimpsest, after Maitland 1897. ``The idea of seeing the landscape as a 'document' and something that can be 'read' has been popular amongst commentators and analysists fo the British landscape for well over half a century''. ``Difference periods and practices produce different shapes in the landscape''. Suggests that it may be possible to automate the process of reading the landscape and explores the feasibility of this. uses the model of a Harris Matrix - a way of characterising the vertical stratigraphy of an archeological site - as translates this to a horizontal matrix - 'horizontal stratigraphy'. ``Horizontal stratigraphy can aid the establishment of a relative chronology between various landscape linears''. Values OS MasterMap polygons that represent roads according to their age relative to some other road feature - i.e. whether they are clearly earlier, later or their relationship is ambiguous, based on the spatial relationship of the roads. for exampe, a road that appears to overlay the subject road is probably of later construction. Focus is on Bexhill-on-Sea. Again, I like the premise, that that landscape can be read but the execution is rather too limited to be conclusive.},
  creationdate = {2015.07.29},
  keywords     = {landscape, ImageLearn},
  owner        = {ISargent},
  year         = {2015},
}

@Unpublished{Langlands2015d,
  author       = {Alexander Langlands},
  title        = {Mapping the \emph{Genius Loci}: Exploring the character of space and place in Ordnance Survey topographic detail. PART 3: Reading the Map: Spatial Morphology and Demographic Analysis},
  note         = {Project co-funded by University of Winchester and Ordnance Survey},
  comment      = {''If space was both consciously and subconsciously created, surely it must be the case that through detailed study of it in mapped and experienced form, we could genearte understandings of the societies that generated that space. The key question that Part 3 of the project seeks to answer is whether the space depicted in the Ordnance Survey's MasterMap data can be 'read'. In order to produce a social, economic and cultural undestanding of British society''.

Case studies on Earsash, Hampshire and Bradford, West Yorkshire using census 2011 and indices of muliple deprivation. 

From Langlands2015a: ``In the 1980s, the concept of 'space syntax' and 'morphological grammar' emerged as means with which to measure space in different ways but also as a language with which to better understand the function and 'social logic' of space. if sapce was both consciously and subconsciously created, surely it must be the case that through detailed study of it in mapped and experienced form, we could generate understandings of the societies that generated that space.'' ``There is clearly a correlation between the number of cul-de-sacs and the rise in wealth''.''At the simplest level, apsect of the social and economic character of an area cold be 'read' from the distribution of size of certain themed geometric shapes''.},
  creationdate = {2015.07.29},
  keywords     = {landscape, ImageLearn},
  owner        = {ISargent},
  year         = {2015},
}

@Article{LansdallWelfareSTLC2017,
  author       = {Lansdall-Welfare, Thomas and Sudhahar, Saatviga and Thompson, James and Lewis, Justin and FindMyPast Newspaper Team and Cristianini, Nello},
  title        = {Content analysis of 150 years of British periodicals},
  doi          = {10.1073/pnas.1606380114},
  eprint       = {http://www.pnas.org/content/early/2017/01/03/1606380114.full.pdf},
  url          = {http://www.pnas.org/content/early/2017/01/03/1606380114.abstract},
  abstract     = {Previous studies have shown that it is possible to detect macroscopic patterns of cultural change over periods of centuries by analyzing large textual time series, specifically digitized books. This method promises to empower scholars with a quantitative and data-driven tool to study culture and society, but its power has been limited by the use of data from books and simple analytics based essentially on word counts. This study addresses these problems by assembling a vast corpus of regional newspapers from the United Kingdom, incorporating very fine-grained geographical and temporal information that is not available for books. The corpus spans 150 years and is formed by millions of articles, representing 14\% of all British regional outlets of the period. Simple content analysis of this corpus allowed us to detect specific events, like wars, epidemics, coronations, or conclaves, with high accuracy, whereas the use of more refined techniques from artificial intelligence enabled us to move beyond counting words by detecting references to named entities. These techniques allowed us to observe both a systematic underrepresentation and a steady increase of women in the news during the 20th century and the change of geographic focus for various concepts. We also estimate the dates when electricity overtook steam and trains overtook horses as a means of transportation, both around the year 1900, along with observing other cultural transitions. We believe that these data-driven approaches can complement the traditional method of close reading in detecting trends of continuity and change in historical corpora.},
  comment      = {Paper taking archive of newpapers and creating frequency over time of use of certain words and phrases. Shows social, political etc events and trends.},
  creationdate = {2017.01.24},
  journal      = {Proceedings of the National Academy of Sciences},
  owner        = {ISargent},
  year         = {2017},
}

@Article{LazarosSG08,
  author       = {Nalpantidis Lazaros and Sirakoulis, Georgios Christou and Antonios Gasteratos},
  title        = {REVIEW OF STEREO VISION ALGORITHMS: FROM SOFTWARE TO HARDWARE},
  journaltitle = {International Journal of Optomechatronics},
  year         = {2008},
  volume       = {22},
  pages        = {435--462},
  comment      = {Paper revieing stacks of stereo matching work. Review takes the form of a paragraph per method and not a great deal of comparison. However the key components of each method are extracted as well as a comparison on speed and a identification of similarities between methods. Categories include: matching cost functions (absolute intensity differences (AD), the squared intensity differences (SD) and the normalized cross correlation (NCC)) and the aggregation of these (sum of absolute differences (SAD), sum of squared difference and NCC) sparse or dense output (the former tend to use area matching and provide more detail and accuracy and the latter feature matching and be faster and closer to biological stereo vision) Local or global methods (dense output only, the former tend to be faster and the latter tend to be more accurate) software or hardware implementations colour usage occlusion handling global optimisation versus dynmaic programming (global methodsonly) 10 local methods from between 2002 and 2006 are reviewed and 23 global methods from between 2004 and 2007. include methods that use techniques such as neural networks, cellular automata, GPU utilization. A better but earlier review was produced in ScharsteinS02.},
  keywords     = {Stereo Matching, 3D},
  owner        = {ISargent},
  creationdate    = {2013.07.22},
}

@InProceedings{LazebnikSP2006,
  author       = {Svetlana Lazebnik and Cordelia Schmid and Jean Ponce},
  booktitle    = {2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)},
  title        = {Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories},
  doi          = {10.1109/CVPR.2006.68},
  pages        = {2169-2178},
  url          = {http://www-cvr.ai.uiuc.edu/ponce_grp/publication/paper/cvpr06b.pdf},
  volume       = {2},
  abstract     = {This paper presents a method for recognizing scene categories based on approximate global geometric correspondence. This technique works by partitioning the image into increasingly fine sub-regions and computing histograms of local features found inside each sub-region. The resulting ``spatial pyramid'' is a simple and computationally efficient extension of an orderless bag-of-features image representation, and it shows significantly improved performance on challenging scene categorization tasks. Specifically, our proposed method exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories. The spatial pyramid framework also offers insights into the success of several recently proposed image descriptions, including Torralba's ``gist'' and Lowe's SIFT descriptors.},
  comment      = {From CastelluccioPSV2015 ``One popular approach is the spatial pyramid match kernal (SPMK) proposed in [15] for object and scene categorization. It consistes in partitioning the image at different levels of resolution and computing weighted histograms of the number of matches of local features at each level''.},
  creationdate = {2016.05.11},
  issn         = {1063-6919},
  keywords     = {ImageLearn, Spatial Scale, Hand-coded},
  owner        = {ISargent},
  year         = {2006},
}

@Article{LeRouxHSW12,
  author       = {Le Roux, Nicolas and Nicolas Heess and Jamie Shotton and John Winn},
  journaltitle = {Neural Computation},
  title        = {Learning a generative model of images by factoring appearance and shape},
  url          = {http://research.microsoft.com/pubs/145592/NECO_a_00086.pdf},
  comment      = {Very well written with a good summary of deriving information from images, esp generative v descriminative and deep learning. Haven't read whole paper as it is very long. ``One premise of the work described in this article is that generative models hold important advantages in computer vision. Their most obvious advantage over discriminative methods is perhaps that they are more amenable to unsupervised learning, which seems of crucial importance in a domain where labeled training data are often expensive while unlabeled data are now easy to obtain.'' Also really useful for the practicalities with learning in RBMs. ``Across experiments, the beta RBM proved more robust and slightly more accurate than all the other types of RBM. We therefore decided to use it to model appearances''. When trying to model shape in images, edges are relevant. If not enough hidden nodes, blurring is evident in recosntructed image. To avoid this entirely, the number of hidden nodes (to account for all the possible locations of edges in the image) would be too massive. Instead have a set of RBMs to explain appearance - where there is an edge, one RBM will explain pixels on one side while another will explain pixels on another. Model is able to factor in occlusion.},
  creationdate = {2013.10.16},
  institution  = {Microsoft Research, Cambridge},
  keywords     = {generative modelling, ImageLearn},
  owner        = {ISargent},
  year         = {2012},
}

@TechReport{LeRouxHSW10,
  author      = {Le Roux, Nicolas and Nicolas Heess and Jamie Shotton and John Winn},
  title       = {Learning a generative model of images by factoring appearance and shape},
  institution = {Microsoft Research, Cambridge},
  year        = {2010},
  url         = {http://research.microsoft.com/pubs/118644/dsn_nc.pdf},
  comment     = {see LeRouxHSW12},
  owner       = {ISargent},
  creationdate   = {2013.10.16},
}

@InProceedings{LeRMDCCDN12,
  author       = {Le, Quoc V. and Marc'Aurelio Ranzato and Rajat Monga and Matthieu Devin and Kai Chen and Corrado, Greg S and Jeff Dean and Ng, Andrew Y.},
  booktitle    = {Proceedings of the Twenty-Ninth International Conference on Machine Learning},
  title        = {Building High-level Features Using Large Scale Unsupervised Learning},
  url          = {https://arxiv.org/abs/1112.6209v5},
  address      = {Edinburgh, Scotland},
  comment      = {This is the google project that famously developed a ``cat detector'' using unsupervised training. Lots of great references and extremely easy to read. Uses lots of features inspired from current understanding of temporal cortex/visual cortex/neocortex. Build an encoder with 3 layers. Each layer has 3 layers: local receptive fields, L2 pooling and local contrast normalisation. receptive fields are not convolutional - weights are not shared. ``In addition to being more biologically plausible, unshared weights  allow  the  learning  of  more  invariances  other than translational invariances (Le et al., 2010)''. Local receptive fields is a method to make convolutional neural networks scalable to large images and involves making only local regions of the image available to each feature in the encoder layer. The L2 pooling layer outputs the square root of the sum of the squares of its inputs. Local contrast normalisation is described in JarrettKRL09. `` we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three day. ... In terms of scale, our network is perhaps one of the largest known networks to date. It has 1 billion trainable parameters, which is more than an order of magnitude larger than other large networks reported in literature'' ``is it possible to learn a face detector using only unlabeled images?'' The network develops neurons for some common features including faces, cat faces and human bodies and performs better than the best previous methods in a number of baseline tests. 

In Sargent et al 2018: ``And early work investigating what deep networks learn is described in~\cite{LeRMDCCDN12} in which millions of stills were taken from YouTube videos and applied to an unsupervised deep network - in this case a nine-layered deep autoencoder. The trained network was then interrogated by finding the nodes that were best at detecting human faces, cat faces and human bodies in unseen data sets that contained these features.''},
  creationdate = {2013.09.12},
  keywords     = {Machine Learning, Representation Learning, Deep Learning, Sparse Coding, ImageLearn, DeepLEAP, toponet metrics, Unsupervised, MLStrat Unsupervised},
  owner        = {ISargent},
  year         = {2012},
}

@Electronic{LeCunXX,
  author       = {Yann LeCun},
  year         = {2008},
  url          = {http://www.cs.nyu.edu/~yann/research/deep/},
  comment      = {Has excellent animation of sparse features being learned for natural images},
  howpublished = {Online},
  keywords     = {sparse coding, ImageLearn},
  owner        = {ISargent},
  creationdate    = {2013.12.19},
}

@Article{LeCunNH98,
  author       = {LeCun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journaltitle = {Proceedings of the IEEE},
  title        = {Gradient-based learning applied to document recognition},
  doi          = {10.1109/5.726791},
  issn         = {0018-9219},
  number       = {11},
  pages        = {2278-2324},
  url          = {http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf},
  volume       = {86},
  comment      = {Excellent paper about character recognition with lots of background about neural networks. Its very long and I've not read it all. Introduces gradient based learning proceedure for graph transformer networks. Excellent reference for convolutional neural networks. Previously a lot of effort put into extracting features and these features specific to each task. This is now not so necessary for 3 reasons: 1) fast and powerful but low-cost machines are now available, 2) large databases/datasets/corpuses can now be obtained and 3) new powerful machine learning techniques that can handle high-dimensional inputs. Theoretical and experimental work (gives refs) has found that the difference between the expected error of the test set and the expected error of the training set decreases with the number of training samples in the order of E_test - E_train = k(h / P)^a where P is the number of training samples, h is a measure of the complexity of the machine ('effective capacity'), a is a number between 0.5 and 1 and k is a constant. Gradient descent has been used since the 1950's but its usefulness was not realised until 3 things happened: 1) it was found that the presence of local minima was not a problem in practise (it is still a bit of a mystery why), 2) the popularisation of this technique by Rumelhart, Hinton and Williams (and others) and 3) the demonstration that the backpropogation algorithm could be used to compute the gradient in non-linear multi-layered systems. This paper report comparitive experiments into recognising handwritten characters and shows that neural networks training with gradient-based learning performs better than all other methods. The best neural networks, convolution networks ``are designed to learn to extract relevant features directly from pixel images''. If we are to move to performing recognition tasks to working directly from imagery, rather than extracted features, then we need to overcome the problems associated with images, namely that they are often massive and so a fully connected network working directly with the whole image would require a massive number of weights, requiring a massive number of input images and also invariance to position and other distortions is difficult to obtain. Another drawback of fully connected architectures is that the topology of the input is not considered . Convolutional networks automatically obtain shift invariance by ``forcing the replication of weight configurations across space'' and have inbuilt ``force the extraction of local features by restricting the receptive fields of hidden units to be local''. Discuss stochastic and batch gradient descent (as well as other methods) and seem to conclude that stochastic gradient descent achieve excellent results and is much more efficient. Say that training a 'multi-module' system to optimise a global measure of performance overcomes need to hand truthing segmented characters yields significantly better recognition performance but I'm not sure how this is achieved. ``Feature extraction is traditionally a fixed transform, generally derived from some expert prior knowledge about the task. This relies on the probably incorrect assumption that the human designer is able to capture all the relevant information in the input''},
  creationdate = {2013.09.17},
  keywords     = {machine learning, neural networks, ImageLearn},
  month        = {11},
  owner        = {ISargent},
  year         = {1998},
}

@InProceedings{LeeEN2008,
  author    = {Lee, H and Ekanadham, C and Ng, A},
  title     = {Sparse deep belief net model for visual area V2},
  booktitle = {Advances in Neural Information Processing Systems 20 (NIPS'07)},
  year      = {2008},
  publisher = {MIT Press, Cambridge, MA},
  pages     = {873--880},
  comment   = {Discussed in ErhanDC2010.},
  editors   = {Platt, J and Koller, D and Singer, Y and Roweis, S},
  keywords  = {ImageLearn},
  owner     = {ISargent},
  creationdate = {2014.07.11},
}

@InProceedings{LeeEN07,
  author       = {Honglak Lee and Chaitanya Ekanadham and Ng, Andrew Y.},
  booktitle    = {NIPS},
  title        = {Sparse deep belief net model for visual area V2},
  url          = {http://cs.stanford.edu/people/ang/papers/nips07-sparsedeepbeliefnetworkv2.pdf},
  comment      = {'' This paper presents an unsupervised learning model that faithfully mimics certain properties of visual area V2'' from GlorotBB11: ``it was found that the features learned in deep architectures resemble those observed in the first two of these stages (in areas V1 and V2 of visual cortex) (Lee et al., 2008)''},
  creationdate = {2014.05.22},
  keywords     = {Neuroscience, deep learning, ImageLearn},
  owner        = {ISargent},
  year         = {2007},
}

@InProceedings{LeeGRN2009,
  author       = {Honglak Lee and Roger Grosse and Rajesh Ranganath and Andrew Y. Ng},
  booktitle    = {ICML '09 Proceedings of the 26th Annual International Conference on Machine Learning},
  title        = {Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations},
  pages        = {609-616},
  url          = {http://www.cs.toronto.edu/~rgrosse/icml09-cdbn.pdf},
  comment      = {Combine deep belief networks (generative) with convolutional networks to learn representations from image of natural scenes. Use probabilistic max pooling. this is the one with the 'faces' and motorbike part representations. On natural images: ``the learned first layer bases are oriented, localized edge filters'', ``The learned second layer bases ... empirically responded selectively to contours, corners, angles, and surface boundaries in the images''. Both are consistent with previous studies (gives refs). Sparsity regularisation was necessary for learning oriented edge filters. ``Building on the first layer representation learned from natural images, we trained two additional CDBN layers using unlabeled images from single Caltech-101 categories ... The second layer learned features corresponding to object parts, even though the algorithm was not given any labels specifying the locations of either the objects or their parts. The third layer learned to combine the second layer's part representations into more complex, higher-level features.'' This is a good paper for its figures of different bases functions for different layers. Seems to be 3-layer networks. reference for hierarchical representations. Also nice simple reasoning for using convolutional filters (section 3).},
  creationdate = {2015.07.08},
  keywords     = {ImageLearn, MLStrat Training},
  owner        = {ISargent},
  year         = {2009},
}

@InProceedings{LeeLPN09,
  author    = {Honglak Lee and Yan Largman and Peter Pham and Ng, Andrew Y.},
  title     = {Unsupervised feature learning for audio classification using convolutional deep belief networks},
  booktitle = {Advances in Neural Information Processing Systems 22},
  year      = {2009},
  editor    = {Y. Bengio and D. Schuurmans and J. Lafferty and C. K. I. Williams and A. Culotta},
  publisher = {Curran Associates, Inc.},
  url       = {http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2009_1171.pdf},
  abstract  = {In recent years, deep learning approaches have gained significant interest as a way of building hierarchical representations from unlabeled data. However, to our knowledge, these deep learning approaches have not been extensively studied for auditory data. In this paper, we apply convolutional deep belief networks to audio data and empirically evaluate them on various audio classification tasks. In the case of speech data, we show that the learned features correspond to phones/phonemes. In addition, our feature representations learned from unlabeled audio data show very good performance for multiple audio classification tasks. We hope that this paper will inspire more research on deep learning approaches applied to a wide range of audio recognition tasks.},
  address   = {Vancouver, B.C., Canada},
  comment   = {example of application to speech data},
  keywords  = {deep learning, ImageLearn, DeepLEAP},
  owner     = {ISargent},
  creationdate = {2013.12.19},
}

@Article{LeeBRL98,
  author       = {Lee, Tai Sing and Mumford, David Bryant and Richard Romero and Lamme, Victor A F},
  journaltitle = {Vision Research},
  title        = {The role of the primary visual cortex in higher level vision},
  number       = {15-16},
  pages        = {2429-2454},
  url          = {http://dash.harvard.edu/bitstream/handle/1/3720031/Mumford_RolePrimaryVisual.pdf?sequence=1},
  volume       = {38},
  comment      = {''Classical ideas going back to Hubel and Wiesel attempt to interpret all neuronal responses as feature detectors, modulated by various contextual factors. In the case of V1, this amounts to filters with various extra receptive field enhancements and suppressions.'' ``Our proposal, that V1 is engaged in many levels of visual analysis through intracortical and feedback connections, is a significant departure from the classical feed-forward views on the nature of information processing and the functional role of V1.'' ``Taken together, the data presented in this paper and others [25 - 28,45] suggest that the V1 is not just a module for computing local features, but possibly serves as a high resolution buffer or visual computer to perform all computations that integrate global information with spatial precision. The intricate intracortical circuitry in V1, together with the recurrent extrastriate cortical feedback, allows V1 to participate in many levels of visual analysis and to represent many kinds of higher level structural information which are critical to recognition.''},
  creationdate = {2014.01.28},
  keywords     = {Neuroscience, ImageLearn},
  owner        = {ISargent},
  year         = {1998},
}

@InProceedings{LeeL2012,
  author       = {Wooyoung Lee and Michael Lewicki},
  booktitle    = {The Deep Learning and Unsupervised Feature Learning Workshop Neural Information Processing Systems (NIPS 2012)},
  title        = {Learning global properties of scene images from hierarchical representations},
  url          = {http://www.eng.uwaterloo.ca/~jbergstr/files/nips_dl_2012/Paper%2021.pdf},
  abstract     = {Scene images with similar spatial layout properties often display characteristic statistical regularities on a global scale. In order to develop an efficient code for these global properties that reflects their inherent regularities, we train a hierarchical probabilistic model to infer conditional correlational information from scene images. Fitting a model to a scene database yields a compact representation of global information that encodes salient visual structures with low dimensional latent variables. Using perceptual ratings and scene similarities based on spatial layouts of scene images, we demonstrate that the model representation is more consistent with perceptual similarities of scene images than the metrics based on the state-of-the-art visual features.},
  comment      = {Unsupervised learning applied to ground-based images (from the SUN dataset). ``Although [hand-crafted feature extraction] approaches have been successful, the features require careful tuning depending on the tasks. Another potential disadvantage of projecting scene images onto hand-designed feature spaces is that they do not necessarily capture all relevant scene information. `` The model encodes the data as a probability distribution and so is robust to noise. The model parameters are dictionaries and weights, each dictionary atom (my term) encodes a common direction along which the covariance units can vary (similar to eigenvectors). Training involves and inference step, applied to a random selection of images, in which latent variables are inferred and a learning step in which the model parameters are updated. The number of latent variables / weights (J) and number of atoms (K) in the dictionary are fixed in advance. Instead of stochastic gradient descent, which requires tuning of hyperparameters, limited memory BFGS (L-BFGS) is used. Training time is much shorter than with stochastic gradient descent. The resulting dictionaries contained 'Gabor-like structures', that were localised in nature (despite the formalisation of the model not constraining to localised structure). They perform a number of analyses to further investigate the dictionaries and their corresponding weights. Firstly, they consider the occurrance of atoms in terms of their scale and orientation and find that horizontal and vertical atoms dominate but these tended to be somewhat more globalised (my interpretation of their histogram). They find that the weights tend to incorporate the global information. They visualise this by assigning a bar in image space that has the same scale and orientation as the atoms and colour this according to the value of the corresponding weight in the image. They show randomly generated images which have the covariance matrix that corresponds to a given weight vector (?) as well as those images for which the magnitude of the latent variable vector is high (?). Even though there are 60 covariance units (A_j), only approximately 20 units are necessary for capturing the correlational structures of a scene image''. Finally, they consider methods of measuring visual similarity between images. Use subjects to select images that are similar to a target image to create data set ``Subjects were specifically instructed to focus on the shape and spatial layout of the scenes and to ignore non-spatial attributes such as color or types of objects in the scenes''. Find that their model works well for comparing similarity for outdoor scenes but other models perform simiarly with indoor scenes - suggest this is because their model is less good with sharp edges. Also consider openness of scenes and find their model seems to be better for finding similarly open scenes. ``This result suggests that the global structures that DCM automatically learns from the scene images effectively encode perceptually relevant information''. Useful detail on how all comparisons were performed. ``Also, the probabilistic distance measure introduced in this paper can be utilized not only for whole image retrieval but also for finding local interest matching points between images''.},
  creationdate = {2014.06.25},
  keywords     = {machine learning, deep learning, scene analysis, representation learning, image analysis, ImageLearn},
  owner        = {ISargent},
  year         = {2012},
}

@InProceedings{Lemmens88,
  author    = {Mathias J.P.M. Lemmens},
  title     = {A SURVEY ON STEREO MATCHING TECHNIQUES},
  booktitle = {Proceedings of 16th ISPRSC},
  year      = {1988},
  note      = {In ASPRS Vol. 27/B8 11-23},
  comment   = {A review of stereo matching techniques up to 1988 with respect to aerial photography. Very useful paper for background and foundation of photogrammetry and stereo matching but a bit old now.},
  keywords  = {Stereo Matching, 3D},
  owner     = {ISargent},
  creationdate = {2013.07.22},
}

@Online{LevinNM2015,
  author           = {Golan Levin and David Newbury and Kyle McDonald},
  title            = {Terrapattern},
  url              = {http://www.terrapattern.com},
  creationdate     = {2017.05.30},
  keywords         = {ImageLearn},
  modificationdate = {2023-12-06T08:02:50},
  owner            = {ISargent},
  year             = {2015},
}

@Article{LiSLF13,
  Title                    = {Object Bank: An Object-Level Image Representation for High-Level Visual Recognition},
  Author                   = {Li-Jia Li and Hao Su and Yongwhan Lim and Li Fei-Fei},
  Year                     = {2013},

  Abstract                 = {It is a remarkable fact that images are related to objects constituting them. In this paper, we propose to represent images by using objects appearing in them. We introduce the novel concept of object bank (OB), a high-level image representation encoding object appearance and spatial location information in images. OB represents an image based on its response to a large number of pre-trained object detectors, or 'object filters', blind to the testing dataset and visual recognition task. Our OB representation demonstrates promising potential in high level image recognition tasks. It significantly outperforms traditional low level image representations in image classification on various benchmark image datasets by using simple, off-the-shelf classification algorithms such as linear SVM and logistic regression. In this paper, we analyze OB in detail, explaining our design choice of OB for achieving its best potential on different types of datasets. We demonstrate that object bank is a high level representation, from which we can easily discover semantic information of unknown images. We provide guidelines for effectively applying OB to high level image recognition tasks where it could be easily compressed for efficient computation in practice and is very robust to various classifiers.},
  Journaltitle             = {International Journal of Computer Vision},
  Owner                    = {ISargent},
  creationdate                = {2014.01.28}
}

@Article{Lintott2008,
  author       = {Lintott, Chris J. and Schawinski, Kevin and Slosar, Anze and Land, Kate and Bamford, Steven and Thomas, Daniel and Raddick, M. Jordan and Nichol, Robert C. and Szalay, Alex and Andreescu, Dan and Murray, Phil and Vandenberg, Jan},
  journaltitle = {Monthly Notices of the Royal Astronomical Society},
  title        = {Galaxy Zoo: morphologies derived from visual inspection of galaxies from the Sloan Digital Sky Survey},
  number       = {3},
  pages        = {1179-1189},
  volume       = {389},
  abstract     = {In order to understand the formation and subsequent evolution of galaxies one must first distinguish between the two main morphological classes of massive systems: spirals and early-type systems. This paper introduces a project, Galaxy Zoo, which provides visual morphological classifications for nearly one million galaxies, extracted from the Sloan Digital Sky Survey (SDSS). This achievement was made possible by inviting the general public to visually inspect and classify these galaxies via the internet. The project has obtained more than 4 Ãƒâ€” 107 individual classifications made by ~105 participants. We discuss the motivation and strategy for this project, and detail how the classifications were performed and processed. We find that Galaxy Zoo results are consistent with those for subsets of SDSS galaxies classified by professional astronomers, thus demonstrating that our data provide a robust morphological catalogue. Obtaining morphologies by direct visual inspection avoids introducing biases associated with proxies for morphology such as colour, concentration or structural parameters. In addition, this catalogue can be used to directly compare SDSS morphologies with older data sets. The colour-magnitude diagrams for each morphological class are shown, and we illustrate how these distributions differ from those inferred using colour alone as a proxy for morphology.},
  creationdate = {2015.07.20},
  keywords     = {RapidDC},
  owner        = {ISargent},
  year         = {2008},
}

@Article{Lok2011,
  author       = {Corie Lok},
  title        = {Vision science: Seeing without seeing},
  journaltitle = {Nature},
  year         = {2011},
  volume       = {469},
  pages        = {284-285},
  url          = {http://www.usailighting.com/stuff/contentmgr/files/1/53cb74aad2cd76417e9143c5c497ee02/misc/seeing_lockley.pdf},
  comment      = {Article about discovery and understanding of a 3rd type of photoreceptor - the ipRGCs. They were discovered in 2002 and thought to only have a role in synchronising the circadian clock. However, studies with mice and humans with no functioning rods or cones have shown that there is some contribution to vision. There appears to be some sharing of role between rods and ipRGCs and collectively they allow response to light across a wide range of brightnesss. They seem to be most sensitive to blue light. The latter part of article discusses the effect of blue light on circadian clock and on macular health - we don't yet know if there are long term health implications of being exposed to light at certain wavelengths.},
  keywords     = {ImageLearn, Neuroscience},
  owner        = {ISargent},
  creationdate    = {2015.07.04},
}

@Article{LongSD2015,
  author        = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  title         = {Fully Convolutional Networks for Semantic Segmentation},
  eprint        = {1411.4038},
  url           = {https://arxiv.org/abs/1411.4038},
  archiveprefix = {arXiv},
  comment       = {The FCN paper. ``We adapt contemporary classification networks (AlexNet [22], the VGG net [34], and GoogLeNet [35]) into fully convolutional networks and transfer their learned representations by fine-tuning [5] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves stateof-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.'' [5] is DonahueJVHZTD2014 however, despite what it says above, this paper uses features learned from a different data set but does not fine-tune.},
  creationdate  = {2017.03.09},
  journal       = {CVPR (to appear)},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition, deep learning, segmentation, MLStrat Segmentation},
  month         = {11},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  year          = {2015},
}

@InProceedings{LouRBF13,
  author       = {Qi Lou and Raviv Raich and Forrest Briggs and Fern, Xiaoli Z.},
  booktitle    = {2013 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SI GNAL PROCESSING},
  title        = {Novelty Detection under Multi-Label Multi-Instance Framework},
  address      = {SOUTHAMPTON, UK},
  comment      = {Multi-instance multi-label (MIML) learning is applied to bags of instances each of which may have several labels (in multi-instance learning only one label is possible and so this can be positive or negative indicating the presence or absence of the class). This paper takes MIML further by performing novelty detection such that the classifier can indicate that an instance of an unknown class is present in the bag. ``such novel instances can be presented back to the experts for further inspection''. They use a kernal-based scoring function that I don't fully understand and compare their method to one-class SVM. This is applied to the MNIST handwritten digit set (bags created by putting set of digits together) and to recordings of birdsong which can include several birds singing as well as noise. They include pseudo-code. Results are good. MaronR98 describes Multiple Instance learning and bag of instances in terms of images reaonsably well. ZhouZHL12 is another reference.},
  creationdate = {2013.09.27},
  keywords     = {Machine Learning, Bag of Instances, Classification, Novelty Detection},
  month        = {9},
  owner        = {ISargent},
  year         = {2013},
}

@Article{Lowe04,
  author       = {Lowe, David G.},
  title        = {Distinctive image features from scale-invariant keypoints},
  journaltitle = {International Journal of Computer Vision},
  year         = {2004},
  volume       = {60},
  number       = {2},
  pages        = {91-110},
  url          = {http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf},
  comment      = {The SIFT paper},
  keywords     = {feature extraction, DeepLEAP},
  owner        = {ISargent},
  creationdate    = {2013.12.18},
}

@Article{LuXJ2014,
  Title                    = {Contrast Preserving Decolorization with Perception-Based Quality Metrics},
  Author                   = {Cewu Lu and Li Xu and Jiaya Jia},
  Year                     = {2014},
  Number                   = {2},
  Pages                    = {222-239},
  Volume                   = {110},

  Abstract                 = {Converting color images into grayscale ones suffer from information loss. In the meantime, it is one fundamental tool indispensable for single channel image processing, digital printing, and monotone e-ink display. In this paper, we propose an optimization framework aiming at maximally preserving color contrast. Our main contribution is threefold. First, we employ a bimodal objective function to alleviate the restrictive order constraint for color mapping. Second, we develop an efficient solver that allows for automatic selection of suitable grayscales based on global contrast constraints. Third, we advocate a perceptual-based metric to measure contrast loss, as well as content preservation, in the produced grayscale images. It is among the first attempts in this field to quantitatively evaluate decolorization results.},
  Journaltitle             = {International Journal of Computer Vision},
  Owner                    = {ISargent},
  creationdate                = {2014.11.05}
}

@Book{Lucas2012,
  author       = {Gavin Lucas},
  title        = {Understanding the Archaeological Record},
  publisher    = {Cambridge University Press},
  comment      = {''Crawford...was the first to use the concept of palimpsest about the landscape in a systematic way''},
  creationdate = {2015.07.30},
  keywords     = {landscape, ImageLearn},
  owner        = {ISargent},
  year         = {2012},
}

@InProceedings{MullerZ2005,
  author        = {S\''onke M\''uller and Daniel Wilhelm Zaum},
  booktitle     = {In: Proceedings International Society for Photogrammetry and Remote Sensing, Workshop CMRT},
  title         = {Robust building detection in aerial images},
  creationdate  = {2016-12-13 17:10:40 +0000},
  date-modified = {2016-12-13 17:11:03 +0000},
  owner         = {ISargent},
  year          = {2005},
}

@Article{VanDerMaaten2008,
  author    = {van der Maaten, Laurens and Geoffrey Hinton},
  title     = {Visualizing Data using {t-SNE}},
  journal   = {Journal of Machine Learning Research},
  year      = {2008},
  volume    = {9},
  pages     = {2579-2605},
  keywords  = {ImageLearn},
  owner     = {ISargent},
  creationdate = {2017.05.29},
}

@Article{MacEachrenRSGMG2005,
  Title                    = {Visualizing geospatial information uncertainty: What we know and what we need to know. Cartography and Geographic Information Science},
  Author                   = {A MacEachren and A Robinson and S Hopper and S Gardner and R Murray and M Gahegan and B Hetzler},
  Year                     = {2005},
  Number                   = {3},
  Pages                    = {139--160},
  Volume                   = {21},

  Abstract                 = {Developing reliable methods for representing and managing information uncertainty remains a persistent and relevant challenge to GIScience. Information uncertainty is an intricate idea, and recent examinations of this concept have generated many perspectives on its representation and visualization, with perspectives emerging from a wide range of disciplines and application contexts. In this paper, we review and assess progress toward visual tools and methods to help analysts manage and understand information uncertainty. Specifically, we report on efforts to conceptualize uncertainty, decision making with uncertainty, frameworks for representing uncertainty, visual representation and user control of displays of information uncertainty, and evaluative efforts to assess the use and usability of visual displays of uncertainty. We conclude by identifying seven key research challenges in visualizing information uncertainty, particularly as it applies to decision making and analysis.},
  Keywords                 = {Quality},
  Owner                    = {ISargent},
  creationdate                = {2015.02.06},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.86.1426&rep=rep1&type=pdf}
}

@Misc{mackay2006,
  Title                    = {Gaussian Process Basics},

  Author                   = {David MacKay},
  Month                    = {6},
  Year                     = {2006},

  Address                  = {Bletchley Park},
  Booktitle                = {Gaussian Processes in Practice Workshop},
  Institution              = {University of Cambridge},
  Owner                    = {ISargent},
  creationdate                = {2016.10.22},
  Url                      = {http://videolectures.net/gpip06_mackay_gpb}
}

@Article{Maguire2001,
  author       = {Maguire, M},
  journaltitle = {International Journal of Human-Computer Studies},
  title        = {Methods to Support Human Centred Design},
  doi          = {doi:10.1006/ijhc.2001.0503},
  pages        = {587--634},
  url          = {http://www.cse.chalmers.se/research/group/idc/ituniv/courses/06/ucd/papers/maguire%202001a.pdf},
  volume       = {55},
  creationdate = {2015.03.13},
  keywords     = {3DCharsPaper},
  owner        = {ISargent},
  year         = {2001},
}

@Article{MahendranV2016,
  author           = {A. Mahendran and A. Vedaldi},
  journaltitle     = {International Journal of Computer Vision},
  title            = {Visualizing deep convolutional neural networks using natural pre-images},
  comment          = {Published version of MahendranV2015. According to presentation at BMVA Deep Learning symposium on 8/7/2016 this paper ``shows how deconvnets don't work'', i.e. they show the strongly responded to parts of the image but this is independant of the specific neuron.

Is this saying the same thing as \cite{AdebayoGMGHK2018}?},
  creationdate     = {2016.07.12},
  keywords         = {toponet metrics, visualisation, deep learning},
  modificationdate = {2023-01-25T09:56:01},
  owner            = {ISargent},
  year             = {2016},
}

@InProceedings{MahendranV2015,
  author           = {Aravindh Mahendran and Andrea Vedaldi},
  booktitle        = {IEEE Conference on Computer Vision and Pattern Recognition},
  title            = {Understanding Deep Image Representations by Inverting Them},
  location         = {Boston, MA, USA},
  url              = {http://arxiv.org/abs/1412.0035},
  comment          = {Model a representation as a function of the input image and then invert this to visualise the representation. Results in 'up-convolutional neural network'. So, given an input image, this work extracts the values at a particular layer (in this case, the penultimate layer) and then learns the mapping directly from this representation to the original image and visualises this. Consider different regularisers and optimisation. Results to really give any insights into what the original network has learned.},
  creationdate     = {2015.07.16},
  keywords         = {ImageLearn, Visualisation, MLStrat Discovery, explainability},
  modificationdate = {2022-04-05T09:43:10},
  owner            = {ISargent},
  year             = {2015},
}

@InProceedings{MarkovM13,
  author    = {Konstantin Markov and Tomoko Matsui},
  title     = {Music Genre Classification Using Gaussian Process Models},
  booktitle = {2013 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING},
  year      = {2013},
  month     = {9},
  address   = {SOUTHAMPTON, UK},
  comment   = {Using a gaussian processes method to classify music to 4 different genres. Interesting, but i personally think unsupervised analysis would be more interesting.},
  keywords  = {Machine Learning},
  owner     = {ISargent},
  creationdate = {2013.10.02},
}

@InProceedings{MarmanisWGSDS2016,
  author       = {D. Marmanis and J. D. Wegner and S. Galliani and K. Schindler and M. Datcu and U. Stilla},
  booktitle    = {ISPRS XXIII CONGRESS},
  date         = {12-19 July},
  title        = {Semantic Segmentation of Aerial Images with an Ensemble of {CNN}s},
  comment      = {Use 'fully convolutional networks' (Zeiler et al., 2010, Long et al., 2015) that ``view the fully connected layers as a large set of 1 — 1 convolutions, such that one can track back the activations at different image locations'' to perform semantic labelling - basically land cover classification of imagery.},
  creationdate = {2016.07.05},
  keywords     = {ImageLearn, DeepLEAP, Remote Sensing, segmentation, MLStrat Segmentation},
  owner        = {ISargent},
  year         = {2016},
}

@InProceedings{MaronR98,
  author       = {Maron, O and Ratan, A. L.},
  title        = {Multiple-instance learning for natural scene classification},
  pages        = {341--349},
  url          = {http://luthuli.cs.uiuc.edu/~daf/courses/Signals%20AI/Papers/MIL-CSP/maron98multipleinstance.pdf},
  comment      = {Images have been classified for retrieval using a range of techniques from simply using the image histogram as well as measures that use the histogram with some information about the spatial distribution of colour in the image. ``The flexible templates constructed by Lipson encode the scene classes as a set of image pathces and qualitative relationships between those patches.'' This could be a codebook. A bag of instances is a set of observations with a label given to the set (either 'this set contains x' or 'this set does not contain x'). If the bag is labelled positively, it remains unknown which instance or instances are x, it is just known that the bag contains x. A way of achieving this is the Diverse Density algorithm. This algorithm maps all the instances from all the bags in the training data in feature space and finds the location that is closest to at least one instance in all the positively labelled bags and furthest from all the instances in the negatively labelled bags. This location of maximum diverse density is then used to classify data in the training set according to their proximity to the location. This paper uses the diverse density on images from the Corel image library (image = bag of instances). The instances are features created by combining RGB values from regions of 20 pixels (into 15-element vector). I find their method of creating these instances a bit odd. They say their results are good for identifying images containing mountains, sunsets, lakes, waterfalls and fields.},
  creationdate = {2013.09.27},
  inbook       = {Proceedings of the Fifteenth International Conference on Machine Learning},
  keywords     = {Machine Learning, Image Processing, Bag of Instances, Classification},
  owner        = {ISargent},
  year         = {1998},
}

@Book{Marr1982,
  author        = {Marr, David},
  title         = {Vision: A Computational Investigation into the Human Representation and Processing of Visual Information},
  year          = {1982},
  publisher     = {Henry Holt and Co., Inc.},
  isbn          = {0716715678},
  address       = {New York, NY, USA},
  comment       = {In Marr's framework, the process of vision constructs a set of representations, starting from a description of the input image and culminating with a description of three-dimensional objects in the surrounding environment. A central theme, and one that has had far-reaching influence in both neuroscience and cognitive science, is the notion of different levels of analysis—in Marr's framework, the computational level, the algorithmic level, and the hardware implementation level.},
  creationdate    = {2016-12-13 13:58:41 +0000},
  date-modified = {2016-12-13 14:00:41 +0000},
  keywords      = {feature extraction, representation learning, imageLearn},
  owner         = {ISargent},
  creationdate     = {2017.04.13},
}

@Misc{Marshall2015,
  author       = {Paul Marshall},
  title        = {Personal Communication: Ordnance Survey's Photogrammetric Surveyors are encouraged to seek advice from, and mentor, each other},
  comment      = {Short discussion with Paul Marshall. ``50\% of training is mentoring''.},
  creationdate = {2015.07.16},
  keywords     = {ImageLearn},
  month        = {6},
  owner        = {ISargent},
  year         = {2015},
}

@Article{SpaceWarps2015a,
  author       = {Philip J. Marshall and Aprajita Verma and Anupreeta More and Christopher P. Davis and Surhud More and Amit Kapadia and Michael Parrish and Chris Snyder and Julianne Wilcox and Elisabeth Baeten and Christine Macmillan and Claude Cornen and Michael Baumer and Edwin Simpson and Chris J. Lintott and David Miller and Edward Paget and Robert Simpson and Arfon M. Smith and Rafael K\''ung and Prasenjit Saha and Thomas E. Collett and Matthias Tecza},
  journaltitle = {arXiv},
  title        = {Space Warps: I. Crowd-sourcing the Discovery of Gravitational Lenses},
  number       = {arXiv:1504.06148},
  url          = {http://arxiv.org/abs/1504.06148},
  comment      = {Space warps is a Zooniverse project detecting gravitational lensing around galaxies. I thought space warps was one of the best projects on Zooniverse. It involved training whilst classification was underway and, because the class that we were trying to capture was very rare, it also included simulated lenses. This enabled instant feedback on classification inputs. As well as findings of classifications this paper also reports on the properties of the crowd that contributed to this work. The project had two stages and the paper looks at changes in effort, contribution, skill and information between the stages.},
  creationdate = {2015.07.10},
  keywords     = {crowdsourcing, RapidDC, MLStrat Experts},
  owner        = {ISargent},
  year         = {2015},
}

@InProceedings{Martensen,
  author       = {Brett N. Martensen},
  booktitle    = {ICCM 2013 The 12th International Conference on Cognitive Modelling},
  title        = {Perceptra: A New Approach to Pattern Classification Using a Growing Network of Binary Neurons (Binons)},
  url          = {http://www.adaptroninc.com/Perceptra_-_ICCM_2013.pdf},
  comment      = {Binons are binary neurons. Perceptra builds a network in response to learning through data. The result is a deterministic model. ``Fechner's law states that human subjective sensation is proportional to the logarithm of the stimulus intensity (Portugal \& Svaiter, 2011)'' See also \url{https://www.linkedin.com/groupItem?view=&item=5876731085613727746&type=member&gid=77471&trk=eml-b2_anet_digest-null-18-null&fromEmail=fromEmail&ut=1-1uxoafUNFmg1}},
  creationdate = {2014.06.10},
  keywords     = {machine learning, neural networks},
  owner        = {ISargent},
  year         = {2013},
}

@Article{Mayer2008,
  author       = {Mayer, H},
  journaltitle = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title        = {Object extraction in photogrammetric computer vision},
  number       = {2},
  pages        = {213-222},
  url          = {https://www.cis.rit.edu/~cnspci/references/dip/urban_extraction/mayer2008.pdf},
  volume       = {63},
  abstract     = {This paper discusses state and promising directions of automated object extraction in photogrammetric computer vision considering also practical aspects arising for digital photogrammetric workstations (DPW). A review of the state of the art shows that there are only few practically successful systems on the market. Therefore, important issues for a practical success of automated object extraction are identified. A sound and most important powerful theoretical background is the basis. Here, we particularly point to statistical modeling. Testing makes clear which of the approaches are suited best and how useful they are for praxis. A key for commercial success of a practical system is efficient user interaction. As the means for data acquisition are changing, new promising application areas such as extremely detailed three-dimensional (3D) urban models for virtual television or mission rehearsal evolve.},
  comment      = {Discusses object extraction from the point of view of key elements: strategy and scale, data sources and GIS data, statistical modelling, geometry and statistics, and learning. Some useful references, e.g. use of pyramid layers for extraction from imagery, Bayesian approaches seem to be about finding planes (data-based or bottom-up modelling), eversible Jump (RJ) Markov Chain Monte Carlo (MCMC) is a thing, there are some papers on recreating facades using basic primitives such as windows and doors, there is a little research that uses machine vision/machine learning approaches. Discussion of quality assessment is data- (rather than user-) focused and mentions of humans are about data capture rather than data use. Regarding scale: ``Our experience is that a multi-scale approach is in many cases useful. Depending on the type of object, smoothing with the linear scale-space, elimination of interfering details by means of gray-scale morphology (K\''{o}the, 1996), or a combination of both such as in (Kimia et al., 1995) is most suitable. In (Mayer and Steger, 1998) we give an example for the application of the latter on road extraction, while in (Mayer, 1998) we show that this is advantageous because it preserves the elongatedness of roads while at the same time suppressing most other objects.''},
  creationdate = {2014.10.29},
  keywords     = {object extraction, 3Dbuildings, road, quality, ImageLearn, Spatial Scale, DeepLEAP1},
  owner        = {ISargent},
  year         = {2008},
}

@InProceedings{MayurathanPN13,
  author    = {B. Mayurathan and Pinidiyaarachchi, U. A. J. and M. Niranjan},
  title     = {Compact Codebook Design For Visual Scene Recognition By Sequential Input Space Carving},
  booktitle = {2013 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING},
  year      = {2013},
  month     = {9},
  address   = {SOUTHAMPTON, UK},
  comment   = {University of Southampton. Images are described by a set of visual descriptors and these are used to build a codebook by sequential input space carving. Applies to PASCAL VOC 2007 dataset, human action recognition and texture classification.},
  keywords  = {Machine Learning, Representation Learning, Computer Vision, ImageLearn},
  owner     = {ISargent},
  creationdate = {2013.09.30},
}

@InProceedings{MccluneMMH2014,
  Title                    = {Automatic Urban 3D Building Reconstruction from Multi-Ray Photogrammetry},
  Author                   = {Andrew P. McClune and Pauline E. Miller and Jon P. Mills and David Holland},
  Booktitle                = {ISPRS Technical Commission III Symposium Conference Proceedings},
  Year                     = {2014},

  Address                  = {Zurich, Switzerland},

  Keywords                 = {3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2014.11.25}
}

@Misc{McCormick2014,
  author       = {Chris McCormick},
  title        = {Intuition Behind Whitening Image Patches},
  year         = {2014},
  howpublished = {Blog Article},
  url={https://chrisjmccormick.wordpress.com/2014/07/23/intuition-behind-whitening-image-patches/},
  month        = {7},
  url          = {https://chrisjmccormick.wordpress.com/2014/07/23/intuition-behind-whitening-image-patches/},
  comment      = {Clear description of why 'whitening' is a necessary pre-processing step before e.g. kmeans. Pixels tend to be correlated with their neighbours so removing this correlation is important to subsequent tests of similarity between image patches.},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  creationdate    = {2015.06.08},
}

@InProceedings{MichelinTTMP13,
  author    = {Michelin, Jean-Christophe and Tierny, Julien and Tupin, Florence and Mallet, Cl{\'e}ment and Paparoditis, Nicolas},
  title     = {Quality Evaluation of 3{D} City Building Models with Automatic Error Diagnosis},
  booktitle = {Proc. of ISPRS Conference on SSG 2013},
  year      = {2013},
  url       = {http://perso.telecom-paristech.fr/~tierny/stuff/papers/michelin_ssg13.pdf},
  comment   = {not read},
  keywords  = {3D, quality, toread},
  owner     = {ISargent},
  creationdate = {2013.10.21},
}

@Article{MichelinTTMP2013,
  author       = {Michelin, Jean-Christophe and Tierny, Julien and Tupin, Florence and Mallet, Cl{\'e}ment and Paparoditis, Nicolas},
  title        = {Quality evaluation of 3D city building Models with automatic error diagnosis},
  journaltitle = {ISPRS-International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  year         = {2013},
  volume       = {1},
  number       = {2},
  pages        = {161--166},
  url          = {http://perso.telecom-paristech.fr/~tierny/stuff/papers/michelin_ssg13.pdf},
  comment      = {Quality assessment of 3D building models to a set of 9 error classes by comparing the models with features derived from aerial photography. Error classes are Erroneous outline, unexisting building, missing inner court, inaccurate footprint, under-segmentation, over-segmentation, inaccurate roof, z-translation, vegetation occlusion.},
  keywords     = {3D Quality, 3DCharsPaper},
  owner        = {ISargent},
  creationdate    = {2015.11.06},
}

@InProceedings{MidhunNPK2014,
  author    = {Midhun, M. E. and Nair, Sarath R and Prabhakar, V. T. Nidhin and Kumar, S. Sachin},
  title     = {Deep Model for Classification of Hyperspectral Image Using Restricted Boltzmann Machine},
  booktitle = {Proceedings of the 2014 International Conference on Interdisciplinary Advances in Applied Computing},
  year      = {2014},
  series    = {ICONIAAC '14},
  publisher = {ACM},
  location  = {Amritapuri, India},
  isbn      = {978-1-4503-2908-8},
  pages     = {35:1--35:7},
  doi       = {10.1145/2660859.2660946},
  url       = {http://doi.acm.org/10.1145/2660859.2660946},
  acmid     = {2660946},
  address   = {New York, NY, USA},
  articleno = {35},
  comment   = {Apply Restricted Boltzmann Machines to hyperspectral imagery to extract features that are then used for classification.},
  keywords  = {ImageLearn, Remote Sensing, Hyperspectral, DeepLEAP},
  numpages  = {7},
  owner     = {ISargent},
  creationdate = {2016.05.11},
}

@InProceedings{MikolovSCCD2013,
  author       = {Tomas Mikolov and Ilya Sutskever and Kai Chen and Greg Corrado and Jeffrey Dean},
  booktitle    = {Advances in neural information processing systems},
  title        = {Distributed Representations ofWords and Phrases and their Compositionality},
  url          = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf},
  abstract     = {The recently introduced continuous Skip-gram model is an efficient method for
learning high-quality distributed vector representations that capture a large number
of precise syntactic and semantic word relationships. In this paper we present
several extensions that improve both the quality of the vectors and the training
speed. By subsampling of the frequent words we obtain significant speedup and
also learn more regular word representations. We also describe a simple alternative
to the hierarchical softmax called negative sampling.
An inherent limitation of word representations is their indifference to word order
and their inability to represent idiomatic phrases. For example, the meanings of
“Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated
by this example, we present a simple method for finding phrases in text, and show
that learning good vector representations for millions of phrases is possible.},
  comment      = {From Bengio lecture: Semantic relations appear as linear relationships in the space of learned representations
• King - Queen ~ Man - Woman
• Paris - France + Italy ~ Rome},
  creationdate = {2017.01.16},
  owner        = {ISargent},
  year         = {2013},
}

@Misc{InferNET12,
  author    = {Minka, T. and Winn, J.M. and Guiver, J.P. and Knowles, D.A.},
  title     = {{Infer.NET 2.5}},
  year      = {2012},
  note      = {Microsoft Research Cambridge},url={http://research.microsoft.com/infernet},
  comment   = {Reference to be cited when using Infer.NET in research.},
  keywords  = {machine learning, probabilistic programming},
  owner     = {ISargent},
  creationdate = {2014.06.04},
}

@InProceedings{MnihH2010,
  author    = {Volodymyr Mnih and Geoffrey Hinton},
  title     = {Learning to Detect Roads in High-Resolution Aerial Images},
  booktitle = {Proceedings of the 11th European Conference on Computer Vision (ECCV)},
  year      = {2010},
  month     = {9},
  abstract  = {Reliably extracting information from aerial imagery is a difficult problem with many practical applications. One specific case of this problem is the task of automatically detecting roads. This task is a difficult vision problem because of occlusions, shadows, and a wide variety of non-road objects. Despite 30 years of work on automatic road detection, no automatic or semi-automatic road detection system is currently on the market and no published method has been shown
to work reliably on large datasets of urban imagery. We propose detecting roads using a neural network with millions of trainable weights which looks at a much larger context than was used in previous attempts at learning the task. The network is trained on massive amounts of data using a consumer GPU. We demonstrate that predictive performance can be substantially improved by initializing the feature detectors using recently developed unsupervised learning methods as well as by taking advantage of the local spatial coherence of the output labels.We show that our method works reliably on two challenging urban datasets that are an order of magnitude larger than what was used to evaluate previous approaches.},
  comment   = {Use a very large data set},
  keywords  = {ImageLearn, remote sensing},
  owner     = {ISargent},
  creationdate = {2017.04.05},
}

@InProceedings{MnihH12,
  author       = {Mnih, Volodymyr and Hinton, Geoffrey E.},
  booktitle    = {International Conference on Machine Learning},
  title        = {Learning to Label Aerial Images from Noisy Data},
  url          = {http://www.cs.toronto.edu/~hinton/absps/noisy_maps.pdf},
  comment      = {Excellent paper on training image labelling using map data. The map data are 'noisy' due to omission (something that should have been mapped has been omitted) and local registration error. ``training to initialize the deep neural network following the approach described in (Nair \& Hinton, 2010) for training Restricted Boltzmann Machines with rectified linear units''. Has a lot in common with convolution neural networks however ``Weight-sharing in convolutional architectures is advantageous on smaller datasets because it helps reduce overfitting by restricting the number of parameters, but we do not need such a restriction because the abundance of labels, combined with random rotations, allows us to avoid overfitting by training on millions of labeled aerial image patches''. Propose a robust loss function that explicitly models asymmetric ommission noise which has the effect that the neural network is penalised less for a confident but incorrect prediction and another that also handles local registration errors where subsampling of translated patches seems to be part of the training. Also of interest is that ``The best published results on this data (Mnih \& Hinton, 2010) make use of a postprocessing procedure that improves the predictions of a base model by training a new predictor that takes a patch of predictions of the base model as input instead of the aerial image''. ``(He \& Zemel, 2008) pointed out that the lack of accurately labeled data is a bottleneck in general image labeling''. Includes references to python classes for matrix algebra and GPU utilisation.},
  creationdate = {2013.10.09},
  keywords     = {Machine Learning, Deep Learning, Aerial Imagery, ImageLearn},
  owner        = {ISargent},
  year         = {2012},
}

@Article{MoonetM2014,
  author       = {Peter Mooney and Jeremy Morley},
  journaltitle = {EuroSDR European Spatial Data Research Official Publication},
  title        = {Crowdsourcing in National Mapping},
  volume       = {64},
  comment      = {Report on Phase 1 of a EuroSDR and AGILE collaboration describing 5 crowdsourcing projects and the work leading up to those projects. Workshops identified key themes around the topic: crowd attention, crowd type, crowd retention, OpenStreetMap, crowdsourcing spatial data from imagery, quality/validation, data conflation and triggering crowdsourcing. The projects variously created mobile apps, webmapping applications, object classifications, map tags and lots of publications. A further Phase (2) is planned or underway. This will continue with the oversight of projects and ``some National mapping Agencies involved ... felt that there will need to be more focus on the social aspects of crowdsourcing of spatial data. What type of communication for the crowd? What are the best channels to engage the crowd?''. May be worth getting in touch if we do a crowdsourcing project.},
  creationdate = {2014.10.03},
  keywords     = {crowdsourcing, RapidDC},
  owner        = {ISargent},
  year         = {2014},
}

@Misc{MordvintsevOT2015,
  author           = {Alexander Mordvintsev and Christopher Olah and Mike Tyka},
  title            = {Inceptionism: Going Deeper into Neural Networks},
  howpublished     = {Web log post},
  url              = {https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html},
  comment          = {Paper about visualisation of the higher layers in a deep network. Firstly used 'inverted' network to enhance an input of random noise to match its model of a given output. Some interesting images of its 'understanding' of a banana, screp, parachute, dumbbell. Alternatively there is ``inceptionism'' by which a trained network is presented with a single image and then any layer is 'asked' to enhance what it has found. Need to impose a prior constraint that the image should have similar statistics to natural images, such as neighboring pixels needing to be correlated At lower levels this is edges, orientations (a bit van Gogh) and at higher levels it is given objects even to the point of finding animals in clouds. A final stage is the generate images starting from random noise - some nice results when a network is trained on a single subject - places.},
  creationdate     = {2015.06.19},
  keywords         = {ImageLearn, Visualisation, TopoNet Metrics, MLStrat Discovery, explainability},
  modificationdate = {2023-01-26T10:22:30},
  month            = {6},
  owner            = {ISargent},
  year             = {2015},
}

@online{MortonXX,
  Title                    = {Virtual City Models},

  Author                   = {Peter James Morton},
  HowPublished             = {Website},url={http://www.virtualcitymodels.co.uk/},
  Year                     = {Last accessed October 2014},

  Abstract                 = {This website acts as a central hub for sharing information regarding virtual city models in general and the development and distribution of virtual city models on a world-wide scale. This website will also host research published by the author (Peter James Morton).},
  Owner                    = {ISargent},
  creationdate                = {2014.10.29},
  Url                      = {http://www.virtualcitymodels.co.uk/}
}

@InProceedings{Morton2013,
  author    = {Peter James Morton},
  title     = {A Global Perspective in the Development and Distribution of VCMs},
  booktitle = {Northumbria Research Conference},
  year      = {2013},
  date      = {May 15th and 16th},
  url       = {http://www.virtualcitymodels.co.uk/uploads/1/0/3/0/10300291/vcms_a_global_perspective_-_poster.pdf},
  comment   = {Nice graphics showing the production (as in if they have been produced) of virtual city models worldwide. Increase has been steady since 2005, 2-year cycle of peaks and troughs.},
  keywords  = {3D, 3DCharsPaper},
  owner     = {ISargent},
  creationdate = {2014.10.29},
}

@InProceedings{MortonHDT2012,
  author    = {Morton, P. J. and Horne, M. and Dalton, R. C. and Thompson, E. M.},
  title     = {Virtual City Models: Avoidance of Obsolescence},
  booktitle = {eCAADe 30th conference: Digital Physicality | Physi cal Digitality},
  year      = {2012},
  date      = {12-14 September},
  pages     = {213- 224},
  address   = {Prague, Czech Republic},
  comment   = {Emerging Issues: File Format (Maintaining a widely used file format); Interoperability (To exchange and use VCM information between software platforms and database structures); Requirement (VCM user requirements change); Accessibility and Usability (VCM user requirements change); Intellectual Property (VCM user requirements change); Specialist Personnel (Retention of specialist personnel or knowledge share between a team); Level-of-Detail (VCM user requirements change) ; Finance (A sustainable financial business model},
  owner     = {ISargent},
  creationdate = {2014.10.29},
}

@InProceedings{MortonTD2012,
  author       = {Morton, P. J. and Thompson, E. M. and Dalton, R. C.},
  booktitle    = {CityGML in National Mapping Workshop},
  date         = {21-22 January},
  title        = {Virtual City Models: A Global Perspective},
  url          = {http://www.virtualcitymodels.co.uk/uploads/1/0/3/0/10300291/vcms_a_global_perspective_short_paper.pdf},
  address      = {Paris, France},
  comment      = {over 1200 virtual city models have been produced worldwide. ``VCM coverage at over 83,000km^2 spread over 80 countries and all continents''},
  creationdate = {2014.10.29},
  owner        = {ISargent},
  year         = {2013},
}

@InProceedings{MoserAK2010,
  author       = {Julia Moser and Florian Albrecht and Bernhard Kosar},
  booktitle    = {5th International Conference on 3D GeoInformation},
  title        = {Beyond Visualisation -- {3D} {GIS} Analyses for Virtual City Models},
  editor       = {Thomas H. Kolbe and Gerhard K\''{o}nig and Claus Nagel},
  address      = {Berlin, Germany},
  comment      = {I'm not really sure what this paper is doing. It seems to be flying the flag for 3D GI analyses functions. Creates 4 real or fictitious case studies and explains how these can be addressed in 3D GIS. Seems that the analyses already exist in GI so I'm not really sure what the outcome of the paper is. I have PDF.},
  creationdate = {2015.11.10},
  keywords     = {3D GI},
  month        = {11},
  owner        = {ISargent},
  year         = {2010},
}

@Article{Mumford92,
  author       = {David Mumford},
  title        = {On the computational architecture of the neocortex II The role of cortico-cortical loops},
  journaltitle = {Biological Cybernetics},
  year         = {1992},
  volume       = {66},
  pages        = {241-251},
  url          = {http://cs.brown.edu/people/tld/projects/cortex/course/suggested_reading_list/supplements/documents/MumfordBC-92.pdf},
  comment      = {Has the dalmation picture and a diagram showing layers and pathways in the cortex},
  keywords     = {Neuroscience},
  owner        = {ISargent},
  creationdate    = {2014.01.28},
}

@Article{Murtagh01011985,
  author       = {Murtagh, F.},
  title        = {A Survey of Algorithms for Contiguity-constrained Clustering and Related Problems},
  journaltitle = {The Computer Journal},
  year         = {1985},
  volume       = {28},
  number       = {1},
  pages        = {82-88},
  doi          = {10.1093/comjnl/28.1.82},
  eprint       = {http://comjnl.oxfordjournals.org/content/28/1/82.full.pdf+html},
  url          = {http://comjnl.oxfordjournals.org/content/28/1/82.abstract},
  abstract     = {A large number of non-parametric clustering algorithms from a wide range of applications in the social sciences, earth sciences, pattern recognition, and image processing, are critically appraised. These algorithms all have the common property of seeking to use a relational-usually contiguity-constraint, in addition to proximity information. The constraint is necessary in many applications for the visualisation of clustering results. The primary objective of this survey is to sketch out the major algorithmic paradigms in current use, with a view towards facilitating the task of algorithm design in this area.},
  comment      = {review of methods for continuity-constrained clustering - clustering that find boundaries in space (in time too?)},
  keywords     = {clustering, LOCUS},
  owner        = {ISargent},
  creationdate    = {2017.01.16},
}

@InProceedings{MusialskiWAWVP2012,
  author      = {Musialski, P. and Wonka, P. and Aliaga, D. G. and Wimmer, M. and van Gool, L. and Purgathofer, W.},
  title       = {A Survey of Urban Reconstruction},
  booktitle   = {{EUROGRAPHICS} 2012 State of the Art Reports},
  year        = {2012},
  pages       = {1--28},
  url         = {https://www.cs.purdue.edu/cgvlab/papers/aliaga/egstar2012.pdf},
  comment     = {comprehensive review of methods of reconstructing buildings from remotely sensed 9and other?) data.},
  institution = {Eurographics Association},
  keywords    = {3DCharsPaper},
  owner       = {ISargent},
  creationdate   = {2015.11.11},
}

@InProceedings{Musungu2015,
  author    = {K. Musungu},
  title     = {Assessing Spatial Data Quality of Participatory {GIS} Studies: A Case Study in {C}ape {T}own},
  booktitle = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences. 2015 Joint International Geoinformation Conference},
  year      = {2015},
  volume    = {II-2/W2},
  month     = {10},
  pages     = {75--82},
  address   = {Kuala Lumpur, Malaysia},
  comment   = {Useful paper for listing different aspects of quality assessment - internal and external; spatial, temporal and thematic, ...must read},
  keywords  = {3DCharsPaper},
  owner     = {ISargent},
  creationdate = {2015.11.11},
}

@online{Nelson2014,
  Title                    = {New building height data released},

  Author                   = {Gemma Nelson},
  url={http://www.ordnancesurvey.co.uk/blog/2014/03/new-building-height-data-released/},
  Month                    = {3},
  Year                     = {2014},

  Keywords                 = {3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2015.01.29},
  Url                      = {http://www.ordnancesurvey.co.uk/blog/2014/03/new-building-height-data-released/}
}

@online{Nelson2013,
  Title                    = {We've launched a new height product -- OS Terrain 5},

  Author                   = {Gemma Nelson},
  url={http://www.ordnancesurvey.co.uk/blog/2013/07/weve-launched-a-new-height-product-os-terrain-5/},
  Month                    = {7},
  Year                     = {2013},

  Keywords                 = {3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2015.01.29},
  Url                      = {http://www.ordnancesurvey.co.uk/blog/2013/07/weve-launched-a-new-height-product-os-terrain-5/}
}

@Misc{NelwamondoMM07,
  author           = {Nelwamondo, Fulufhelo V and Mohamed, Shakir and Marwala, Tshilidzi},
  title            = {Missing Data: A Comparison of Neural Network and Expectation Maximisation Techniques},
  url              = {http://arxiv.org/ftp/arxiv/papers/0704/0704.3474.pdf},
  comment          = {Give different types of missing data. Describe autoencoder neural networks. Use backpropogation. Describes genetic algorithms. GA uses selection, crossover and mutation. ``The method used here combines the use of auto-associative neural networks with genetic algorithms to approximate missing data''. Compare this method to expectation-maximisation for single imputation. The NN-GA algorithm seems to perform better to a higher accuracy and can also give results when input data aren't positive definite. Shame this paper isn't peer reviewed. Maybe they have published something since?},
  creationdate     = {2013.11.15},
  modificationdate = {2023-12-06T08:00:43},
  owner            = {ISargent},
  year             = {2007},
}

@Online{Nervana2017,
  author           = {{Nervana Systems}},
  title            = {neon},
  url              = {http://neon.nervanasys.com/docs/latest/},
  creationdate     = {2017.05.30},
  keywords         = {ImageLearn},
  modificationdate = {2023-12-06T08:01:58},
  owner            = {ISargent},
  year             = {2017},
}

@InProceedings{Nevatia1999,
  author       = {Nevatia, R},
  booktitle    = {ISPRS Proceedings of the International Workshop on 3D Geospatial Data Production},
  title        = {On Evaluation of 3-D Geospatial Modelling Systems},
  address      = {Paris},
  comment      = {''Systems for 3-D geospatial data modelling are becoming mature''},
  creationdate = {2014.11.06},
  keywords     = {3DCharsPaper},
  owner        = {ISargent},
  year         = {1999},
}

@InProceedings{NgYD2015,
  author       = {Ng, Joe Yue-Hei and Fan Yang and Davis, Larry S.},
  booktitle    = {CVPR2015},
  title        = {Exploiting Local Features from Deep Networks for Image Retrieval},
  url          = {https://pdfs.semanticscholar.org/2201/2bcc1e24bddde80f3a5acac84f1deeccd351.pdf},
  comment      = {Looking at instance-based image retrieval using deep networks trained for classification and find that lower level features in the network are important. Also, experiment with changing the scale of input images and using the same feature extraction and encoding methods. ``It is surprising that the behavior of filters in each layer change significantly with respect to the scale of input images. With input images of higher resolution, even the filters at higher layers effectively capture local characteristics of images as well, apart from semantic concepts of objects, thus producing better features and subsequent better retrieval results.''},
  creationdate = {2016.03.10},
  keywords     = {ImageLearn, Spatial Scale},
  owner        = {ISargent},
  year         = {2015},
}

@Book{Nielsen1994,
  author       = {Nielsen, Jakob},
  title        = {Usability engineering},
  publisher    = {Elsevier},
  url          = {https://books.google.co.uk/books?hl=en&lr=&id=DBOowF7LqIQC&oi=fnd&pg=PP1&dq=neilsen+usability&ots=Bk49SPGQvN&sig=CcNjhTZClLrT1_nSdGy2O815RRI#v=onepage&q=want&f=false},
  comment      = {JennyH says: ``In the context of Participatory Design he says 'It is important to realise that participatory design should not just consist of asking users what they want, since users often do not know what they want or what they need, or even what the possibilities are'},
  creationdate = {2015.11.11},
  keywords     = {3DCharsPaper},
  owner        = {ISargent},
  year         = {1994},
}

@Article{NiesterowiczS13,
  author       = {Jacek Niesterowicz and Stepinski, Tomasz F.},
  journaltitle = {Applied Geography},
  title        = {Regionalization of multi-categorical landscapes using machine vision methods},
  pages        = {250--258},
  volume       = {45},
  comment      = {Not as exciting as implied by the publicity \url{http://www.uc.edu/news/NR.aspx?id=18700}. Find the pre-print at \url{http://sil.uc.edu/pdfFiles/appliedGeography_Sept_2013.pdf}. ``The method is underpinned by principles of machine vision rather than more traditional principles stemming from ecological landscape analysis.'' They start with a raster landcover map and perform analysis on tiles from this. Within each tile, the clumping of each landcover type and the extent (no of pixels) of each clump is determined. A 2D histogram is then produced for the existence of landcover type in one axis and extent of cluster on the other. They claim that the use of histograms, rather than a vector of landscape metrics (e.g. HainesYoungC96) makes this novel. I dispute that the first axis is much of a histogram, since the data are nominal. A combination of segmentation and clustering is then performed using the histogram data to produce a map of landscape types (regionalization). Would have been more interesting if applied to imagery rather than classified raster. Could also be applied to topo data such as OS MasterMap Topography Layer.},
  creationdate = {2013.11.06},
  keywords     = {landscape, ImageLearn},
  month        = {12},
  owner        = {ISargent},
  year         = {2013},
}

@Book{NixonA2008,
  Title                    = {Feature Extraction and Image Processing},
  Author                   = {Mark Nixon and Alberto Aguado},
  Year                     = {2008},
  Edition                  = {Second Edition},

  Keywords                 = {feature extraction},
  Owner                    = {ISargent},
  creationdate                = {2017.04.12}
}

@TechReport{Norris2015,
  author       = {James Norris},
  institution  = {United Nations Committee of Experts on Global Geospatial Information Management ({UN-GGIM})},
  title        = {Future Trends in geospatial information management: the five to ten year vision SECOND EDITION},
  comment      = {''The role of National Spatial Data Infrastructures will become increasingly important. They can provide the means to organise and deliver core geographies for many national and global challenges including sustainable development. The paradigm of data availability is changing; there is a huge increase in the tracking and availability of real-time data. It is now recognised that this data is no longer just for mapping and delivery, but for integration, analytics, modelling and aggregation-capable of providing more informed decision making.''},
  creationdate = {2016.09.21},
  keywords     = {DeepLEAP},
  owner        = {ISargent},
  year         = {2015},
}

@Online{Norvig2015,
  author           = {Peter Norvig},
  title            = {Deploying machine learning applications in the Enterprise},
  url              = {https://www.youtube.com/watch?v=BJ2QVzGmb2w},
  comment          = {Quite a good lecture (YouTube video) but mostly just covering the basic ML story at the start. Makes a clear disctiction between Rationalism (~rule-base) and Empiricism (~optimisation) and their respective philosophers Descartes and Hume. Final part lists 10 types of technical debt which can be incurred using ML approaches. Relates to the paper SculleyHGDPECY2014 This seems to be particularly because these approaches can allow rapid creation of solutions. The 10 points are:
Lack of Clear Abstraction Barriers (difficult to find the bug); Changing anything changes everything; Feedback loops (like reinforcement of return priority in Google searches); Attractive nuisance (?); Data dependencies; Configuration dependencies; Optimizer's curse (in deployment, even the best model will perform worse); Allur of standard packages (even though ML is perhaps on 5\% of the s/w in the end); Non-stationarity (there need to be methods for replacing the model); Lack of intuition (how did the model make its decision?).},
  creationdate     = {2016.11.15},
  modificationdate = {2023-12-06T08:03:03},
  owner            = {ISargent},
  year             = {2015},
}

@Article{OlshausenF97,
  author       = {Olshausen, Bruno A. and Field, David J.},
  journaltitle = {Vision Research},
  title        = {Sparse coding with an overcomplete basis set: A strategy employed by V1?},
  number       = {23},
  pages        = {3311--3325},
  volume       = {37},
  comment      = {From SimoncelliO01: ``Olshausen \& Field (1996; 1997) reexamined the relationship between simple cell receptive fields and sparse coding without imposing a particular functional form on the receptive fields. They created a model of images based on a linear superposition of basis functions and adapted these functions so as to maximize the sparsity of the representation (number of basis functions whose coefficients are zero) while preserving information in the images (by maintaining a bound on the mean squared reconstruction error). The set of functions that emerges after training on hundreds of thousands of image patches randomly extracted from natural scenes, starting from completely random initial conditions, strongly resemble the spatial receptive field properties of simple cells - i.e. they are spatially localized, oriented, and band-pass in different spatial frequency bands (Figure 7). This method may also be recast as a probabilistic model that seeks to explain images in terms of components that are both sparse and statistically independent (Olshausen \& Field 1997) and thus is a member of the broader class of ICA algorithms''},
  creationdate = {2013.12.18},
  keywords     = {Representation learning, ImageLearn, DeepLEAP},
  owner        = {ISargent},
  year         = {1997},
}

@Article{OlshausenF96,
  author       = {Bruno A. Olshausen and David J. Field},
  title        = {Emergence of simple-cell receptive field properties by learning a sparse code for natural images},
  journaltitle = {Letters to Nature},
  year         = {1996},
  url          = {https://courses.cs.washington.edu/courses/cse528/11sp/Olshausen-nature-paper.pdf},
  comment      = {The original paper?},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  creationdate    = {2015.02.24},
}

@Misc{Ormerod2015,
  author       = {Andrew Ormerod},
  title        = {Personal Communication: Overview of how Ordnance Survey's Photogrammetric Surveyors are Trained},
  comment      = {Discussion with Andy about how Photogrammetric Surveyors are trained. See log book 2/7/2015. Key aspects of air photo interpretation include use of shadows to infer direction and height/shape of objects, understanding that shadow can be obscuring (tools can be used to enhance this), use cars to determine sizes of other objects, use general knowledge of world e.g. cricket v football pitch, the side of the railway station that is open, interpretation based on context and inference, e.g. this path/stream must continue somewhere, this building is not on private land, use physics to infer e.g. ripples and sediment in water can indicate direction of flow. Use active techniques to identify objects in field visit - discuss how we identify an object's label and function. Encourage the use of imagination, what would scene be like from a ground-perspective. Surveyors build confidence over time. Early days use rules, later on ``I know what they are from experience''. Features can be more or less important. For important featuers there must be high confidence in their label - can use field survey to identify these feature. For less important features the photogrammetric surveyor can make a judgement based on their best estimate.

An important aspect of image interpretation is being able to read context of a feature and interpret its function - often without a complete view and from an unusual angle. An example of this was the discovery of an unusual structure within a recently built railway [building] site. The highly experienced photogrammetric surveyor needed to infer, from the nature of several sets of rails, the activity going on within a roofed building. With the help of several other staff the purpose of the structure was determined to be a type of traverser, of which there are only a handful in Britain.},
  creationdate = {2015.07.16},
  keywords     = {ImageLearn, RapidDC},
  month        = {7},
  owner        = {ISargent},
  year         = {2015},
}

@Misc{OSInsight,
  Title                    = {http://www.ordnancesurvey.co.uk/business-and-government/help-and-support/os-insight/index.html},

  Author                   = {{OS Insight}},
  HowPublished             = {Webpage},
  Note                     = {Last viewed 11th November 2015},
  Year                     = {2014},

  Keywords                 = {3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2014.11.06},
  Url                      = {http://www.ordnancesurvey.co.uk/business-and-government/help-and-support/os-insight/index.html}
}

@online{Osborne2013,
  Title                    = {OS OpenData product update -- OS Terrain 50},

  Author                   = {Melanie Osborne},
  url={http://www.ordnancesurvey.co.uk/blog/2013/04/os-opendata-product-update-os-terrain-50/},
  Month                    = {4},
  Year                     = {2013},

  Keywords                 = {3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2015.01.29},
  Url                      = {http://www.ordnancesurvey.co.uk/blog/2013/04/os-opendata-product-update-os-terrain-50/}
}

@InProceedings{OtteOSWHDZ13,
  author       = {Sebastian Otte and Christoph Otte and Alexander Schlaefer and Lukas Wittig and Gereon H\''uttmann and Daniel Dr\''omann and Andreas Zell},
  booktitle    = {2013 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING},
  title        = {Oct A-Scan Based Lung Tumor Tissue Classification With Bidirectional Long Short Term Memory Networks},
  address      = {SOUTHAMPTON, UK},
  comment      = {Recurrent neural networks 2are able to learn temporal dependencies in data sequences, whereas feed-forwards networks (FFNs), for instance the well known multilayer perceptron (MLP), can only learn static pattern mapping.'' A long short-term memory block overcomes the vanishing gradient problem by trapping the error with a constant error carousel so that it can be conserved for long time periods. LSTM blocks are capable of handling very long time-lags. Where the past context is not enough for learning, bi-directional recurrent neural networks have been devised to introduce the future context. These have two hidden layers that are not connected to each other. The input sequence enters the first layer in a forwards direction but in the 2nd hidden layer it is reversed. The output layer then combines both the past (1st hidden layer) and future (2nd hidden layer) contexts. Use for soft tissue classification when using OCT A-scans using a needle probe.},
  creationdate = {2013.09.30},
  keywords     = {Machine Learning, Neural Networks},
  month        = {9},
  owner        = {ISargent},
  year         = {2013},
}

@Article{OudeElberinkV2011,
  author       = {Oude Elberink, Sander and George Vosselman},
  journaltitle = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title        = {Quality analysis on 3D building models reconstructed from airborne laser scanning data},
  pages        = {157--165},
  volume       = {66},
  comment      = {Perform internal (aka intrinsic not using reference data) quality assessment of 3D models as a combination of Quality of input data and Geometric quality of data features. Quality of input data includes Accuracy of laser point clouds, laser point density (discusses briefly the causes and effect of variations in lidar point density across a scene) and data gaps. Geometric quality of data features includes error modelling of features, roof planes, boundaries of roof faces, and abstraction precision (where reality is ignored for the sake of the model such as leaving out the detail of roof tiles or the inclination of gutters). ``In our research no usage has been made of reference data and we did not include user requirements, although both of them are needed to answer the question whether the modelled data is suitable for a certain application.'' Makes reference to the user ``important is the role of the user,and his user requirements, in defining criteria to indicate the quality of the automatic extracted building models'' and approach to user-focus of quality measures is: ``Future customers of 3D city models...would be well-advised to carefully set up well defined quality criteria.''},
  creationdate = {2015.03.22},
  keywords     = {3D quality, 3DCharsPaper},
  owner        = {ISargent},
  year         = {2011},
}

@TechReport{OvtcharovRKFSC2015,
  author       = {Kalin Ovtcharov and Olatunji Ruwase and Joo-Young Kim and Jeremy Fowers and Karin Strauss and Eric S. Chung},
  institution  = {Microsoft Research},
  title        = {Accelerating Deep Convolutional Neural Networks Using Specialized Hardware},
  url          = {http://research.microsoft.com/pubs/240715/CNN%20Whitepaper.pdf},
  comment      = {Paper describting how MS sped up the compution in their deep convolutional neural nets. includes explanation of CNN.},
  creationdate = {2015.02.27},
  keywords     = {Convolutional neural networks, deep learning, data processing, processor architecture},
  month        = {2},
  owner        = {ISargent},
  year         = {2015},
}

@Book{PaineK,
  author       = {David P. Paine and James D. Kiser},
  title        = {Aerial Photography and Image Interpretation},
  publisher    = {Wiley},
  comment      = {''Photogrammetry is the art of science of obtaining reliable quantitative information (measurements) from aerial photographs (American Society of Photogrammetry 1966). Photo interprestation is the determination of the nature of objects on a photography and the judgement of their significance.},
  creationdate = {2015.06.26},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  year         = {2012},
}

@InProceedings{PalMM2007,
  author    = {Chris Pal and Gideon Mann and Richard Minerich},
  title     = {Putting Semantic Information Extraction on the Map: Noisy Label Models for Fact Extraction},
  booktitle = {Sixth International Workshop on Information Integration on the Web (IIWeb '07)},
  year      = {2007},
  comment   = {Department of Defence funded research into automatically extracting semantic attribution about place from natural language text documents. useful for revisiting probabilistic models. Incorporated into their model is assumption that label contains noise and therefore may be incorrect. This is accounted for by providing a hidden variable, which is the actual label. Seem to use the method to extract features that are relevant for particular relationships, specifically which words indicate that a celebraty is born is a given location. It would be interesting to understand where this and similar work is now, since it would be very valuable to OS. More generally it is interesting to understand methods for supervised learning with noisy labels.},
  keywords  = {natural language processing, machine learning, semantic data},
  owner     = {ISargent},
  creationdate = {2014.06.25},
}

@MastersThesis{IMM2012-06284,
  author    = {R. B. Palm},
  title     = {Prediction as a candidate for learning deep hierarchical models of data},
  year      = {2012},
  comment   = {Reference this in papers using Matlab Deep Learning toolbox},
  keywords  = {deep learning},
  owner     = {ISargent},
  creationdate = {2013.10.25},
}

@Article{PasupathyC1999,
  Title                    = {Responses to Contour Features in Macaque Area V4},
  Author                   = {Pasupathy, Anitha and Connor, Charles E.},
  Year                     = {1999},
  Number                   = {5},
  Pages                    = {2490--2502},
  Volume                   = {82},

  Abstract                 = {The ventral pathway in visual cortex is responsible for the perception of shape. Area V4 is an important intermediate stage in this pathway, and provides the major input to the final stages in inferotemporal cortex. The role of V4 in processing shape information is not yet clear. We studied V4 responses to contour features (angles and curves), which many theorists have proposed as intermediate shape primitives. We used a large parametric set of contour features to test the responses of 152 V4 cells in two awake macaque monkeys. Most cells responded better to contour features than to edges or bars, and about one-third exhibited systematic tuning for contour features. In particular, many cells were selective for contour feature orientation, responding to angles and curves pointing in a particular direction. There was a strong bias toward convex (as opposed to concave) features, implying a neural basis for the well-known perceptual dominance of convexity. Our results suggest that V4 processes information about contour features as a step toward complex shape recognition.},
  ISBN                     = {1522-1598},
  ISSN                     = {0022-3077},
  Journaltitle             = {Journal of Neurophysiology},
  Keywords                 = {ImageLearn},
  Owner                    = {ISargent},
  Publisher                = {American Physiological Society},
  creationdate                = {2015.07.15}
}

@InProceedings{PenattiND2015,
  author       = {Ot\'{a}vio A. B. Penatti and Keiller Nogueira and dos Santos, Jefersson A.},
  booktitle    = {2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  title        = {Do deep features generalize from everyday objects to remote sensing and aerial scenes domains?},
  doi          = {10.1109/CVPRW.2015.7301382},
  pages        = {44-51},
  comment      = {Applies to CNNs/ConvNets, OverFeat and Caffe to remote sensing data. Extracts features from pretrained (on ImageNet) networks and compares these to hand crafted descriptors such as HoG and BoVW in a image classification problem (UC-Merced USGS aerial imagery with land-use classes and Brazilian Coffee Scenes SPOT imagery with coffee and non-coffee labels).

OverFeat has two models, both of which are similar to AlexNet. ``The main objective of this paper is to evaluation the generalization capacity of ConvNets''. 

Find that ConvNets do generalize well to remote sensing applications.},
  creationdate = {2016.05.11},
  issn         = {2160-7508},
  keywords     = {ImageLearn, Remote Sensing, DeepLEAP, MLStrat DLRS},
  month        = {6},
  owner        = {ISargent},
  year         = {2015},
}

@InProceedings{PerronninS2011,
  author    = {Florent Perronnin and Jorge S\'{a}nchez},
  title     = {Compressed Fisher vectors for LSVRC},
  booktitle = {{PASCAL VOC} / {ImageNet} workshop at the The 13th International Conference on Computer Vision},
  year      = {2011},
  address   = {Barcelona},
  comment   = {The best result on ImageNet before AlexNet won with learned features in 2012.},
  keywords  = {imageLearn},
  owner     = {ISargent},
  creationdate = {2017.05.30},
}

@InProceedings{PeterHF2008,
  author       = {Michael Peter and Norbert Haala and Dieter Fritsch},
  booktitle    = {Proceedings of XXI ISPRS Congress},
  title        = {Preserving Ground Plan and Facade Lines for 3D Building Generalization},
  url          = {http://www.isprs.org/proceedings/XXXVII/congress/2_pdf/3_WG-II-3/18.pdf},
  address      = {Beijing, China},
  comment      = {About generalising/generalisation in 3D. References our paper SargentHF07 in that it notes that ``the height of a building's highest point is an important feature for many applications and should not be changed, according to (Sargent et al., 2007)''.},
  creationdate = {2015.03.17},
  keywords     = {3D},
  owner        = {ISargent},
  year         = {2008},
}

@Article{PeternellS2004,
  author       = {Peternell, M and Steiner, T},
  title        = {Reconstruction of piecewise planar objects from point clouds},
  journaltitle = {Computer-Aided Design},
  year         = {2004},
  volume       = {36},
  number       = {4},
  pages        = {333-342},
  url          = {http://www.geometrie.tuwien.ac.at/peternell/rec_planar_final.pdf},
  comment      = {Appears to be classic piont cloud segmentation then plane-fitting method for 3D object extraction.},
  keywords     = {3D, capture},
  owner        = {ISargent},
  creationdate    = {2014.10.29},
}

@Article{PlautM2010,
  author       = {Plaut, D. C. and McClelland, J. L.},
  journaltitle = {Psychological Review},
  title        = {Locating object knowledge in the brain: comment on Bowers's (2009) attempt to revive the grandmother cell hypothesis},
  number       = {1},
  pages        = {284-288},
  volume       = {117},
  comment      = {response to Bowers2009 arguing for the distributed knowledge model},
  keywords     = {ImageLearn, neuroscience},
  owner        = {ISargent},
  creationdate    = {2015.07.15},
  year         = {2010},
}

@Article{PoncianoB2014,
  author       = {Lesandro Ponciano and Francisco Brasileiro},
  journaltitle = {Human Computation},
  title        = {Finding Volunteers' Engagement Profiles in Human Computation for Citizen Science Projects},
  number       = {2},
  pages        = {245-264},
  url          = {http://arxiv.org/abs/1501.02134},
  volume       = {1},
  abstract     = {Human computation is a computing approach that draws upon human cognitive abilities to solve computational tasks for which there are so far no satisfactory fully automated solutions even when using the most advanced computing technologies available. Human computation for citizen science projects consists in designing systems that allow large crowds of volunteers to contribute to scientific research by executing human computation tasks. Examples of successful projects are Galaxy Zoo and FoldIt. A key feature of this kind of project is its capacity to engage volunteers. An important requirement for the proposal and evaluation of new engagement strategies is having a clear understanding of the typical engagement of the volunteers; however, even though several projects of this kind have already been completed, little is known about this issue. In this paper, we investigate the engagement pattern of the volunteers in their interactions in human computation for citizen science projects, how they differ among themselves in terms of engagement, and how those volunteer engagement features should be taken into account for establishing the engagement encouragement strategies that should be brought into play in a given project. To this end, we define four quantitative engagement metrics to measure different aspects of volunteer engagement, and use data mining algorithms to identify the different volunteer profiles in terms of the engagement metrics. Our study is based on data collected from two projects: Galaxy Zoo and The Milky Way Project. The results show that the volunteers in such projects can be grouped into five distinct engagement profiles that we label as follows: hardworking, spasmodic, persistent, lasting, and moderate. The analysis of these profiles provides a deeper understanding of the nature of volunteers' engagement in human computation for citizen science projects},
  comment      = {Define several metrics of degree of engagement. The activity ratio is the poportion of days on which the volunteer was active in relation to the total of datys he reamined linked tot he project. The Daily devoted time is the averaged hours the volunteer remained executing takss on each day he/she is active. The relative activity duration is the ratio fo days during which a volunteer remains linked to t aproject in relation to the total number of days between the volunteer joining the project and the project finishing. The variation of periodicity is the standard deviation of the times elapsed between each pair of sequential active days. Using these metrics they cluster the data.using k-means (interesting method of deciding on value of k - using within-groups sum of squares and average silhouette statistic for two projects, Galaxy Zoo and Milky Way project). K= 5 best optimised the trade-off between the number of groups and the within-group sum of squares. Labelled these 5 clusters as hardwarding, spasmodic, persistent, lasting and moderate.},
  keywords     = {crowdsourcing, RapidDC, MLStrat Experts},
  owner        = {ISargent},
  creationdate    = {2015.01.20},
  year         = {2014},
}

@Article{PrandiRF2010,
  author       = {Federico Prandi and Raffaella Brumana and Francesco Fassi},
  title        = {Semi-Automatic Objects Recognition in Urban Areas Based on Fuzzy Logic},
  journaltitle = {Journal of Geographic Information System},
  year         = {2010},
  volume       = {2},
  pages        = {55-62},
  url          = {http://www.SciRP.org/journal/jgis},
  comment      = {use fuzzy logic and neural networks to recognise and extract objects in DSMs. Haven't really read, doesn't look very successful but perhaps I'm just a cynic.},
  howpublished = {Published Online April 2010},
  keywords     = {3D buildings, machine learning},
  owner        = {ISargent},
  creationdate    = {2014.10.29},
}

@Unpublished{LoughboroughWorkshop,
  author       = {Stuart Pretty},
  date         = {29th November},
  title        = {Your 3D future - how could 3D work for you?},
  note         = {Internal Ordnance Survey document},
  address      = {Holywell Conference Park, Loughborough},
  comment      = {Entire content: Your 3D future - how could 3D work for you? 29th November 2005, Holywell Conference Park, Loughborough Executive Summary Introduction: The aim of the day was to determine what the face of 3D might be in the coming years, what the market demand might be and what role there might be for the known players in the market, including Ordnance Survey. The day was broken into various elements, Ordnance Survey with other independent speakers set scenes for discussion and the basis of break out sessions where the group created views of the future from their group's perspective. The day was facilitated by Robin McLaren [The Know Edge Consortium] to ensure all views were aired and considered. Presentations: Carsten R\''onsdorf opened proceedings with a thought provoking presentation - The world is flat - which we all course instinctively knew was not true, but put the day into perspective. This was followed by the keynote address from Tim Case [Parsons Brinckerhoff] titled ``3-D technologies for Infrastructure Services: Present and future for engineers, planners and asset managers''. Tim spoke from a position of extensive experience and demonstrated this with examples from a wide range of projects. He clearly demonstrated that there was significant activity in the area of 3D; however it was disparate with little or no commonality. One of his concluding recommendations was that there should be a move away from 'pioneering 3D activity' with co-ordination and discussion between major players, with the definition of standards. This was a recurring theme during the day. Dirk D\''orschlag, University of Bonn, presented his work on multiscale urban modelling in City GML. This presentation drew on the work carried out by the University of Bonn which was both academic and commercial. He introduced the concept of Levels of Detail [LOD] and created a base line from which discussions followed. He presented the proposition that there are 5 levels of detail - The five levels of detail were: 0 = region; 1= block model (buildings); 2= thematic surfaces (walls, roofs etc.) and building modules (e.g. balconies); 3= architectural model openings (windows, doors etc.); and 4= indoors. This level of detail was that which they had identified, there was clearly room for more discussion on this point and whether there was scope for additional levels as was identified in at least one of the 'breakout' groups. There was some discussion on Dirk's presentation on the higher levels of detail which included internal detail of buildings, which usefully led into the presentation by Aidan Slingsby. Aidan Slingsby, UCL, presented on 3D Structures within Buildings. His presentation was centred on perhaps the highest level of detail, providing the ability to map for example public access in shopping centres. He explained the difficulty in defining when 'inside' was in fact 'inside' and that it could equally be 'outside'. His definition of 3D defining space and spatial relationships made this concept more readily understandable. Breakout Sessions: Four breakout groups were created for the day. These groups were tasked at various stages during the day to discuss and agree what the future was from their perspective and to 'create' models of what this looked like. These models were revisited after presentations and discussions, creating refinements to these models. These groups were made up with different market sector representatives leading to wide ranging views. Despite the range of views there was a significant amount of connectivity between the groups. Conclusions: Robin McLaren, as facilitator, drew the day to a close by pulling together the conclusions and agreements from those the audience. * Agreement that the immediate issues to be considered are standards and interoperability, and City GML was accepted as a potential step in the right direction. There was a clear mandate from the audience that Ordnance Survey should be leading this activity as the National Mapping Agency. * Agreement that the immaturity of 3D modelling at the moment means that we have no standards and many players, lots of unknowns and little certainty. Requirement for universal, established standards to improve interoperability would lead to more development as the standards would provide a platform for development * Requirement for base-line product of fairly low LOD (1 or 2) to which other LODs can be added by others, resulting in a system of 'federated' data, with many domain and application-specific models of high LOD evolving from the base-line product. * Agreement that LOD 0 to 2 can be included in the generic base dataset. It was suggested that it could be the responsibility of Ordnance Survey to provide the basic 3D infrastructure, upon which others can build and add more detail. * It was suggested that the Olympics 2012 could be a way forward, to be used as a standard bearer, looking at interoperability issues, because, during the Olympics planning, many different industries will be coming together. * Requirement for topology as well as visualization, in order to analyse data and link 2D and 3D. * Requirement for underground modelling, of both natural and man-made objects would become important. * Requirement would include rural modelling as well as urban. This was a new view being based on the need for development to take place in the peri-urban and rural areas in the future, as there will reduced scope for continued development in urban and city areas. * The games industry, where visualization is of impressive quality, was seen as an important sector. * Requirement for augmented reality combined with modelling. * Possibility of looking at '4D' modelling, including airspace mapping, or undersea mapping. Next Steps: As the London Olympics could serve as a showcase for 3D data Ordnance Survey to approach Greater London Authority and find out if they are involved with 3D Geospatial planning. As it was suggested by attendees a follow-up event will be organised, this may be under the OS Insight Programme with more demos and application examples. Ordnance Survey to consider the creation of a Special Interest Group on 3D. Stuart Pretty Senior Product Manager, Height \& Imagery Ordnance Survey},
  creationdate = {2014.11.06},
  keywords     = {3DCharsPaper},
  owner        = {ISargent},
  year         = {2005},
}

@Article{ProvostF13,
  author       = {Foster Provost and Tom Fawcett},
  journaltitle = {Big Data},
  title        = {Datascience and its Relationship to Big Data And Data Driven Decision Making},
  number       = {1},
  pages        = {51-59},
  url          = {http://online.liebertpub.com/doi/pdf/10.1089/big.2013.1508},
  volume       = {1},
  comment      = {An introduction to Data Science for business, from the authors of the book ``Data Science for Business''. Doesn't go into detail about the data science but more about how it can be used. Aimed at the managers etc who will have to oversee data science projects to get the most out of them. Give evidence that the 'productivity' of businesses is enhanced if data science is used for decision-making and data mining. provides an analogy of web 1.0 for current 'big data' technologies. Now, those organisations that have established their data infrastructure in order to be in the game (like establishing their website) are able to move onto big data 2.0 in which they see what these technologies can do for them. Amazon, Google, Wal-Mart are all examples. Lists a number of fundamental concepts of data science. The section, 'Chemistry Is Not About Test Tubes: Data Science vs. the Work of the Data Scientist' describes how early chemists spent a lot of their time doing work that is principally that of a technician, i.e. getting equipment and materials together. Today's data scientists spend a lot of their time getting data together. In fact, when asked to describe their work, AI specialists tended to talk about ``problem solving, writing code, and building systems''. These don't really differentiate AI from other fields. However, ``In ten years' time, the predominant technologies will likely have changed or advanced enough that today's choices would seem quaint. On the other hand, the general principles of data science are not so differerent than they were 20 years ago and likely will change little over the coming decades.},
  creationdate = {2013.09.11},
  keywords     = {Machine Learning},
  owner        = {ISargent},
  year         = {2013},
}

@Misc{Pudwell2016,
  author       = {Sam Pudwell},
  title        = {The real world value of geospatial data},
  year         = {2016},
  howpublished = {Online. Last visited: 4th December 2016},
  note         = {Last visited: 4th December 2016},
  month        = {4},
  url          = {http://www.itproportal.com/2016/04/27/the-real-world-value-of-geospatial-data/},
  comment      = {Includes Trimble infographic demonstrating how geospatial data are used in different sectors.},
  keywords     = {DeepLEAP},
  owner        = {ISargent},
  creationdate    = {2016.12.05},
}

@InProceedings{QuerciaOKC2014,
  author    = {Quercia, Daniele and O'Hare, Neil Keith and Cramer, Henriette},
  title     = {Aesthetic Capital: What Makes London Look Beautiful, Quiet, and Happy?},
  booktitle = {Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work \&\#38; Social Computing},
  year      = {2014},
  series    = {CSCW '14},
  publisher = {ACM},
  location  = {Baltimore, Maryland, USA},
  isbn      = {978-1-4503-2540-0},
  pages     = {945--955},
  doi       = {10.1145/2531602.2531613},
  url       = {http://doi.acm.org/10.1145/2531602.2531613},
  acmid     = {2531613},
  address   = {New York, NY, USA},
  comment   = {Really interesting work. Use crowdsourcing to decide what views of london are beautiful quiet or happy and then extract visual words (you know, the SIFT kind) to indicate what features could be said to be beautiful, ugle, quiety, noisey, happy or sad. Unfortuantely, point-based words don't have much meaning when shown. Would be interesting to take this a step further using deep networks or some such.},
  keywords  = {unsupervised, crowdsourcing},
  numpages  = {11},
  owner     = {ISargent},
  creationdate = {2016.10.19},
}

@Article{QuianQuiroga2012,
  author       = {Quian Quiroga, Rodrigo},
  title        = {Opinion: Concept cells: the building blocks of declarative memory functions},
  journaltitle = {Nature Reviews Neuroscience},
  year         = {2012},
  volume       = {13},
  pages        = {587-597},
  url          = {http://www.nature.com/nrn/journal/v13/n8/full/nrn3251.html},
  comment      = {A review of the evidence of 'concept cells' (grandmother, jennifer anniston, halle berry cells). Excellent article with masses of essential references and information box/figures. receptive fields in the visual cortex.},
  keywords     = {ImageLearn, Neuroscience},
  owner        = {ISargent},
  creationdate    = {2015.07.08},
}

@Book{Rackham86,
  author       = {Oliver Rackham},
  title        = {The History of the Countryside},
  comment      = {''The landscape is like a historic library of 50,000 books, ... Many were written in remote antiquity in languages which have only lately been deciphered; some of the languages are still unknown... Every year a thousand volumes are taken at random by people who cannot read them, and sold for the value of the parchment'' From Broadleaf13a},
  creationdate = {2013.12.03},
  keywords     = {landscape, ImageLearn},
  owner        = {ISargent},
  year         = {1986},
}

@Article{RadfordMC2015,
  author       = {Alec Radford and Luke Metz and Soumith Chintala},
  journaltitle = {arXiv},
  title        = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
  url          = {http://arxiv.org/abs/1511.06434},
  comment      = {fascinating paper describing use of deep convolutional generative adversarial networks for unsupervised learning and generation of images. Remarakble images of bedrooms, faces etec produced. Visualisation of feature space showing smooth transition over this space. I'd like to understand more about the detail of how this is achieved.},
  keywords     = {machine learning, generaImageLearn, ImageLearn, MLStrat Training},
  owner        = {ISargent},
  creationdate    = {2015.12.01},
  year         = {2015},
}

@Article{RamananN11,
  author       = {Amirthalingam Ramanan and Mahesan Niranjan},
  journaltitle = {International Journal of Signal Processing Systems},
  title        = {A review of codebook models in patch-based visual object recognition},
  number       = {3},
  url          = {http://eprints.soton.ac.uk/272867/1/RamananJSPS2011.pdf},
  volume       = {68},
  comment      = {''This review is organised as follows. ... we summarise the widely used visual descriptors, SIFT and SURF, in a patch-based visual object recognition framework. ... we present the bag-of-features approach .. various techniques ... have been used in the literature in constructing visual codebook for object categorisation. The popular K-means method is also described ... together with its drawbacks ... the types of codebook models are discussed ... we provide a review of several codebook models that are prominent in the literature of object recognition or scene classification which have been proposed in the last decade [We] ... discusses a recent work which is free of a codebook model for visual object recognition.'' The codebook-free method uses random forest, bootstrap A codebook is a set of extracted features for images which can be based on spectral and/or spatial information in the image. These features make up a bag-of-features or bag-of-words or bag-of-visual-words for the image by which images can be categorised or classified. Methods can be unsupervised or supervised, the latter resulting in potentially more discriminatory features for the given labels. Worth returning to this review for further references. Looking at the illustrations of the codebooks, they look a lot like sparse encodings. Perhaps they are the hand-cranked version?},
  creationdate = {2013.09.30},
  keywords     = {Machine Learning, Representation Learning, Computer Vision, ImageLearn},
  owner        = {ISargent},
  page         = {333--352},
  year         = {2011},
}

@InProceedings{RanzatoBL07,
  author    = {Marc'Aurelio Ranzato and Y-Lan Boureau and Yann LeCun},
  title     = {Sparse Feature Learning for Deep Belief Networks},
  booktitle = {Advances in Neural Information Processing Systems (NIPS 2007)},
  year      = {2007},
  comment   = {Use for handwritten digits and natural images. Haven't fully read.},
  keywords  = {deep Learning, Sparse coding, ImageLearn, DeepLEAP},
  owner     = {ISargent},
  creationdate = {2013.12.19},
}

@Article{Rao99,
  author       = {Rao, Rajesh P.N.},
  title        = {An optimal estimation approach to visual perception and learning},
  journaltitle = {Vision Research},
  year         = {1999},
  volume       = {39},
  pages        = {1963--1989},
  url          = {http://homes.cs.washington.edu/~rao/vr99.pdf},
  comment      = {Approach here is 'appearance-based' as opposed to '3D model-based' or 'geometry-based' approaches. That is, there is no requirement for explicit 3D models of objects because the representations are derived directly from images. Prior to this work, most of the appearance-based approaches have used fixed models or filters (e.g. SIFT?) rather than learning them from the data. Hand-picking the basis functions can result in failure of the system. However, if the system can autonomously 'tailor its basis vectors to match the statistics of its input stream' an efficient internal model of the input environment can be learned. Describes PCA and why this has been shown to not represent natural images very well. A well-written but long paper that I haven't read in full. Great figure showing 'the estimator' (brain/animal) interacting with 'the world'. Good reference to show reason for learning features from data rather than using predefined models and filters.},
  keywords     = {Neuroscience, neural Networks, receptive fields, ImageLearn},
  owner        = {ISargent},
  creationdate    = {2014.03.18},
}

@Article{RaoB99,
  author       = {Rao, Rajesh P N and Ballard, Dana H},
  journaltitle = {Nature Neuroscience},
  title        = {Predictive coding in the visual cortex: a fufunction interpretation of some extra-classical receptive-field effects},
  pages        = {79-87},
  url          = {http://homes.cs.washington.edu/~rao/nn.pdf},
  volume       = {2},
  abstract     = {We describe a model of visual processing in which feedback connections from a higher- to a lower- order visual cortical area carry predictions of lower-level neural activities, whereas the feedforward connections carry the residual errors between the predictions and the actual lower-level activities. When exposed to natural images, a hierarchical network of model neurons implementing such a model developed simple-cell-like receptive fields. A subset of neurons responsible for carrying the residual errors showed endstopping and other extra-classical receptive-field effects. These result},
  comment      = {Paper test theories of predctive modelling in cortex by building a neural network with several layers and feedback pathways between each layer. By training with several thousand natural images, the learned synaptic weights resulted in receptive field profiles that resembled classic oriented-edge/bar detectors in level 1 or (if using a sparse prior distribution) Gabor wavelets. Level 2 receptive field profiles were combinations of the level 1 profiles. Gives examples from neurological research that support theory and results of this paper (e.g. experiments on cats and monkeys). ``postulates that neural networks learn the statistical regularities of the natural world, signalling deviations from ... regularities to higher processing centres. This reduces redundancy by removing the predictable, and hence redundant, components of the input signal''. ``the raw image-intesity value at each pixel can be replace by the difference between a center pixel value and its spatial prediction from a linear weighted sum of the surrounding values...functional explanation for the center-surround receptive fields...values of a given pixel also tend to correlate over time...interpreted as difference between the actual input and its temporal prediction...similarly, the responses of retinal photoreceptors sensitive to different wavelengths are often correlated because their spectral sensitivities overlap. Thus the L-cone (long-wavelength or 'red' receptor) reponse may predict the M-cone ... resonse and the L- and M-cone responses may preduct the S-cone reponse. Thus, the color-oppenent (red-green) and blue-(red+green) channels in the retina might reflect predictive coding in the chromatic domain similar to that of the spatial and temporal domains''},
  creationdate = {2014.03.18},
  keywords     = {Neuroscience, neural networks, receptive fields, ImageLearn, vision},
  owner        = {ISargent},
  year         = {1999},
}

@Misc{Rasmussen06,
  author       = {C Rasmussen},
  title        = {Advances in Gaussian Processes},
  year         = {2006},
  howpublished = {Tutorial at NIPS},
  url          = {http://nips.cc/Conferences/2006/Media/},
  comment      = {Highly recommended tutorial (video) on Gaussian Processes/Kriging.},
  keywords     = {Kriging, Gaussian Processes, Machine Learning},
  owner        = {ISargent},
  creationdate    = {2014.06.03},
}

@Article{RazavianASC14,
  author       = {Ali Sharif Razavian and Hossein Azizpour and Josephine Sullivan and Stefan Carlsson},
  journaltitle = {CoRR},
  title        = {{CNN} Features off-the-shelf: an Astounding Baseline for Recognition},
  url          = {http://arxiv.org/abs/1403.6382},
  volume       = {abs/1403.6382},
  bibsource    = {dblp computer science bibliography}, 
  biburl       = {http://dblp.uni-trier.de/rec/bib/journals/corr/RazavianASC14},
  comment      = {\url{http://dblp.org} Reference for using pre-trainined CNNs. MelG: ``it has been found by Razavian et al (2014) among others that generic features extracted using a deep network not trained specifically for the task can successfully perform classification tasks.},
  creationdate = {2015.06.01},
  keywords     = {ImageLearn, CNN},
  owner        = {ISargent},
  year         = {2014},
}

@InBook{RegoczeiH1992,
  author    = {Stephen B Regoczei and Graeme Hirst},
  title     = {The psychology of expertise},
  year      = {1992},
  editor    = {Robert R Hoffman},
  publisher = {Springer New York},
  chapter   = {Knowledge and knowledge acquisition in the computation context},
  pages     = {12-25},
  url       = {http://dx.doi.org/10.1007/978-1-4613-9733-5_2},
  comment   = {Very readable introduction to what isknowledge and the issues around its acquisition and representation. Takes the perspective of AI. For a long time AI only looked at knowledge representation, assuming that its acquisition would be trivial. However, there are many considering with turning knowledge in an expert into data/information/knowledge in an expert system. The elicitator cannot be considered unbiased (infer that they impart some bias into knowledge when translating from expert to machine). Knowledge is difficult to represent verbally/in language (izzy: the brain does not representation know as language so how can a machine). There are many approaches to overcoming these issues.},
  keywords  = {ImageLearn},
  owner     = {ISargent},
  creationdate = {2015.07.16},
}

@Article{RemondinoTGNHMTM2016,
  author       = {Fabio Remondino and Isabella Toschi1 and Markus Gerke and Franscesco Nex and David Holland and A. McGill and Talaya Lopez, J. and A. Magarinos},
  date         = {14 June},
  title        = {Oblique Aerial Imagery for NMA - Some Best Practices},
  pages        = {639--645},
  volume       = {XLI-B4},
  abstract     = {Oblique airborne photogrammetry is rapidly maturing and being offered by service providers as a good alternative or replacement of the more traditional vertical imagery and for very different applications (Fig.1). EuroSDR, representing European National Mapping Agencies (NMAs) and research organizations of most EU states, is following the development of oblique aerial cameras since 2013, when an ongoing activity was created to continuously update its members on the developments in this technology. Nowadays most European NMAs still rely on the traditional workflow based on vertical photography but changes are slowly taking place also at production level. Some NMAs have already run some tests internally to understand the potential for their needs whereas other agencies are discussing on the future role of this technology and how to possibly adapt their production pipelines. At the same time, some research institutions and academia demonstrated the potentialities of oblique aerial datasets to generate textured 3D city models or large building block models. The paper provides an overview of tests, best practices and considerations coming from the R\&D community and from three European NMAs concerning the use of oblique aerial imagery.},
  creationdate = {2016.11.22},
  journal      = {International Archives of Photogrammetry, Remote Sensing and Spatial Information Science},
  keywords     = {DeepLEAP1},
  owner        = {ISargent},
  year         = {2016},
}

@Article{RentschK12,
  author       = {M Rentsch and P Krzystek},
  title        = {Lidar strip adjustment with automatically reconstructed roof shapes},
  journaltitle = {The Photogrammetric Record},
  year         = {2012},
  volume       = {27},
  pages        = {272-292},
  comment      = {Adjusting the position of lidar strips to remove discrepencies. Lots of lit review of previous work into this. This technique is in 3D (others have been planimetric only) and uses intersections of roof ridgelines. These ridgelines are derived by extracting the lidar points for a suitable roof (L-shaped or T-shaped) identifying plane direction at each point using RANSAC and clustering points using k-means. Clustering may also use intensity and pulse-width features especially for full waveform lidar. Small clusters are removed which should leave the main planes of the roof for intersection to identify the ridgeline. The two ridgelines of the roof are interesected planimetrically and this is used to adjust the strips (I can't see how this can give an adjustment in 3D but I haven't read the whole paper). Claim the results are as good are some previous major works.},
  keywords     = {Roof Shape, 3D, Shape},
  owner        = {Izzy},
  creationdate    = {2013.04.05},
}

@Article{RomeroGC2016,
  author       = {A. Romero and C. Gatta and G. Camps-Valls},
  journaltitle = {IEEE Transactions on Geoscience and Remote Sensing},
  title        = {Unsupervised Deep Feature Extraction for Remote Sensing Image Classification},
  doi          = {10.1109/TGRS.2015.2478379},
  issn         = {0196-2892},
  number       = {3},
  pages        = {1349-1362},
  url          = {https://arxiv.org/abs/1511.08131},
  volume       = {54},
  comment      = {Unsupervised convolutional networks with a one hot code and constraint of same mean activitation among all outputs. Echos the work in Deep Belief Networks (HintonOT06 BengioLPL2007). Trained network in an unsupervised  layer-wise fashion.
This is the ENFORCING POPULATION AND LIFETIME SPARSITY (EPLS) ALGORITHM.},
  keywords     = {ImageLearn, Unsupervised, CNN, Hyperspectral, Remote Sensing, DeepLEAP, pre-training, unsupervised, MLStrat DLRS},
  month        = {3},
  creationdate    = {2016.09.21},
  year         = {2016},
}

@Article{RonnebergerFB2015,
  author    = {Olaf Ronneberger and Philipp Fischer and Thomas Brox},
  title     = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
  url       = {http://arxiv.org/abs/1505.04597},
  volume    = {abs/1505.04597},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/RonnebergerFB15},
  comment   = {Semantic segmentation of electron microscope images.

CNNs label images. To label pixels / enable localization, usual approach is sliding window approach (CiresanGGS2012) but this is slow and there is a trade-off between (input image size) localization and the use of context meaning that the size. This paper builds on the work of LongSD2015.

Perform data augmentation to address under represented classes.},
  journal   = {CoRR},
  keywords  = {deep learning, segmentation, MLStrat Segmentation},
  owner     = {ISargent},
  creationdate = {2017.03.09},
  year      = {2015},
}

@Article{Root2013,
  author       = {Anton Root},
  title        = {What Motivates the Crowd to Participate?},
  journaltitle = {Crowdsourcing.org},
  year         = {2013},
  month        = {12},
  url          = {http://www.crowdsourcing.org/editorial/what-motivates-the-crowd-to-participate/29590},
  comment      = {Article about Zooniverse and why people participate},
  keywords     = {crowdsourcing, RapidDC},
  owner        = {ISargent},
  creationdate    = {2014.12.08},
}

@Misc{Rosebrock14,
  author       = {Adrian Rosebrock},
  title        = {Get off the deep learning bandwagon and get some perspective},
  year         = {2014},
  howpublished = {Blog in pyimagesearch},
  month        = {6},
  url          = {http://www.pyimagesearch.com/2014/06/09/get-deep-learning-bandwagon-get-perspective},
  comment      = {Article describing fads/crazes in machine learning algorithms through time.},
  keywords     = {machine learning},
  owner        = {ISargent},
  creationdate    = {2014.06.11},
}

@InCollection{RothL2003,
  author       = {Roth, Volker and Tilman Lange},
  booktitle    = {Advances in Neural Information Processing Systems 16},
  title        = {Feature Selection in Clustering Problems},
  editor       = {S. Thrun and L. K. Saul and B. Sch\''{o}lkopf},
  pages        = {473--480},
  publisher    = {MIT Press},
  url          = {http://papers.nips.cc/paper/2486-feature-selection-in-clustering-problems.pdf},
  comment      = {Performs feature selection and clustering at the same time, using Gaussian mixture model to constrain the problem.},
  creationdate = {2016.10.19},
  owner        = {ISargent},
  year         = {2004},
}

@InProceedings{RothermelW2012,
  Title                    = {{SURE} - Photogrammetric Surface Reconstruction from Imagery},
  Author                   = {Rothermel, M. and Wenzel, K.},
  Booktitle                = {{LC3D} Workshop},
  Year                     = {2012},

  Address                  = {Berlin, Germany},

  Date                     = {12},
  Keywords                 = {3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2015.10.20}
}

@Book{Rouse2008,
  author       = {E Rouse},
  title        = {Cranborne Chase and West Wiltshire Downs Area of Outstanding Natural Beauty Historic Landscape Characterisation},
  comment      = {''HLC works at a landscape scale. It recognises that the notion of present day landscape is a human construction. The fabric of the land that individuals and groups use to create their own notion of landscape is the product of thousands of years of human activity, although what remains to be seen today may be very recent, and has undergone successive periods of change and modification. Landscape, therefore, can only be understood if its dynamic nature is taken into account.''},
  creationdate = {2017.04.04},
  keywords     = {Landscape, Characterisation, ImageLearn},
  owner        = {ISargent},
  year         = {2008},
}

@Article{RumelhartHW86,
  author       = {Rumelhart, David E and Hinton, Geoffrey E. and Williams, Ronald J.},
  title        = {Learning representations by back-propogating errors},
  journaltitle = {Nature},
  year         = {1986},
  volume       = {323},
  pages        = {533-536},
  url          = {http://www.iro.umontreal.ca/~pift6266/A06/refs/backprop_old.pdf},
  comment      = {Describe building networks from layers - input layer, any number of intermediate layers, and an output layer. Aim is to 'find the set of weights that ensure that for each input vector the putput vector produced by the network is the same as (or sufficiently close to) the desired output vector'. Paper describes back-propogation, sometimes credited as the first (but see Werbos1974). Well written.},
  keywords     = {Machine Learning, Neural Networks, ImageLearn},
  owner        = {ISargent},
  creationdate    = {2013.12.19},
}

@Article{RussakovskyEtAl2015,
  author       = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
  journaltitle = {International Journal of Computer Vision},
  title        = {ImageNet Large Scale Visual Recognition Challenge},
  doi          = {10.1007/s11263-015-0816-y},
  pages        = {211---252},
  volume       = {115},
  comment      = {The ImageNet paper. Background to ImageNet, the challenge tasks, challenge entries and lots more! ``We investigated the performance of trained human annotators on a sample of 1500 ILSVRC test set images. Our results indicate that a trained human annotator is capable of out-performing the best model (GoogLeNet) by approximately 1.7%(p=0.022)''. I have PDF.},
  creationdate = {2015/1/20},
  keywords     = {DeepLEAP, MLStrat Milestones},
  owner        = {ISargent},
  year         = {2015},
}

@Article{Salakhutdinov2015,
  author       = {Ruslan Salakhutdinov},
  journaltitle = {Annual Review of Statistics and Its Application},
  title        = {Learning Deep Generative Models},
  pages        = {361--85},
  url          = {http://www.cs.toronto.edu/~rsalakhu/papers/annrev.pdf},
  volume       = {2},
  comment      = {Well-written overview of deep generative models.},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  creationdate    = {2016.03.10},
  year         = {2015},
}

@InProceedings{SalakhutdinovH09,
  author       = {R. Salakhutdinov and G. Hinton},
  booktitle    = {AISTATS},
  title        = {Deep Boltzmann Machines},
  pages        = {448--455},
  volume       = {5},
  comment      = {A new learning algorithm for training multilayer Boltzmann machines. Shows difference between deep bolzmann machine and deep belief network. It seems that DBMs are better: ``Deep Boltzmann machines are interesting for several reasons. First, like deep belief networks, DBM's have the potential of learning internal representations that become increasingly complex, which is considered to be a promising way of solving object and speech recognition problems. Second, high-level representations can be built from a large supply of unlabeled sensory inputs and very limited labeled data can then be used to only slightly fine-tune the model for a specific task at hand. Finally, unlike deep belief networks, the approximate inference procedure, in addition to an initial bottomup pass, can incorporate top-down feedback, allowing deep Boltzmann machines to better propagate uncertainty about, and hence deal more robustly with, ambiguous inputs.''},
  creationdate = {2013.10.16},
  keywords     = {Machine Learning, Deep Learning, ImageLearn},
  owner        = {ISargent},
  year         = {2009},
}

@Article{SamadzadeganAHL2005,
  author    = {Samadzadegan, F and Azizi, A and Hahn, M and Lucas, C},
  title     = {Automatic 3D object recognition and reconstruction based on neuro-fuzzy modelling},
  year      = {2005},
  volume    = {59},
  number    = {5},
  pages     = {255--277},
  comment   = {Haven't read because I don't have a copy. Abstract says that neuro-fuzzy approach is used for three-dimensional object recognition and reconstruction (ORR) - the network does the recognition and extracts parameters and then the object is reconstructed based on this. Demonstrate the approach for buildings, cars and trees from aerial colour images of an urban area.},
  keywords  = {3D buildings, machine learning},
  owner     = {ISargent},
  creationdate = {2014.10.29},
}

@Article{SampathS2010,
  author       = {Sampath, A and Shan, J},
  title        = {Segmentation and reconstruction of polyhedral building roofs from aerial lidar point clouds},
  journaltitle = {IEEE Transactions on Geoscience and Remote Sensing},
  year         = {2010},
  volume       = {48},
  number       = {3},
  pages        = {1554--1567},
  comment      = {Clusters points in lidar point cloud according to their surface normals (derived using eigenanalysis on the voronoi cells) then fits planes to these clusters.},
  keywords     = {3D objects3DCharsPaper, 3DCharsPaper},
  owner        = {ISargent},
  creationdate    = {2014.10.29},
}

@Misc{Samuel59,
  author       = {Arthur Samuel},
  comment      = {Purportedly said Machine Learning is the ``field of study that gives computers the ability to learn without being explicitly programmed''},
  creationdate = {2013.10.25},
  owner        = {ISargent},
  year         = {~1959},
}

@InProceedings{dosSantosFTRGPF2012,
  author       = {J. A. dos Santos and F. A. Faria and R. d. S. Torres and A. Rocha and P. H. Gosselin and S. Philipp-Foliguet and A. FalcÃƒÂ£o},
  booktitle    = {Pattern Recognition (ICPR), 2012 21st International Conference on},
  title        = {Descriptor correlation analysis for remote sensing image multi-scale classification},
  pages        = {3078-3081},
  abstract     = {This paper addresses the problem of remote sensing image multi-scale classification by: (i) showing that using multiple scales does improve classification results, but not all scales have the same importance; (ii) showing that image descriptors do not offer the same contribution at all scales, as commonly thought, and some of them are very correlated; (iii) introducing a simple approach to automatically select segmentation scales, descriptors, and classifiers based on correlation and accuracy analysis.},
  comment      = {From PenattiND2015: ``analyzed the effectiveness and the correlation of different low-level descritors in multiple segmentation scales. They also poposed a methodology to select a subset of complementary descriptors for combination''.},
  creationdate = {2016.05.11},
  issn         = {1051-4651},
  keywords     = {ImageLearn, Spatial Scale, Remote Sensing, DeepLEAP1},
  month        = {11},
  owner        = {ISargent},
  year         = {2012},
}

@Misc{Sargent2014,
  Title                    = {Learning representations of large scale aerial photography: Project Proposal},

  Author                   = {Isabel Sargent},
  Note                     = {(SEARCH Title Word:ImageLearn in HP TRIM)},
  Year                     = {2014},

  Keywords                 = {ImageLearn},
  Owner                    = {ISargent},
  creationdate                = {2016.06.20}
}

@Misc{SargentH2014,
  author           = {Isabel Sargent and David Holland},
  date             = {2014},
  title            = {Machine Learning applied to the Creation of User-focused Mapping Products},
  note             = {Poster Presented to Machine Learning Summer School 2014 at AISTATS 2014},
  creationdate     = {2014.11.04},
  keywords         = {3DCharsPaper},
  modificationdate = {2023-12-04T21:58:15},
  month            = {4},
  owner            = {ISargent},
  year             = {2014},
}

@Report{SargentEuroSDR2012,
  author           = {Isabel Sargent and Andr\'{e} Streilein and Pilemann Olsen, Brian and Christine Ressl and Emilio Domenech and Hugues Bruynseels and Mark Tabor and Poul Frederiksen and Tobias Kellenberger and Thomas Lit\'{e}n and Nicolas Champion},
  date             = {2012},
  institution      = {EuroSDR},
  title            = {Goals and requirements of {E}uropean {N}ational {M}apping {O}rganisations for change detection Findings of the {E}uro{SDR} Working Group on {C}ommon goals and requirements for {NMA}s in change detection},
  subtitle         = {Findings of the EuroSDR Working Group on Common goals and requirements for NMAs in change detection},
  creationdate     = {2016.06.21},
  keywords         = {ImageLearn},
  modificationdate = {2023-06-23T17:31:11},
  owner            = {ISargent},
  year             = {2012},
}

@Misc{SargentYSH2016,
  Title                    = {ImageLearn Workshop},

  Author                   = {Isabel Sargent and David Young and Jonathon Hare},
  Note                     = {(FOR slides, search Title Word:ImageLearn in HP TRIM)},
  Year                     = {2015},

  Date                     = {1},
  Institution              = {Ordnance Survey / University of Southampton},
  Keywords                 = {ImageLearn},
  Owner                    = {ISargent},
  creationdate                = {2016.06.20}
}

@Report{SargentYHA2015,
  author           = {Isabel Sargent and David Young and Jonathon Hare and Peter Atkinson},
  date             = {7},
  institution      = {Ordnance Survey / University of Southampton},
  title            = {Machine Learning for Aerial Image Analysis: Literature Review},
  note             = {Internal OS Document},
  url              = {https://ordnancesurvey.sharepoint.com/:b:/r/sites/teampandi/Deep%20Machine%20Learning%20Library/ImageLearn/LitReview.pdf?csf=1&web=1&e=CFJvrI},
  creationdate     = {2016.06.20},
  keywords         = {ImageLearn, MLStrat},
  modificationdate = {2023-12-06T08:01:21},
  owner            = {ISargent},
  year             = {2015},
}

@InProceedings{SargentH2015,
  Title                    = {A machine learning approach to roof shape classification},
  Author                   = {Isabel Melanie Jane Sargent and David Anthony Holland},
  Booktitle                = {RSPSoc, NCEO and CEOI - ST Joint Annual Conference 2015},
  Year                     = {2015},

  Keywords                 = {3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2015.11.11}
}

@Article{ScharsteinS02,
  author       = {Daniel Scharstein and Richard Szeliski},
  title        = {A Taxonomy and Evaluation of Dense Two-Frame Stereo Correspondence Algorithms},
  journaltitle = {International Journal of Computer Vision},
  year         = {2002},
  volume       = {47},
  number       = {1},
  pages        = {7-42},
  comment      = {A review of stereo matching algorithms. Much better comparison than in LazarosSG08. Build a taxonomy based on the assumption that stereo matching usually performs the following steps: 1. matching cost computation 2. cost (support) aggregation 3. disparity computation / optimization 4. disparity refinement Collaboration between Middlebury College and Microsoft Corporation. Created their own test bed of stereo pairs. Good discussion of disparity space.},
  keywords     = {Stereo Matching, 3D},
  owner        = {ISargent},
  creationdate    = {2013.07.22},
}

@Misc{Schmid2013,
  Title                    = {Value of Geospatial Data in Local Services},

  Author                   = {Gesche Schmid},
  HowPublished             = {Presentation to AGI Scotland},
  Month                    = {10},
  Year                     = {2013},

  Booktitle                = {AGI Scotland},
  Keywords                 = {DeepLEAP},
  Owner                    = {ISargent},
  creationdate                = {2016.12.06},
  Url                      = {https://agiscotland.org.uk/2013/10/01/the-value-of-geographic-information-in-local-services-gesche-schmid-review/}
}

@Article{Schmidhuber2015,
  author       = {J\''{u}rgen Schmidhuber},
  title        = {Deep learning in neural networks: An overview},
  pages        = {85--117},
  url          = {https://arxiv.org/abs/1404.7828},
  volume       = {61},
  comment      = {Looks like an excellent review of everything deep learning and neural network including all the very early work in the field (back to Gauss in the 1800s)},
  creationdate = {2016.12.05},
  journal      = {Neural Networks},
  keywords     = {DeepLEAP, ImageLearn, MLStrat Milestones},
  owner        = {ISargent},
  year         = {2015},
}

@InProceedings{ScholzeMV2002,
  author    = {S. Scholze and T. Moons and L. Van Gool},
  booktitle = {Proceedings of the ISPRS Commision III Symposium},
  title     = {A probabilistic approach to roof extraction and reconstruction},
  pages     = {231--236},
  volume    = {34},
  comment   = {Buildings, roofs are made up of planes which are defined using 3D line segments from image matching. Uses a probilistic approach to construct models of buildings.},
  keywords  = {3DCharsPaper},
  month     = {9},
  owner     = {ISargent},
  creationdate = {2014.10.28},
  year      = {2002},
}

@InProceedings{SculleyHGDPECY2014,
  author           = {D. Sculley and Gary Holt and Daniel Golovin and Eugene Davydov and Todd Phillips and Dietmar Ebner and Vinay Chaudhary and Michael Young},
  booktitle        = {SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop)},
  title            = {Machine Learning: The High Interest Credit Card of Technical Debt},
  comment          = {The goal of this paper is highlight several machine learning specific risk factors and design patterns to be avoided or refactored where possible. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, changes in the external world, and a variety of system-level anti-patterns.},
  creationdate     = {2016.11.15},
  modificationdate = {2022-12-11T19:56:45},
  owner            = {ISargent},
  year             = {2014},
}

@MastersThesis{Selvaraj08,
  author    = {Sadhvi Selvaraj},
  title     = {Classification of Roof Types from Aerial Photographs},
  year      = {2008},
  month     = {9},
  comment   = {Mentions geomorphometrics, neural networks, should be re-read.},
  keywords  = {Geomorphometry, aerial imagery},
  owner     = {isargent},
  school    = {The University of Leeds, School of Geography},
  creationdate = {2013.11.07},
}

@Article{SermanetEZMFL2013,
  author        = {Pierre Sermanet and David Eigen and Xiang Zhang and Micha{\''{e}}l Mathieu and Rob Fergus and Yann LeCun},
  title         = {OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks},
  url           = {http://arxiv.org/abs/1312.6229},
  volume        = {abs/1312.6229},
  url    = {http://arxiv.org/abs/1312.6229},
  biburl        = {http://dblp.uni-trier.de/rec/bib/journals/corr/SermanetEZMFL13},
  creationdate  = {2016-12-13 13:58:41 +0000},
  modificationdate = {2016-12-13 14:00:41 +0000},
  journal       = {CoRR},
  keywords      = {ImageLearn},
  owner         = {ISargent},
  year          = {2013},
}

@Article{Shannon48,
  author       = {Shannon, C},
  journaltitle = {Bell Systems Technical Journal},
  title        = {The mathematical theory of communication},
  pages        = {379--423},
  volume       = {27},
  comment      = {From SimoncelliO01: ``Shannon (1948) developed the theory in order to quantify and solve problems in the transmission signals over communication channels. But his formulation of a quantitative measurement of information transcended any specific application, device, or algorithm and has become the foundation for an incredible wealth of scientific knowledge and engineering developments in acquisition, transmission, manipulation, and storage of information. Indeed, it has essentially become a theory for computing with signals''},
  creationdate = {2014.01.14},
  keywords     = {information theory},
  owner        = {isargent},
  year         = {1948},
}

@InCollection{Sherman2005,
  author    = {S. Murray Sherman},
  title     = {Thalamic relays and cortical functioning},
  year      = {2005},
  volume    = {149},
  series    = {Progress in Brain Research},
  publisher = {Elsevier BV},
  isbn      = {ISSN 0079-6123},
  chapter   = {9},
  url       = {http://shermanlab.uchicago.edu/files/ThalRelCorFunct.pdf},
  comment   = {Review of research into the function of the thalamus. Detail of neuron connections, type and nature of neuron, synapse, branches etc. Surprisingly readible for neuroscietific work. In summary, role of thalamus is more than just relay from senses (drivers) to the cortex but rather a modulator of the relayed information that incorporates information from other sources including the cortex and motor regions. Thus thalamus may be linked to behaviour, attention.},
  keywords  = {ImageLearn, Neuroscience},
  owner     = {ISargent},
  creationdate = {2015.07.06},
}

@Article{ShoaibBDSH2015,
  Title                    = {A Survey of Online Activity Recognition Using Mobile Phones},
  Author                   = {Muhammad Shoaib and Stephan Bosch and Durmaz Incel, Ozlem and Hans Scholten and Havinga, Paul J.M.},
  Year                     = {2015},
  Number                   = {15},
  Pages                    = {2059--2085},

  Doi                      = {doi:10.3390/s150102059},
  ISSN                     = {1424-8220},
  Journaltitle             = {Sensors},
  Keywords                 = {LOCUS, clustering},
  Owner                    = {ISargent},
  creationdate                = {2016.10.19},
  Url                      = {http://www.mdpi.com/1424-8220/15/1/2059}
}

@Article{SimoncelliO01,
  author       = {Simoncelli, Eero P and Olshausen, Bruno A},
  journaltitle = {Annual Review of Neuroscience},
  title        = {Natural image statistics and neural representation},
  pages        = {119--216},
  url          = {http://www.cns.nyu.edu/pub/eero/simoncelli01-reprint.pdf},
  volume       = {24},
  abstract     = {It has long been assumed that sensory neurons are adapted, through both evolutionary and developmental processes, to the statistical properties of the signals to which they are exposed. Attneave (1954) and Barlow (1961) proposed that information theory could provide a link between environmental statistics and neural responses through the concept of coding efficiency. Recent developments in statistical modeling, along with powerful computational tools, have enabled researchers to study more sophisticated statistical models for visual images, to validate these models empirically against large sets of data, and to begin experimentally testing the efficient coding hypothesis for both individual neurons and populations of neurons.},
  comment      = {Summary of work exploring the link between environmental statistics and neural responses. ``Natural images are statistically redundant. Many authors have pointed out that of all the visual images possible, we see only a very small fraction (e.g. Attneave 1954, Field 1987 (Field87), Daugman 1989, Ruderman \& Bialek 1994)'' Cover recearch into the topic in terms of: intensity statistics, color statistics, spatial correlations, higher-order statistics and space-time statistics. The section on higher order statistics is especially interesting. ``Field (1987) and Daugman (1989) provided additional direct evidence of the non-Gaussianity of natural images. They noted that the response distributions of oriented bandpass filters (e.g. Gabor filters) had sharp peaks at zero, and much longer tails than a Gaussian density (see Figure 6). Because the density along any axis of a multidimensional Gaussian must also be Gaussian, this constitutes direct evidence that the overall density cannot be Gaussian. Field (1987) argued that the representation corresponding to these densities, in which most neurons had small amplitude responses, had an important neural coding property, which he termed sparseness. By performing an optimization over the parameters of a Gabor function (spatial-frequency bandwidth and aspect ratio), he showed that the parameters that yield the smallest fraction of significant coefficients are well matched to the range of response properties found among cortical simple cells (i.e. bandwidth of 0.5-1.5 octaves, aspect ratio of 1-2 ).'' Then talks about how this developed into the sparse coding work in OlshausenF97.},
  creationdate = {2014.01.14},
  keywords     = {Neuroscience, ImageLearn},
  owner        = {isargent},
  year         = {2001},
}

@Article{SimonyanVZ2013,
  author       = {Karen Simonyan and Andrea Vedaldi and Andrew Zisserman},
  title        = {Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps},
  url          = {http://arxiv.org/abs/1312.6034},
  volume       = {abs/1312.6034},
  biburl       = {http://dblp.uni-trier.de/rec/bib/journals/corr/SimonyanVZ13},
  comment      = {Apply two methods to visualising the representations learned by a deep convnet. The first iterates from a zero image so that the chosen class node is maximally fired producing. The result can then be directly labelled with the class value (although I presume this could be applied to any node in the network - except there is no predefined label for the network). the second method is to rank the pixels of an input image based on their influence on the score for a particular class (they then use this to initialise ``GraphCut-based object segmentation without the need to train dedicated segmentation or detection models''.

From BachBMKMS2020: ``lies between partical derivatives at the input point x and a full Taylor-series around a different point x_0''},
  creationdate = {2017.05.30},
  journal      = {CoRR},
  keywords     = {ImageLearn, visualising learned representations, explaining ML},
  owner        = {ISargent},
  year         = {2013},
}

@InProceedings{SimonyanZ2015,
  author       = {Karen Simonyan and Andrew Zisserman},
  booktitle    = {Proceedings of the International Conference on Learning Representations},
  title        = {Very deep convolutional networks for large-scale image recognition.},
  address      = {San Diego, CA, USA},
  comment      = {The VGG paper. Investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. In HuXHZ2015: ``demonstrate that the depth of the network plays a significant role in improving classification accuracy''.},
  creationdate = {2016.05.09},
  keywords     = {ImageLearn, MLStrat Milestones},
  month        = {5},
  owner        = {ISargent},
  year         = {2015},
}

@InProceedings{SimpsonRPR2012,
  author    = {Simpson, Edwin and Reece, Steven and Penta, Antonio and Ramchurn, Sarvapali D},
  title     = {Using a Bayesian Model to Combine LDA Features with Crowdsourced Responses},
  booktitle = {The Twenty-First Text REtrieval Conference (TREC 2012)},
  year      = {2012},
  date      = {6-9 November},
  url       = {http://www.robots.ox.ac.uk/~reece/publications/TREC12.pdf},
  comment   = {Paper from Oxford and Southampton unis combing crowdsourcing with machine learning to improve performance of document labelling.},
  keywords  = {crowdsourcing, machine Learning, RapidDC},
  owner     = {ISargent},
  creationdate = {2015.11.03},
}

@InProceedings{SimpsonPD2014,
  Title                    = {Zooniverse: Observing the World's Largest Citizen Science Platform},
  Author                   = {Robert Simpson and Kevin R. Page and De Roure, David},
  Booktitle                = {The seond Web Observatory Workshop},
  Year                     = {2014},

  Abstract                 = {This paper introduces the Zooniverse citizen science project and software framework, outlining its structure from an observatory perspective: both as an observable web-based system in itself, and as an example of a platform iteratively
developed according to real-world deployment and used at scale. We include details of the technical architecture of Zooniverse, including the mechanisms for data gathering across the Zooniverse operation, access, and analysis. We consider the lessons that can be drawn from the experience of designing and running Zooniverse, and how this might inform development of other web observatories.},
  Keywords                 = {RapidDC},
  Owner                    = {ISargent},
  creationdate                = {2015.07.20},
  Url                      = {http://wow.oerc.ox.ac.uk/wow-2014-papers/zooniverse-observing-the-world2019s-largest-citizen-science-platform/view}
}

@InProceedings{SipolaCRATBN13,
  author    = {Tuomo Sipola and Fengyu Cong and Tapani Ristaniemi and Vinoo Alluri and Petri Toiviainen and Elvira Brattico and Asoke K. Nandi},
  title     = {Diffusions MAP For Clustering FMRI Spatial MAP Extracted By Independent Component Analysis},
  booktitle = {2013 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING},
  year      = {2013},
  month     = {9},
  address   = {SOUTHAMPTON, UK},
  comment   = {fMRI scans of subjects whilst playing them music. Created diffusions MAP (matrix of distances between observations) and then performed clustering. One cluster in data is intriguing but infortunately we don't know what this is...},
  keywords  = {Machine Learning},
  owner     = {ISargent},
  creationdate = {2013.10.02},
}

@InProceedings{SladeJR2015,
  Title                    = {Semantic and geometric enrichment of 3D geo-spatial building models with photo captions and illustration labels using template matching \& SIFT},
  Author                   = {Jon Slade and Christopher B. Jones and Paul L Rosin},
  Booktitle                = {GISRUK 2015},
  Year                     = {2015},

  Address                  = {Leeds, UK},
  Month                    = {4},
  Note                     = {In preparation},

  Keywords                 = {3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2015.03.06}
}

@Book{Soja1989,
  author       = {Edward W Soja},
  title        = {Postmodern Geographies. The Reassertion of Space in Critical Social Theory},
  publisher    = {Verso},
  url          = {http://isites.harvard.edu/fs/docs/icb.topic844613.files/week2/soja%20-%20postmodern%20geographies.pdf},
  comment      = {Appears to be arguing for the greater emphasis on the study of place/space in social theory so that it is on a par with history/time. One of those marvellous human geography pieces - why use 1 word with 10 obscure ones will do? Referenced in Langlands2015a.},
  creationdate = {2015.07.29},
  keywords     = {human geography, ImageLearn, landscape},
  owner        = {ISargent},
  year         = {1989},
}

@InProceedings{SpringenbergDBR2015,
  author           = {Springenberg, Jost Tobias and Alexey Dosovitskiy and Thomas Brox and Martin Riedmiller},
  booktitle        = {ICLR-2015},
  title            = {Striving for Simplicity: The All Convolutional Net},
  note             = {arXiv:1412.6806},
  url              = {http://arxiv.org/abs/1412.6806},
  comment          = {Investigation into what components of CNNs achieve state of the art performance. Simplify CNN by replacing pooling layers with convolution layers that have a stride > 1 'the all convolutional network'. 'The pooling layer can be seen as performing a feature-wise convolution in which the activation function is replaced by the p-norm'. Can be seen as learning the pooling operation rather than fixing it. rectified linear activations with a final averaging+softmax layer to produce predictions over the whole image ('we make use of the fact this if the image area covered by units in the topmost convolutional layer convers a portion of the image large enough to recognise its content then fully connected layers can also be replaced by simple 1 by 1 convolutions'). Test various architectures against cifar-10, cifar-100 and imagenet. Try different methods for visualising the concepts learned by neurons, all use relu activation. The 'deconvolution' method of Zeiler \& Fergus which works well for lower layers this 'masks out values corresponding to negative entries of the top gradient'. Find that very first layer of the network does not learn gabor filters but higher layers (in example, 3rd layer) do. However, because higher layers have more invariant representations, this method doesn't produce sharp reconstructions at higher layers. An alternative is backpropagation of activation of a target neuron after a forward pass through the network which masks out bottom data. This is related to deconvolution (Simonyan et al 2014). This paper proposes to combine these two approaches as 'guided backpropogation'. In this case, the masking is applied when at least one of the top or bottom data is negative. Method is apparently what is implemented in nvis in neon.},
  creationdate     = {2016.03.07},
  keywords         = {ImageLearn, CNN, visualisation, DeepLEAP, toponet metrics, MLStrat Training, explainability},
  modificationdate = {2022-04-05T09:43:26},
  owner            = {ISargent},
  year             = {2015},
}

@Article{SrivastavaWG2015,
  author    = {Divya Srivastava and Rajesh Wadhvani and Manasi Gyanchandan},
  title     = {A Review: Color Feature Extraction Methods for Content Based Image Retrieval},
  journal   = {IJCEM International Journal of Computational Engineering \& Management},
  year      = {2015},
  volume    = {18},
  issue     = {3},
  comment   = {reference for colour feature extraction},
  keywords  = {ImageLearn, Feature Extraction},
  owner     = {ISargent},
  creationdate = {2017.05.29},
}

@Article{StollTNE2015,
  author       = {Josef Stoll and Michael Thrun and Antje Nuthmann and Wolfgang Einh\''auser},
  journaltitle = {Vision Research},
  title        = {Overt attention in natural scenes: Objects dominate features},
  pages        = {36--48},
  url          = {http://www.sciencedirect.com/science/article/pii/S0042698914002958},
  volume       = {107},
  comment      = {Very useful overview of exsting theories and findings of what guides attention in vision (i.e. what we look at). There have been two basic models - those of ``salience-view'' and those of ``object-view''. Saliency tends to manifest as regions of contrast within the scene and studies have shown that visual attention appears to be drawn to these areas resulting in a good prediction in studies of regions of high attention by identifying salient regions. A popular model for saliency is Itti and Koch's (2000). However, studies have also been able to predict locations of attention as the locations of objects. Further, attention seem to be focused on the centre of objects, where as their salient regions are likely to be the edges that define their outline. If attention is object-based, the location of objects needs to be known. This study finds that the object-view model outperforms the salience-view model. Suggest that ``Attention is likely to act in parallel with object processing rather than being a mere ''pre-processing'' step''. Goes on to discuss the hierarchy of understanding the scene - the relation between parts of objects and the objects, the relationship between objects and the scene. there is evidence of a coarse to fine processing step after an initial estimation of the scene (based on evidence that humans can quickly determine the layout of the scene, even before objects have been recognised). ``It is well conceivable that several scales and several categorical levels (scene, object, proto-objects, parts, features) contribute to attentional guidance. Indeed, recent evidence shows that the intended level of processing (superordinate, subordinate) biases fixation strategies (Malcolm, Nuthmann, \& Schyns, 2014).''},
  creationdate = {2015.06.30},
  keywords     = {ImageLearn, vision, Image Interpretation, psychology, Spatial Scale},
  owner        = {ISargent},
  year         = {2015},
}

@InProceedings{StoterSPBCHRL2013,
  author       = {Stoter, J. E. and Streilein, A. and Pla, M. and Baella, B. and Capstick, D. and Home, R. and Roensdorf, C. and Lagrange, J. P.},
  booktitle    = {8th 3DGeoInfo Conference \& WG II/2 Workshop, Istanbul, Turkey, 27--29 November 2013, ISPRS Archives Volume II-2/W1},
  title        = {Approaches of national {3D} mapping: Research results and standardisation in practice},
  url          = {http://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/II-2-W1/269/2013/isprsannals-II-2-W1-269-2013.pdf},
  comment      = {Describes the approaches to 3D data of the 5 mapping agencies: Swisstopo, ICC, IGN France, The Netherlands, Ordnance Survey GB.
''3D mapping at the national level, two approaches can be distinguished...The first approach ... followed by ICC, IGN and swisstopo ... implements 3D mapping at the most fundamental level ... The second approach (still) considers the 2D data as prime source of data and extends these data into the third dimension to obtain 3D products and services .. Netherlands and OSGB'' ``despite maturing 3D 
technologies, still these are not fully exploited in practise ... [because] ... The unfamiliarity with 3D causes 3D to be considered as something complicated ... there is a big question of who will be the users of 3D and, with that, what will be the user requirements and what is the business case for the needed investment.''

''* More insights in real customer needs of 3D data sets are required (i.e. what topics are relevant and to what extent). These needs should be formulated in terms of what problems 3D can solve.This will stimulate other use of 3D data than visualisation.
* More research is needed on how to fuse data, combining existing data into 3D databases can create problems, i.e. integrating existing non-3D datasets (addresses, boundaries, etc.) with 3D datasets. 
* Consistency between 2D and 3D needs further attention . For most mapping authorities 3D geoinformation originates from their legacy datasets in 2D. In using this as a basis many associated issues emerge.
* Maintenance and incremental updating of sustainable 3D datasets (sustainability vs. visualization) requires further attention.
* The need of national and European policies on spatial information to stimulate the use of available 3D technologies. ``},
  creationdate = {2015.11.06},
  keywords     = {3DCharsPaper, 3D},
  month        = {11},
  owner        = {ISargent},
  year         = {2013},
}

@Article{StrykerSLH1978,
  author       = {M. P. Stryker and H. Sherk and A. G. Leventhal and H. V. Hirsch},
  journaltitle = {Journal of Neurophysiology},
  title        = {Physiological consequences for the cat's visual cortex of effectively restricting early visual experience with oriented contours},
  number       = {4},
  volume       = {41},
  comment      = {Raise cat in environtment devoid of horizontal lines - cat cannot see them.},
  keywords     = {ImageLearn, neuroscience},
  owner        = {ISargent},
  creationdate    = {2015.07.16},
  year         = {1978},
}

@TechReport{SukittanonSPB04,
  author       = {Somsak Sukittanon and Surendran, Arun C. and Platt, John C. and Burges, Chris J. C.},
  institution  = {Microsoft Research},
  title        = {Convolutional Networks for Speech Detection},
  comment      = {Referenced by Arel et al 2010. ``the inherent ability of the system to create robust, learned internal representations is certainly one of the strengths of a convolutional network''},
  creationdate = {2013.12.18},
  keywords     = {ImageLearn, DeepLEAP},
  owner        = {ISargent},
  year         = {2004},
}

@InProceedings{SzegedyLJSRAEVR2015,
  author    = {Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
  booktitle = {Computer Vision and Pattern Recognition (CVPR)},
  title     = {Going Deeper with Convolutions},
  url       = {http://arxiv.org/abs/1409.4842},
  comment   = {The GoogLeNet paper},
  keywords  = {deep learning, MLStrat Milestones},
  owner     = {ISargent},
  creationdate = {2017.07.04},
  year      = {2015},
}

@Article{SzegedyZSBEGF2014,
  author       = {Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus},
  journaltitle = {arXiv:1312.6199v4},
  title        = {Intriguing properties of neural networks},
  comment      = {Intriguing indeed. Firstly show that random directions within trained network can have semantic meaning as much as individual neurons can. Secondly discover 'adversarial examples', examples with tiny perturbations that cause misclassification not just in one network but in many (with different parameters and even trained using different data). Find these adversarial examples by optimsing input to maximise the error. Although these examples are likely to be rare, they exist very close to almost every true example and result in 100\% misclassification. Suggest using these techniques to generate data for training networks to increase generalisation.},
  creationdate = {2016.04.08},
  keywords     = {ImageLearn, toponet metrics},
  owner        = {ISargent},
  year         = {2014},
}

@Article{TackGB12,
  author       = {F Tack and R Goossens and G Buyuksalih},
  title        = {Assessment of a photogrammetric approach for urban DSM extraction from tri-stereoscopic satellite imagery},
  journaltitle = {The Photogrammetric Record},
  year         = {2012},
  volume       = {27},
  number       = {139},
  pages        = {293-310},
  comment      = {Lots of image-matching for DSM generation references. Use high resolution satellite imagery from Ikonos and match 3 images at a time - references trinocular vision and refers to this technique as tri-stereoscopic. These images are the stereopair (March 2002) and a 'nadir' image from May 2005. Following epipolar resampling and rectification, they match using feature points, grid points and 3D edges using images pyramids. Use Wallis filter to enhance and normalise image texture to improve area-based matching. Summed normalised cross correlation (SNCC) is used (rather than NCC) which integrates the similarity constraints of each stereopair. Matching parameters are derived using adaptive determination and self-tuning which takes into account the local terrain types and its complexity. Automatically extracted building edges are added as breaklines to model discontinuities. Result is a 3m grid spacing DSM. The third (nadir) image dramatically improve the inspected quality of the DSM. RMSEs for X, Y and Z position are calculated at 35 check points with planimetric accuracy being better than for vertical and accuracy for the tri-stereoscopic method being slightly better than for the stereoscopic method. These differences between the two approaches are not significant despite the qualitative analysis showing clear improvements. This is because the improvements observed are around building edges where check points are not sited. Quantitative analysis of height accuracy is undertaken by extracting single heights for buildings 'by digitisation of the oulines of the buildings at cornice level'. These were then used to create difference maps 'by subtracting the rasterised 3D building contours from the DSMs produced by a pixel-based approach'. Histograms of these differences were viewed (these had a Gaussian distribution) and statistics of the signed (to show bias: too high or too low) differences between the DSm and the reference data were produced: min dZ, max dZ, mean dZ, std, mean absolute error and RMSE. 'The mean absolute error (MAE) and RMSE parameters both describe the central tendency of the data. The MAE is a linear score, meaning that all the individual differences are weighted equally in the average. The RMSE applies a quadratic scoring rule, measuring the average magnitude of the error. As the errors are squared before they are averaged, the RMSE gives a relativedly high weight to large errors and makes the measure more sensitive to ouliers...The MAE and RMSE values of the tri-stereoscopic model are significantly better compared with the steroscopic approach'. Very interesting paper as much for the methods of QA as for the method of DSM generation.},
  keywords     = {3D, quality, DSM comparison},
  owner        = {Izzy},
  creationdate    = {2013.04.05},
}

@InProceedings{TangSH12,
  author    = {Y. Tang and R. Salakhutdinov and G. Hinton},
  title     = {Tensor Analyzers},
  booktitle = {Deep Learning and Unsupervised Feature Learning NIPS workshop},
  year      = {2012},
  url       = {http://www.cs.toronto.edu/~fritz/absps/ta.pdf},
  comment   = {Factor analysers/ factor analyzers can be used for mixture modelling and modelling of data with real or hypothetical latent variables so long as the interactions are additive. This paper extends factor analysis to tensor analysis. Tensor analysers/tensor analyzers can be used for modelling data that results from multiple groups of latent factors which interact multiplicatively. These models are trained using expectation-maximisation (EM) algorithms which are a 2-stage iterative process for finding the maximum likelihood parameters of a statistical model. The E-step uses Markov Chain Monte Carlo or Annealed Importance Sampling to sample the data and the M-step statistics are derived and the model updated. A number of examples are given. Of interest are models to separate style and content. This is exemplified in a face recognition experiment in which the Tensor Analyzer was show to be able to simultaneously decompose a test image into separate identity and lighting factors.},
  keywords  = {Machine Learning, Source Separation},
  owner     = {ISargent},
  creationdate = {2013.09.12},
}

@Article{TateFLK2006,
  author       = {Andrew J Tate and Hanno Fischer and Andrea E Leigh and Keith M Kendrick},
  title        = {Behavioural and neurophysiological evidence for face identity and face emotion processing in animals},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  year         = {2006},
  volume       = {61},
  pages        = {2155--2172},
  url          = {http://rstb.royalsocietypublishing.org/content/royptb/361/1476/2155.full.pdf},
  comment      = {review of experimental evidence for specialized face processing systems in animals, available from behavioural, electrophysiological and neuroimaging studies. Regions and individual neurons that repond to faces.},
  keywords     = {Imagvision, Neuroscience},
  owner        = {ISargent},
  creationdate    = {2015.07.08},
}

@Misc{Taylor2015,
  Title                    = {Personal Communication: Anecdotal evidence that experienced British Photogrammetric Surveyors have had trouble interpreting aerial imagery from overseas and that overseas Photogrammetric Surveyors have been unable to recognise British landscape features},

  Author                   = {Stefan Taylor},
  Year                     = {2015},

  Keywords                 = {ImageLearn},
  Owner                    = {ISargent},
  creationdate                = {2015.08.18}
}

@InProceedings{TinatiVSLSS2015,
  author       = {Tinati, Ramine and Van Kleek, Max and Simperl, E. and Luczak-R\''osch, Markus and Simpson, Robert and Shadbolt, Nigel},
  booktitle    = {ACM CHI Conference on Human Factors in Computing Systems (CHI2015)},
  title        = {Designing for citizen data analysis: a cross-sectional case study of a multi-domain citizen science platform},
  comment      = {Excellent paper bringing together findings of the Zooniverse with respect to attracting and maintaining participants in citizen science projects. Divide Citizen Science projects into 3 types of task: classifying, marking and transcribing. Identified four themes: Task Specificity (which seems to come down to enabling users to easily colaborate on and discuss the data provided which increased interest in the subject as well as caused serendiptous discovery); Community Development (giving priveleges to experienced participants and support from the science team); Task Design (factors that encourage participants to maintain interest and give better responses); Public Relations and Engagement (how project launchs and campaigns encourage participation). ``Participation in Zoonivrese is inherently non-competitive, and was found to be motivated by altruism, curiosity and other intrinsic drives''. This paper is a brilliant for designing CS or crowdsourcing platforms.},
  creationdate = {2015.09.22},
  keywords     = {RapidDC, crowdsourcing, citizen science, MLStrat Crowdsourcing},
  owner        = {ISargent},
  year         = {2015},
}

@Article{TiwariGS2013,
  author    = {Aastha Tiwari and Anil Kumar Goswami and Mansi Saraswat},
  title     = {Feature Extraction for Object Recognition and Image Classification},
  journal   = {International Journal of Engineering Research \& Technology},
  year      = {2013},
  volume    = {2},
  issue     = {10},
  url       = {http://www.ijert.org/view-pdf/5674/feature-extraction-for-object-recognition-and-image-classification},
  comment   = {Describes all the fundamental ('hard coded') image feature extraction techniques using colour, texture and shape. Gives pseudocode. Compare the different features for time to extract, storage requirements and accuracy of recognition (of objects in image scene). Possibly a useful reference for basic image features and to demonstrate the history of FE from imagery.},
  keywords  = {feature extraction},
  owner     = {ISargent},
  creationdate = {2017.04.12},
}

@Article{TokarczykWWS2014,
  author       = {Piotr Tokarczyk and Jan Dirk Wegner and Stefan Walk and Konrad Schindler},
  journaltitle = {Geoscience and Remote Sensing},
  title        = {Features, Color Spaces, and Boosting: New Insights on Semantic Classification of Remote Sensing Images},
  url          = {http://www.igp.ethz.ch/photogrammetry/publications/pdf_folder/Tokarczyk_etal_TGRS_2014.pdf},
  comment      = {Building on DollarTPB2009 by generating a lot of features and using boosting to select and classify aerial images. Needs further scrutiny.

'' However, in a recent evaluation [34] exploiting deep learning for feature extraction to classify VHR remote sensing data (with random forests), we found that deep learning is brittle to set up, and often, results are not improved compared to simple linear filter banks. ``

''This approach can be viewed as an intuitive intermediate step between deep learning methods [7], [8], and [31--33]) and standard feature banks (e.g., [24], [25], and [44]''},
  creationdate = {2015.06.05},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  year         = {2014},
}

@Article{TonioniSTSS2017,
  author    = {Tonioni, Alessio and Salti, Samuele and Tombari, Federico and Spezialetti, Riccardo and Stefano, Luigi Di},
  title     = {Learning to Detect Good 3D Keypoints},
  journal   = {International Journal of Computer Vision},
  year      = {2017},
  month     = {8},
  issn      = {1573-1405},
  doi       = {10.1007/s11263-017-1037-3},
  url       = {https://doi.org/10.1007/s11263-017-1037-3},
  abstract  = {The established approach to 3D keypoint detection consists in defining effective handcrafted saliency functions based on geometric cues with the aim of maximizing keypoint repeatability. Differently, the idea behind our work is to learn a descriptor-specific keypoint detector so as to optimize the end-to-end performance of the feature matching pipeline. Accordingly, we cast 3D keypoint detection as a classification problem between surface patches that can or cannot be matched correctly by a given 3D descriptor, i.e. those either good or not in respect to that descriptor. We propose a machine learning framework that allows for defining examples of good surface patches from the training data and leverages Random Forest classifiers to realize both fixed-scale and adaptive-scale 3D keypoint detectors. Through extensive experiments on standard datasets, we show how feature matching performance improves significantly by deploying 3D descriptors together with companion detectors learned by our methodology with respect to the adoption of established state-of-the-art 3D detectors based on hand-crafted saliency functions.},
  comment   = {Uses random forests to learn 3D keypoints for image matching},
  day       = {08},
  keywords  = {machine learning, image matching},
  owner     = {ISargent},
  creationdate = {2017.08.17},
}

@Article{TorralbaFF08,
  author       = {Torralba, Antonio and Fergus, Rob and Freeman, William T},
  title        = {80 million tiny images: a large dataset for non-parametric object and scene recognition},
  journaltitle = {IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE},
  year         = {2008},
  volume       = {30},
  number       = {11},
  pages        = {1958-1970},
  url          = {http://people.csail.mit.edu/billf/papers/80millionImages.pdf},
  comment      = {Object recognition is a combination of model and data. recent focus has been on model and this paper redresses the balance by focusing on the data. The dataset that became the cifar10 and cifar100 dataset I think.},
  keywords     = {Machine Learning, Representation Learning, Deep Learning, ImageLearn},
  owner        = {ISargent},
  creationdate    = {2013.06.28},
}

@Article{TorralbaF2014,
  author       = {Antonio Torralba and William T. Freeman},
  journaltitle = {International Journal of Computer Vision},
  title        = {Accidental Pinhole and Pinspeck Cameras. Revealing the Scene Outside the Picture},
  url          = {http://download.springer.com/static/pdf/253/art%253A10.1007%252Fs11263-014-0697-5.pdf?auth66=1415182823_48149fa83d947865fd7441a8433d40ba&ext=.pdf},
  creationdate = {2014.11.05},
  owner        = {ISargent},
  year         = {2014},
}

@Misc{TranBFTP2015,
  author    = {Du Tran and Lubomir Bourdev and Rob Fergus and Lorenzo Torresani and Manohar Paluri},
  title     = {Learning Spatiotemporal Features with 3D Convolutional Networks},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.0767},
  comment   = {Work using a 3D convnet (CNN) to extract representations from video. Salient aspect of movement are encoded, rather than just any change.},
  keywords  = {ImageLearn, Convolutional Neural Networks},
  owner     = {ISargent},
  creationdate = {2015.07.06},
}

@Article{Tranowski1988,
  author       = {Deborah Tranowski},
  title        = {A knowledge acquisition environment for scene analysis},
  journaltitle = {International Journal of Man-Machine Studies},
  year         = {1988},
  volume       = {29},
  number       = {2},
  pages        = {197--213},
  comment      = {Describes a Knowledge Acquisition Environment (KAE) for gathering know ledge from a range of experts involved in aerial image interpretation. There were a number of requirements for this including being able to operate for a number of different domains (which may have different languages) and that experts may not be very computer literate. Result appears to be a series of data input fields presented to the experts, who were all military analysts. The gathered domain information is then turned into intermediate representations such as rules, frames and semantic networks (izzy: onotologies). Possibly one of a kind. I only spotted one citation although there are 13 references. The only citation of this paper seems to be Hoffman2014. Makes me wonder if knowledge management has gone out of favour in recent years when perhaps it is even more vital as (if) more domains develop and there are fewer experts in any single domain (?). Sadly nothing much about image interpretation.},
  keywords     = {ImageLearn, Image Interpretation},
  owner        = {ISargent},
  creationdate    = {2015.06.30},
}

@InProceedings{TriantakonstantisB2009,
  author       = {Triantakonstantis, D. P. and Barr, S. L.},
  booktitle    = {Computational Science and Its Applications: ICCSA 2009, Part 1},
  title        = {A Spatial Structural and Statistical Approach to Building Classification of Residential Function for City-Scale Impact Assessment Studies},
  pages        = {221-236},
  url          = {http://link.springer.com/chapter/10.1007%2F978-3-642-02454-2_16},
  volume       = {5592},
  abstract     = {In order to implement robust climate change adaption and mitigation strategies in cities fine spatial scale information on building stock is required. However, for many cities such information is rarely available. In response, we present a methodology that allows topographic building footprints to be classified to the level of residential spatial topological-building types and corresponding period of construction. The approach developed employs spatial structure and topology to first recognise residential spatial topological types of Detached, Semi-Detached or Terrace. Thereafter, morphological and spatial metrics are employed with multinomial logistic regression to assign buildings to particular periods of construction for use within city-scale impact assessment studies. Overall the system developed performs well for the classification of residential building exemplars for the city of Manchester UK, with an overall accuracy of 83.4%, although with less satisfactory results for the Detached period of construction (76.6%) but excellent accuracies for the Semi-Detached residential buildings (93.0%).},
  comment      = {Classifying buildings using spatial topology of vectors (from OSMM) to get type of building (dtected, semi, terrace) and then morphological and spatial metrics to period of construction for use in assessment of impact of climate change, heatwaves etc.},
  creationdate = {2015.09.08},
  keywords     = {ImageLearn, Landscape, characterisation},
  owner        = {ISargent},
  year         = {2009},
}

@Article{TrierJT1996,
  author    = {Øivind Due Trier and Anil K. Jain and Torfinn Taxt},
  title     = {Feature extraction methods for character recognition-A survey},
  journal   = {Pattern Recognition},
  year      = {1996},
  volume    = {29},
  number    = {4},
  pages     = {641 - 662},
  issn      = {0031-3203},
  doi       = {http://dx.doi.org/10.1016/0031-3203(95)00118-2},
  url       = {http://www.sciencedirect.com/science/article/pii/0031320395001182},
  comment   = {Feature extraction for character recognition. Describes lots of features that can be hard coded. Recommended by NixonA2008 as a review of features for characters.},
  keywords  = {Feature extraction, Optical character recognition, Character representation, Invariance, Reconstructability},
  owner     = {ISargent},
  creationdate = {2017.04.12},
}

@Article{Tsai2012,
  author       = {Chih-Fong Tsai},
  journaltitle = {ISRN Artificial Intelligence},
  title        = {Bag-of-Words Representation in Image Annotation: A Review},
  number       = {Article ID 376804},
  url          = {http://dx.doi.org/10.5402/2012/376804},
  volume       = {2012},
  comment      = {A well written (but not entirely ordered?) article bringing together a lot of work related to bag of visual words methods for image retrieval (which may be thought of as a classification problem). Includes variants of all the stages: ``(i) automatically detect regions/points of interest, (ii) compute local descriptors over those regions/points, (iii) quantize the descriptors into words to form the visual vocabulary, and (iv) find the occurrences in the image of each specific word in the vocabulary for constructing the BoW feature (or a histogram of word frequencies) ``. The BoVW features can then be used for supervised or unsupervised classification, image characterisation, similarity search and image annotation.},
  creationdate = {2016.05.04},
  owner        = {ISargent},
  year         = {2012},
}

@InProceedings{TseDGK2005,
  Title                    = {Building reconstruction using LIDAR data},
  Author                   = {Tse, R. O. C. and Dakowicz, M. and Gold, C. M. and Kidner, D},
  Booktitle                = {4th ISPRS Workshop on Dynamic and Multi-dimensional GIS},
  Year                     = {2005},

  Address                  = {Pontypridd, Wales, UK},
  Pages                    = {156--161},

  Date                     = {9},
  Keywords                 = {3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2015.10.20}
}

@Article{TsengW2003,
  author       = {Tseng, Y.-H. and Wang, S},
  journaltitle = {Photogrammetric engineering and remote sensing},
  title        = {Semiautomated building extraction based on CSG model-image fitting},
  number       = {2},
  pages        = {171--180},
  url          = {http://www.geo.ntnu.edu.tw/files/writing_journal/150/402_b2c69374.pdf},
  volume       = {69},
  comment      = {CSG models are fitted to images. ``The workflow of this approach includes four stages: model ing part) with the shape parameters of length, width, and selection, approximate fitting, optimal fitting, and primitive height''. Seems to be a lot of manual effort in a CAD environment.},
  creationdate = {2014.10.29},
  keywords     = {3DCharsPaper},
  owner        = {ISargent},
  year         = {2003},
}

@InBook{TuzelPM06,
  author    = {Oncel Tuzel and Fatih Porikli and Peter Meer},
  title     = {Computer Vision -- ECCV 2006},
  booktitle = {Computer Vision -- ECCV 2006 Lecture Notes in Computer Science Volume 3952, 2006, pp 589-600},
  year      = {2006},
  volume    = {3952},
  series    = {Lecture Notes in Computer Science},
  publisher = {Springer},
  chapter   = {Region Covariance: A Fast Descriptor for Detection and Classification},
  pages     = {589-600},
  comment   = {Introduce covariance descriptors for images. These are convariances for all features for regions within images. Regions can be any size (covariance matrix remains the same size) and features can be bands, statistics, any information. Would have be buy to read it all.},
  keywords  = {computer vision, ImageLearn},
  owner     = {ISargent},
  creationdate = {2014.03.19},
}

@MastersThesis{Uggla2015,
  author    = {Gustaf Uggla},
  title     = {3D City Models -- A Comparative Study of Methods and Datasets},
  year      = {2015},
  url       = {http://www.diva-portal.org/smash/get/diva2:838813/FULLTEXT01.pdf},
  abstract  = {There are today many available datasets and methods that can be used to create 3D city models, which in turn can be used for numerous applications within the fields of visualization, communication and analysis. The purpose of this thesis is to perform a practical comparison between three methods for 3D city modeling using different combinations of datasets; one using LiDAR data combined with oriented aerial images, one using only oriented aerial images and one using non-oriented aerial images. In all three cases, geometry and textures are derived from the data and the models are imported into the game engine Unity. The three methods are evaluated in terms of the resulting model, the amount of manual work required and the time consumed as well as the cost of data and software licenses. An application example visualizing flooding scenarios in central Stockholm is featured in the thesis to give a simple demonstration of what can be done with 3D city models in a game engine environment. The result of the study shows that combining LiDAR data with oriented images and using a more manual process to create the model gives a higher potential for the result, both in terms of visual appearance and semantic depth. Using only oriented images and commercial software is the easiest and most reliable way to create a usable 3D city model. Non-oriented images and open-source software can be used for 3D reconstruction but is not suited for larger areas or geographic applications. Finding reliable automatic or semi-automatic methods to create semantically rich 3D city models from remote sensed data would be hugely beneficial, as more sophisticated applications could be programmed with the 3D city model as a base.},
  comment   = {Possibly useful for summarising methods of 3D model creation.},
  keywords  = {3D modelling},
  owner     = {ISargent},
  school    = {School of Architecture and the Built Environment, Royal Institute of Technology (KTH), Stockholm, Sweden},
  creationdate = {2015.08.17},
}

@Article{Vailaya1998,
  author       = {Aditya Vailaya and Anil Jain and Hong Jiang Zhang},
  title        = {On Image Classification: City Images Vs. Landscapes},
  journaltitle = {Pattern Recognition},
  year         = {1998},
  volume       = {31},
  number       = {12},
  pages        = {1921 - 1935},
  issn         = {0031-3203},
  doi          = {http://dx.doi.org/10.1016/S0031-3203(98)00079-X},
  url          = {http://www.ee.columbia.edu/~sfchang/course/svia-F03/papers/Vailaya-on-image-classification-city.pdf},
  comment      = {Propose that instead of using hierarchical clustering of images, pairwise classification should be performed using features that discriminate between two classes. Look at the features that are useful for distinguising between landscape and cityscape images. Several colour- and edge-based features are compared and find edge direction features are better (because of tendancy for cityscapes to have strong vertical and horizontal edges, unlike landscapes). useful paper for ways of comparing discriminatory power of features. includes an experience using subjects to group a set of images in to categories that are on meaning to themselves.},
  keywords     = {Image classification},
  owner        = {ISargent},
  creationdate    = {2015.11.16},
}

@InProceedings{VanErpCK2010,
  author       = {Van Erp, Jan B.F. and Anita H.M. Cremers and Judith M. Kessens},
  booktitle    = {5th International Conference on 3D GeoInformation},
  title        = {Challenges in 3D Geo Information and Participatory Design and Decision},
  editor       = {Thomas H. Kolbe and Gerhard K\''{o}nig and Claus Nagel},
  address      = {Berlin, Germany},
  comment      = {paper emphasising the need for Human Factors Engineering technology in the develop of 3D geo information technologies. more about the tools (3D GIS etc) than the data. Describe in detail two scenario - urban planning and crisis management - to illustrate how people would need to be about to use the tools to work well. identifies 3 challenges: Intuitive data access and manipulation, Multi-stakeholder participatory design processes (which should mean that everyone can be involved early and timescales are reduced) and Multisensory experience. I have PDF.},
  creationdate = {2015.11.10},
  keywords     = {3DCharsPaper},
  month        = {11},
  owner        = {ISargent},
  year         = {2010},
}

@Misc{Vardhan2016,
  Title                    = {What is the Impact of Geospatial Information on Society and Economy?},

  Author                   = {Harsha Vardhan},
  HowPublished             = {Online. Last visited: 4th December 2016},
  Month                    = {5},
  Year                     = {2016},

  Keywords                 = {DeepLEAP},
  Owner                    = {ISargent},
  creationdate                = {2016.12.05},
  Url                      = {https://www.geospatialworld.net/what-is-the-impact-of-geospatial-information-on-society-and-economy/}
}

@InProceedings{VikjordS13,
  author    = {Vidar Vikjord and Robert Jenssen},
  title     = {A New Information Theoretic Clustering Algorithm Using K-NN},
  booktitle = {2013 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING},
  year      = {2013},
  month     = {9},
  address   = {SOUTHAMPTON, UK},
  comment   = {One author is from Microsoft in Norway. K-NN information theoretic clustering uses RÃƒÂ©nyi entropy to define the divergence. This paper defines two values of k to describe separately the distance between clusters and the maximum distance that an observation may be from its cluster centroid. The results are very robust to scale differences between clusters.},
  keywords  = {Machine Learning, Clustering},
  owner     = {ISargent},
  creationdate = {2013.09.30},
}

@Book{ViolaJ2001,
  author       = {Viola, P. and Jones, M.},
  title        = {Rapid object detection using a boosted cascade of simple features},
  doi          = {10.1109/CVPR.2001.990517},
  pages        = {I-511-I-518 vol.1},
  volume       = {1},
  abstract     = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the ``integral image'' which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a ``cascade'' which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
  bdsk-url-1   = {http://dx.doi.org/10.1109/CVPR.2001.990517},
  booktitle    = {Computer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on},
  creationdate = {2017.04.13},
  issn         = {1063-6919},
  keywords     = {feature extraction;image classification;image representation;learning (artificial intelligence);object detection;AdaBoost;background regions;boosted simple feature cascade;classifiers;face detection;image processing;image representation;integral image;machine learning;object specific focus-of-attention mechanism;rapid object detection;real-time applications;statistical guarantees;visual object detection;Detectors;Face detection;Filters;Focusing;Image representation;Machine learning;Object detection;Pixel;Robustness;Skin},
  owner        = {ISargent},
  year         = {2001},
}

@Article{VondrickKPMT2016,
  author       = {Carl Vondrick and Aditya Khosla and Hamed Pirsiavash and Tomasz Malisiewicz and Antonio Torralba},
  journaltitle = {International Journal of Computer Vision},
  title        = {Visualizing Object Detection Features},
  pages        = {1--14},
  url          = {http://download.springer.com/static/pdf/845/art%253A10.1007%252Fs11263-016-0884-7.pdf?originUrl=http%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs11263-016-0884-7&token2=exp=1457609829~acl=%2Fstatic%2Fpdf%2F845%2Fart%25253A10.1007%25252Fs11263-016-0884-7.pdf%3ForiginUrl%3Dhttp%253A%252F%252Flink.springer.com%252Farticle%252F10.1007%252Fs11263-016-0884-7*~hmac=88f53f1c58070a22a2786b9992e697edba3cab0a5b8c20d24a213fd1e1955df1},
  comment      = {A general method for visualising features, HOG, CNN},
  creationdate = {2016.03.10},
  owner        = {ISargent},
  year         = {2016},
}

@InProceedings{VondrickPT2016,
  author       = {Carl Vondrick and Hamed Pirsiavash and Antonio Torralba},
  booktitle    = {NIPS2016},
  title        = {Generating Videos with Scene Dynamics},
  url          = {http://web.mit.edu/vondrick/tinyvideo/},
  comment      = {Trained deep net with videos (''equivalent of 2 years'') and then generated 1 second video clips from stills. Izzy: there used ~ 63M seconds of video.},
  creationdate = {2016.09.22},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  year         = {2016},
}

@Article{WacongneLVBND2001,
  author       = {Catherine Wacongne and Etienne Labyt and van Wassenhove,Virginie and Tristan Bekinschtein and Lionel Naccache and Stanislas Dehaene},
  journaltitle = {Proceedings of the National Academy of Sciences of the United States of America},
  title        = {Evidence for a hierarchy of predictions and prediction errors in human cortex},
  url          = {http://www.researchgate.net/profile/Catherine_Wacongne/publication/51858611_Evidence_for_a_hierarchy_of_predictions_and_prediction_errors_in_human_cortex/links/0c9605284dd9c4f6ac000000.pdf},
  comment      = {Study of brain responses to auditory stimuli to determine if the brain predicts sensory data.''imultaneous EEE- magnetoen-
cephalographic recordings verify those predictions and thus strongly support the predictive coding hypothesis. Higher-order predictions appear to be generated in multiple areas of frontal and associative cortices.''},
  creationdate = {2015.07.16},
  keywords     = {ImageLearn, neuroscience},
  owner        = {ISargent},
  year         = {2001},
}

@InCollection{WagnerAC2013,
  author       = {D. Wagner and N. Alam and V. Coors},
  booktitle    = {Urban and Regional Data Management UDMS Annual},
  title        = {Chapter 18. Geometric validation of 3D city models based on standardized quality criteria},
  editor       = {Claire Ellul and Sisi Zlatanova and Massimo Rumor and Robert Laurini},
  pages        = {197--210},
  publisher    = {CRC Press},
  comment      = {Haven't read all but seems to do some very fine-scale quality analysis on the geometry and topology of 3D models - logical consistency rather than accuracy. Another paper that seems to think that the user should know what they need: ``General recommendations for the public to specify model properties in detail are helpful during the production process''},
  creationdate = {2015.03.22},
  keywords     = {3D quality, 3DCharsPaper},
  owner        = {ISargent},
  year         = {2013},
}

@InProceedings{WagnerCB2014,
  author    = {Wagner, D. and Coors, V. and Benner, J.},
  title     = {Semantic validation of {GML}-based geospatial data},
  booktitle = {9th {3DGeoInfo}},
  year      = {2014},
  date      = {11-13 November 2014},
  editor    = {M. Breunig and M. Al-Doori and E. Butwilowski and P. V. Kuper and J. Benner and K.-H. Haefele},
  url       = {http://digbib.ubka.uni-karlsruhe.de/volltexte/documents/3284847},
  address   = {Dubai, UAE},
  comment   = {Another paper about quality checking of 3D data in GML format (CityGML, XPlanGML). Call it 'semantic validation' but to me this is logical consistency checking against the schema. E.g. checks child elements are allowed in the schema.},
  keywords  = {3D Quality},
  owner     = {ISargent},
  creationdate = {2015.11.09},
}

@InProceedings{Wagstaff12,
  author       = {Kiri Wagstaff},
  booktitle    = {International Conference on Machine Learning},
  title        = {Machine Learning that Matters},
  url          = {http://arxiv.org/ftp/arxiv/papers/1206/1206.4656.pdf},
  comment      = {''Machine learning has effectively solved spam email detection (Zdziarski, 2005) and machine translation (Koehn et al., 2003), two problems of global import'' ``And yet we still observe a proliferation of published ML papers that evaluate new algorithms on a handful of isolated benchmark data sets'' ``Increasingly, ML papers that describe a new algorithm follow a standard evaluation template. After presenting results on synthetic data sets to illustrate certain aspects of the algorithm's behavior, the paper reports results on a collection of standard data sets'' ``Virtually none of the ML researchers who work with these data sets happen to also be experts in the relevant scientific disciplines... the ML field neither motivates nor requires such interpretation'' `` There is no expectation that the authors report whether an observed x\% improvement in performance promises any real impact for the original domain. Even when the authors have forged a collaboration with qualified experts, little paper space is devoted to interpretation, because we (as a field) do not require it'' `` It is as if we have forgotten, or chosen to ignore, that each data set is more than just a matrix of numbers. Further, the existence of the UCI archive has tended to over-emphasize research effort on classification and regression problems, at the expense of other ML problems (Langley, 2011).'' ``Jaime Carbonell, then editor of Machine Learning, wrote in 1992 that ``the standard Irvine data sets are used to determine percent accuracy of concept classification, without regard to performance on a larger external task (Carbonell, 1992)'' ``Most often, an abstract evaluation metric (classification accuracy, root of the mean squared error or RMSE, F-measure (van Rijsbergen, 1979), etc.) is used. These metrics are abstract in that they explicitly ignore or remove problem-specific details'' ``Receiver Operating Characteristic (ROC) curves are used to describe a system's behavior for a range of threshold settings, but they are rarely accompanied by a discussion of which performance regimes are relevant to the domain. The common practice of reporting the area under the curve (AUC) (Hanley \& McNeil, 1982) has several drawbacks, including summarizing performance over all possible regimes even if they are unlikely ever to be used (e.g., extremely high false positive rates), and weighting false positives and false negatives equally, which may be inappropriate for a given problem domain (Lobo et al., 2008)''},
  creationdate = {2013.12.06},
  keywords     = {machine learning, ImageLearn, domain expertise},
  owner        = {ISargent},
  year         = {2012},
}

@InProceedings{Wagstaff2004,
  author    = {Kin Wagstaff},
  title     = {Clustering with Missing Values : No Imputation Required},
  booktitle = {Proceedings of the Meeting of the International Federation of Classfication Societies},
  year      = {2004},
  pages     = {649-658},
  url       = {http://www.litech.org/~wkiri/Papers/wagstaff-missing-ifcs04.pdf},
  comment   = {Paper dealing with k-means clustering of sparse data. use soft constraints.},
  keywords  = {ImageLearn},
  owner     = {ISargent},
  creationdate = {2015.07.09},
}

@Article{Walters1987,
  author       = {Deborah Walters},
  journaltitle = {Computer Vision, Graphics, and Image Processing},
  title        = {Selection of image primitives for general-purpose visual processing},
  number       = {2},
  pages        = {261--298},
  volume       = {37},
  comment      = {Fascinating paper looking at what are the visual features in the image that enable it be to be interpreted. Stumbled on it when looking up 'image primatives'. Useful literature review for understanding (in 1987) of human visual system. Evidence that line drawings are interpreted using the same structures as natural images. Some lines are regarded by observers as more significant than others, but this has little to do with the intensity or gradiant amplitude of the edge. Talks about illusory contours - those lines that don't exist in the image but which we perceive as in optical illusions. I suggest this is is opposite to contours that we readily ignore, such as shadow edges (which probably relates to the perceived edge signficance). Discusses several psychophysical experiments and findi that ``patterns with end-connected lines are preferentially processed by th eearly levesl of the human visual system''. Use results to create a 'contrast enhancement' algorithm and a segmentation algorithm (or its alternative, a grouping algorithm).},
  creationdate = {2015.06.18},
  keywords     = {ImageLearn, Vision, Psycholofy},
  owner        = {ISargent},
  year         = {1987},
}

@Article{WangSA2013,
  author    = {Caixia Wang and Anthony Stefanidis and Peggy Agouris},
  title     = {Spatial content-based scene similarity assessment},
  journal   = {ISPRS Journal of Photogrammetry and Remote Sensing},
  year      = {2012},
  volume    = {69},
  pages     = {103--120},
  comment   = {Paper using graphs to compare two scenes, potentially from different modalities, to determin if they are similar/the same.},
  keywords  = {Graph data},
  owner     = {ISargent},
  creationdate = {2017.06.26},
}

@Article{WangJYCHZ2016,
  author    = {Jingdong Wang and Huaizu Jiang and Zejian Yuan and Ming-Ming Cheng and Xiaowei Hu and Nanning Zheng},
  title     = {Salient Object Detection: A Discriminative Regional Feature Integration Approach},
  journal   = {International Journal of Computer Vision},
  year      = {2016},
  pages     = {1--18},
  comment   = {Saved in library folder. A way of identifying saliency in an image. Compares previous methods:contrast feature, image-specific backgroundness feature, objectness feature to this approach: discriminative regional feature integration. Examples are only images with simple foreground-background character.},
  owner     = {ISargent},
  creationdate = {2016.12.06},
}

@Article{WangXCLWXCT2016,
  author       = {Wang, Yongjun and Xu, Hao and Cheng, Liang and Li, Manchun and Wang, Yajun and Xia, Nan and Chen, Yanming and Tang, Yong},
  journaltitle = {Remote Sensing},
  title        = {Three-Dimensional Reconstruction of Building Roofs from Airborne LiDAR Data Based on a Layer Connection and Smoothness Strategy},
  doi          = {10.3390/rs8050415},
  issn         = {2072-4292},
  number       = {5},
  pages        = {415},
  url          = {http://www.mdpi.com/2072-4292/8/5/415},
  volume       = {8},
  abstract     = {A new approach for three-dimensional (3-D) reconstruction of building roofs from airborne light detection and ranging (LiDAR) data is proposed, and it includes four steps. Building roof points are first extracted from LiDAR data by using the reversed iterative mathematic morphological (RIMM) algorithm and the density-based method. The corresponding relations between points and rooftop patches are then established through a smoothness strategy involving ÃƒÂ¢Ã¢â€šÂ¬Ã…â€œseed point selection, patch growth, and patch smoothing.ÃƒÂ¢Ã¢â€šÂ¬? Layer-connection points are then generated to represent a layer in the horizontal direction and to connect different layers in the vertical direction. Finally, by connecting neighboring layer-connection points, building models are constructed with the second level of detailed data. The key contributions of this approach are the use of layer-connection points and the smoothness strategy for building model reconstruction. Experimental results are analyzed from several aspects, namely, the correctness and completeness, deviation analysis of the reconstructed building roofs, and the influence of elevation to 3-D roof reconstruction. In the two experimental regions used in this paper, the completeness and correctness of the reconstructed rooftop patches were about 90\% and 95%, respectively. For the deviation accuracy, the average deviation distance and standard deviation in the best case were 0.05 m and 0.18 m, respectively; and those in the worst case were 0.12 m and 0.25 m. The experimental results demonstrated promising correctness, completeness, and deviation accuracy with satisfactory 3-D building roof models.},
  comment      = {Great example of pipeline approach to roof reconstruction.},
  creationdate = {2016.09.21},
  year         = {2016},
}

@PhdThesis{Werbos1974,
  author       = {Paul Werbos},
  institution  = {Harvard University},
  title        = {Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences},
  comment      = {From Paul John Werbos's 1994 book this thesis is ``The original source for true backpropogation''.},
  creationdate = {2016.05.11},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  year         = {1974},
}

@Article{WeyandKP16,
  author       = {Tobias Weyand and Ilya Kostrikov and James Philbin},
  journaltitle = {CoRR},
  title        = {PlaNet - Photo Geolocation with Convolutional Neural Networks},
  url          = {http://arxiv.org/abs/1602.05314},
  volume       = {abs/1602.05314},
  abstract     = {Is it possible to build a system to determine the location where a photo was taken using just its pixels? In general, the problem seems exceptionally difficult: it is trivial to construct situations where no location can be inferred. Yet images often contain informative cues such as landmarks, weather patterns, vegetation, road markings, and architectural details, which in combination may allow one to determine an approximate location and occasionally an exact location. Websites such as GeoGuessr and View from your Window suggest that humans are relatively good at integrating these cues to geolocate images, especially en-masse. In computer vision, the photo geolocation problem is usually approached using image retrieval methods. In contrast, we pose the problem as one of classification by subdividing the surface of the earth into thousands of multi-scale geographic cells, and train a deep network using millions of geotagged images. While previous approaches only recognize landmarks or perform approximate matching using global image descriptors, our model is able to use and integrate multiple visible cues. We show that the resulting model, called PlaNet, outperforms previous approaches and even attains superhuman levels of accuracy in some cases. Moreover, we extend our model to photo albums by combining it with a long short-term memory (LSTM) architecture. By learning to exploit temporal coherence to geolocate uncertain photos, we demonstrate that this model achieves a 50\% performance improvement over the single-image model.},
  bibsource    = {dblp computer science bibliography, http://dblp.org},
  biburl       = {http://dblp.uni-trier.de/rec/bib/journals/corr/WeyandKP16},
  creationdate = {2016.06.08},
  owner        = {ISargent},
  year         = {2016},
}

@Article{WickhamN94,
  author       = {Wickham, James D. and Norton, Douglas J.},
  journaltitle = {Landscape Ecology},
  title        = {Mapping and analyzing landscape patterns},
  number       = {1},
  pages        = {7-23},
  url          = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.4122&rep=rep1&type=pdf},
  volume       = {9},
  comment      = {''The purpose of this research is to test applications of a landscape mapping and classification system based on Forman and Godron's (1981) concepts of matrix and patch, thereby taking advantage of the tendency for landcover to be spatially clustered into discernable units. ``. Use fractal units and compare to existing classifications systems.},
  creationdate = {2013.12.19},
  keywords     = {ImageLearn, landscape regionalization, ecology},
  owner        = {ISargent},
  year         = {1994},
}

@InProceedings{WiechertGKPS2012,
  Title                    = {The Power of Multi-Ray Photogrammetry - Ultramap 3.0},
  Author                   = {Wiechert, A. and Gruber, M. and Karner, K. and Ponticelli, M. and Schachinger, B.},
  Booktitle                = {ASPRS Annual Conference},
  Year                     = {2012},

  Address                  = {Sacramento, USA},

  Date                     = {3},
  Keywords                 = {3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2015.10.20}
}

@Online{WikipediaCNN,
  author           = {Wikipedia},
  title            = {Convolutional neural network},
  url              = {https://en.wikipedia.org/wiki/Convolutional_neural_network},
  urldate          = {2016.05.11},
  comment          = {'' Stacking many such layers leads to non-linear ``filters'' that become increasingly ``global'' (i.e. responsive to a larger region of pixel space). This allows the network to first create good representations of small parts of the input, then assemble representations of larger areas from them.''},
  creationdate     = {2016.05.11},
  keywords         = {ImageLearn, CNN, Spatial Scale},
  modificationdate = {2023-12-06T08:00:54},
  owner            = {ISargent},
  year             = {2016},
}

@Article{WojnaGLMYLI2017,
  author       = {Zbigniew Wojna and Alex Gorban and Dar-Shyang Lee and Kevin Murphy and Qian Yu and Yeqing Li and Julian Ibarz},
  date         = {2 May 2017},
  title        = {Attention-based Extraction of Structured Information from Street View Imagery},
  url          = {https://arxiv.org/abs/1704.03549},
  urldate      = {2017-05-04},
  comment      = {''We see that the accuracy improves for a while, and then starts to drop as the depth increases. This trend holds for all three models. We believe the reason for this is that character recognition does not benefit from the high-level features that are needed for image classification.''},
  creationdate = {2017.05.04},
  journal      = {arXiv},
  owner        = {ISargent},
  year         = {2017},
}

@InProceedings{WongE2015a,
  Title                    = {Assessing the suitability of using {G}oogle {G}lass in designing 3{D} geographic information for navigation},
  Author                   = {Kelvin Wong and Claire Ellul},
  Booktitle                = {2015 Joint International Geoinformation Conference},
  Year                     = {2015},

  Address                  = {Kuala Lumpur, Malaysia},
  Month                    = {10},
  Volume                   = {II-2/W2},

  Owner                    = {ISargent},
  creationdate                = {2015.11.11}
}

@InProceedings{WongE2013,
  Title                    = {Enhancing Positioning of Photovoltaic Panels Using 3D Geographic Information},
  Author                   = {Kelvin Wong and Claire Ellul},
  Booktitle                = {GISRUK},
  Year                     = {2013},
  Month                    = {4},

  Keywords                 = {3D buildings, 3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2014.11.06},
  Url                      = {http://www.geos.ed.ac.uk/~gisteac/proceedingsonline/GISRUK2013/gisruk2013_submission_17.pdf}
}

@Unpublished{Wong2014,
  author       = {Kelvin Ka Yin Wong},
  title        = {Designing 3D Geographic Information for Navigation using Google Glass},
  note         = {EngD project report},
  comment      = {Kelvin's individual project report. ``3D is a multifaceted and ill-defined problem and it is unclear whether the benefits of the extra dimension outweighs its complexity''.},
  creationdate = {2014.11.04},
  keywords     = {3D buildings, uuser, task analysis},
  month        = {10},
  owner        = {ISargent},
  year         = {2014},
}

@InProceedings{WongE2015,
  Title                    = {Designing 3D Geographic Information for Navigation using Google Glass},
  Author                   = {Wong, K. K. Y. and Ellul, Claire},
  Booktitle                = {23rd GIS Research UK ({GISRUK}) conference},
  Year                     = {2015},

  Address                  = {Leeds, UK},

  Date                     = {4},
  Keywords                 = {3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2015.10.20}
}

@Article{WoodfordPMPS14,
  author       = {Woodford, Oliver J. and Minh-Tri Pham and Atsuto Maki and Frank Perbet and Bjorn Stenger},
  title        = {Demisting the Hough Transform for 3D Shape Recognition and Registration},
  journaltitle = {International Journal of Computer Vision},
  year         = {2014},
  volume       = {106},
  number       = {3},
  month        = {2},
  pages        = {332-341},
  comment      = {Not read yet but could be useful for 3D shape work},
  keywords     = {3D Shape},
  owner        = {ISargent},
  creationdate    = {2014.02.04},
}

@InProceedings{WuKSS09,
  author    = {Zhong Wu and Qifa Ke and Jian Sun and Heung-Yeung Shum},
  title     = {A Multi-sample, Multi-tree Approach to Bag-of-words Image Representation for Image Retrieval},
  booktitle = {The 12th International Conference on Computer Vision (ICCV)},
  year      = {2009},
  url       = {http://research.microsoft.com/pubs/102131/ICCV09.pdf},
  abstract  = {The state-of-the-art content based image retrieval systems has been significantly advanced by the introduction of SIFT features and the bag-of-words image representation. Converting an image into a bag-of-words, however, involves three non-trivial steps: feature detection, feature description, and feature quantization. At each of these steps, there is a significant amount of information lost, and the resulted visual words are often not discriminative enough for large scale image retrieval applications. In this paper, we propose a novel multi-sample multi-tree approach to computing the visual word codebook. By encoding more information of the original image feature, our approach generates a much more discriminative visual word codebook that is also efficient in terms of both computation and space consumption, without losing the original repeatability of the visual features. We evaluate our approach using both a groundtruth data set and a real-world large scale image database. Our results show that a significant improvement in both precision and recall can be achieved by using the codebook derived from our approach.},
  comment   = {Paper about creating codebooks. Uses mutlple image patches to create initial features. I think I need to understand the creation of codebooks better to understand this paper but it looks interesting.},
  keywords  = {Machine Learning, Visual Codebook, Microsoft Research},
  owner     = {ISargent},
  creationdate = {2013.10.16},
}

@Book{XuW2008,
  author    = {Rui Xu and Don Wunsch},
  title     = {Clustering},
  year      = {2008},
  publisher = {John Wiley \& Sons},
  comment   = {Only read a small bit of introduction online but it looks like a very readable guide to clustering/unsupervised learning.},
  month     = {11},
  owner     = {ISargent},
  creationdate = {2016.10.19},
}

@Article{YanJS2012,
  author       = {Yan, J and Jiang, W and Shan, J},
  title        = {Quality Analysis on Ransac-based Roof Facets Extraction from Airborne Lidar Data},
  journaltitle = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  year         = {2012},
  volume       = {XXXIX-B3},
  pages        = {367-372},
  url          = {http://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XXXIX-B3/367/2012/isprsarchives-XXXIX-B3-367-2012.pdf},
  comment      = {Concerned with quality assessing RANSAC for extraction of roof facets. Key issues seem to be over-, under- and non-segmented plans and spurious planes.},
  keywords     = {3D quality, 3D buildings},
  owner        = {ISargent},
  creationdate    = {2014.10.29},
}

@InProceedings{YangN2010,
  author    = {Yi Yang and Shawn Newsam},
  title     = {Bag-Of-Visual-Words and Spatial Extensions for Land-Use Classification},
  booktitle = {ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems (ACM GIS)},
  year      = {2010},
  comment   = {reference for the UC Merced Land Use Dataset},
  owner     = {ISargent},
  creationdate = {2017.07.05},
}

@InProceedings{YaoBF12,
  author    = {Bangpeng Yao and Gary Bradski and Li Fei-Fei},
  title     = {A Codebook-Free and Annotation-Free Approach for Fine-Grained Image Categorization},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2012},
  month     = {6},
  url       = {http://ai.stanford.edu/~bangpeng/document/cvpr12_0654.pdf},
  address   = {Providence, RI, USA},
  comment   = {Uses template matching to classify images. Fine-scales in this case I think is to the level of bird species (rather than coarser-scaled birds). Does not use a codebook and users do not need to label images.},
  keywords  = {Machine Learning, Computer Vision, Representation Learning},
  owner     = {ISargent},
  creationdate = {2013.09.30},
}

@Article{YaoPK2016,
  author       = {W. Yao and P. Poleswki and P. Krzystek},
  date         = {12-19 July},
  title        = {Classification of Urban Aerial Data Based on Pixel Labelling with Deep Convolutional Neural Networks and Logistic Regression},
  doi          = {10.5194/isprs-archives-XLI-B7-405-2016},
  pages        = {405-410},
  url          = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLI-B7/405/2016/},
  volume       = {XLI-B7},
  booktitle    = {ISPRS XXIII CONGRESS},
  comment      = {use imagery including NIR and DSM with CNN. Use Dempster-Shafer theory for sensor fusion: ``(DS) evidence theory is a generalized probabilistic model that has been often used for sensor fusion. DS is defined on degrees of belief level rather than the probability to improve the accuracy and robustness of labeling''. applied to the ISPRS benchmark dataset.},
  creationdate = {2016.07.05},
  journal      = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  keywords     = {deep learning, remote sensing, MLStrat DLRS},
  owner        = {ISargent},
  year         = {2016},
}

@InBook{Yao03,
  author       = {Yao, Y. Y.},
  booktitle    = {Entropy Measures, Maximum Entropy Principle and Emerging Applications},
  title        = {Information-Theoretic Measures for Knowledge Discovery and Data Mining},
  pages        = {115-136},
  series       = {Studies in Fuzziness and Soft Computing},
  url          = {http://www.cse.ohio-state.edu/~weit/Information-Theoretic%20Measures%20for%20Knowledge%20Discovery%20and%20Data%20Mining.pdf},
  volume       = {119},
  comment      = {Excellent paper on using information theory for KDD. Describes a database as an information table with objects as rows and attributes as columns. Well worth re-reading. The section on nformation-theoretic Measures of Attribute Importance divides measures into measures of structuredness (entropy), measures of one-way association (much of machine learning), measures of two-way association (mutual information) and measures of similarity of populations (closely related to two-way association). ``In studying main problem types for KDD, Kl\''{o}sgen [23] discussed the following two types of problems. The classification and predication problem deals with the discovery of a set of rules or similar patterns for predicting the values of a dependent variable. The ID3 algorithm [42] and the mining of associate rules [1] are examples for solving this type of problems. The summary and description problem deals with the discovery of dominant structure that derives a dependence.''},
  creationdate = {2014.02.06},
  keywords     = {information theory, feature selection},
  owner        = {ISargent},
  year         = {2003},
}

@InProceedings{YosinskiCNFL2015,
  author           = {Jason Yosinski and Jeff Clune and Anh Nguyen and Thomas Fuchs and Hod Lipson},
  booktitle        = {Deep Learning Workshop, 31st International Conference on Machine Learning},
  title            = {Understanding Neural Networks Through Deep Visualization},
  address          = {Lille, France},
  comment          = {Looks into visualisation through optimising the activation at a chosen neuron. Created a tool for visualising representations in network trained with ImageNet in Caffe - useful for running with video camera to see what classifications are found. More interesting are the insights into what does and doesn't work and why this may be. Optimisation is initiated with random input and cost is difference between output at chosen neuron and maximum activation (I think). This is then back-propogated to find the gradient at the input and use this to alter the input. Idea is to converge on an input that maximally activates the chosen neuron. Find that regularisation, in terms of constraining the update of the input makes more meaningful images - without regularisation images tend to contain a lot of high-frequency patterns, extreme pixel values and don't resemble natural images. Use four different types of regularisation: L2 decay to penalise extreme values, Gaussian blur to reduce high frequency information, Clipping pixels with small norm and Clipping pixels with small contribution, both to bring more pixels to zero. More information at \url{http://yosinski.com/deepvis}.},
  creationdate     = {2016.01.29},
  keywords         = {ImageLearn, Visualisation, TopoNet Metrics, MLStrat Discovery, explainability},
  modificationdate = {2023-01-25T11:08:48},
  owner            = {ISargent},
  year             = {2015},
}

@Report{YoungSHA2015,
  author           = {David Young and Isabel Sargent and Jonathon Hare and Peter Atkinson},
  date             = {2015},
  institution      = {Ordnance Survey \& University of Southampton},
  title            = {Machine Learning for Aerial Image Analysis: Mid-project Report},
  note             = {Internal Ordnance Survey Report},
  creationdate     = {2016.06.20},
  keywords         = {ImageLearn},
  modificationdate = {2023-12-06T08:01:44},
  mon              = {11},
  owner            = {ISargent},
}

@Book{YuD2015,
  Title                    = {Automatic Speech Recognition - A Deep Learning Approach},
  Author                   = {Dong Yu and Li Deng},
  Year                     = {2015},
  Series                   = {Signals and Communication Technology},

  Keywords                 = {ImageLearn},
  Owner                    = {ISargent},
  creationdate                = {2016.03.03},
  Url                      = {http://link.springer.com/book/10.1007/978-1-4471-5779-3}
}

@InProceedings{ZeilerF2014,
  author           = {Matthew D. Zeiler and Rob Fergus},
  booktitle        = {Computer Vision - ECCV 2014},
  title            = {Visualizing and Understanding Convolutional Networks},
  doi              = {https://doi.org/10.1007/978-3-319-10590-1_53},
  editor           = {Fleet D. and Pajdla T. and Schiele B. and Tuytelaars T.},
  series           = {Lecture Notes in Computer Science},
  url              = {https://arxiv.org/abs/1311.2901},
  volume           = {8689},
  comment          = {Visualise the activations of units at all layers of convnet by adding a deconvolutional neural network to each. ``A deconvnet can be thought of as a convnet model that uses the same components (filtering, pooling) but in reverse, so instead of mapping pixels to features does the opposite.'' Also test the sensitivity of the network to occlusion and show that the model is localising the objects in the scene and what aspects of the image are important for their classification (e.g. dog's face). Also use masking to show that some degree of correspondence between specific object parts is being established in the network.

From BachBMKMS2020: ``solves optimization problems in order to reconstruct the image input, while our approach attempts to reconstruct the classifier decision''

Also explore the ability of the learned features to generalised to classify other datasets: Caltech-101, Caltech-256 and Pascal VOC 2012.},
  creationdate     = {2015.07.08},
  keywords         = {ImageLearn, Visualisation, TopoNet Metrics, explaining ML, MLStrat Discovery, explainability},
  modificationdate = {2022-04-05T09:43:01},
  owner            = {ISargent},
  year             = {2014},
}

@Article{ZengZW2014,
  author       = {Chuiqing Zeng and Ting Zhao and Jinfei Wang},
  journaltitle = {IEEE Geoscience and Remote Sensing Letters},
  title        = {A Multicriteria Evaluation Method for 3-D Building Reconstruction},
  number       = {9},
  pages        = {1619-1623},
  volume       = {11},
  comment      = {''This letter has proposed a multicriteria 3-D building evaluation system based on three components:volume, surface, and point.The volume accuracy reports the percent of correctly reconstructed building volume, which expresses the chance of occurrence of either missing or excess parts in the final reconstruction. The surface accuracy matches 3-D surface directly between the reference and reconstructed buildings. Surface comparison including rooftops and walls is implemented by the sphere parameterization and then the SPHARM expansion. This surface comparison method is essentially different from existing 3-D evaluation that only takes rooftops or matching building surfaces facet-by-facet into account during evaluation. The point accuracy provides the positional accuracy for reconstructed building at feature points, such as corners and centroids''},
  creationdate = {2015.03.17},
  keywords     = {3D quality, 3DCharsPaper},
  owner        = {ISargent},
  year         = {2014},
}

@InProceedings{ZhangYJS2015,
  author       = {Jing Zhang and Huang Yao and Wanshou Jiang and Xin Shen},
  booktitle    = {Indoor-Outdoor Seamless Modelling, Mapping and Navigation},
  title        = {Hierarchical Repetition Extraction for Building Fa\c{c}ade Reconstruction from Oblique Aerial Images},
  url          = {http://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XL-4-W5/183/2015/isprsarchives-XL-4-W5-183-2015.pdf},
  address      = {Tokyo, Japan},
  comment      = {Find repetitive structure to determine facade features using inverse procedural modelling and reconstruct facede from aerial oblique imagery. Procedural modelling `` needs some man-made grammar schemes to represent specific fa\c{c}ade models and mainly used to synthesize realistic looking urban model''. Inverse procedural modelling ``discover structural rules from input data, then use these rules to reconstruct building models in the PM manner''. From Jon Slade: ``They used oblique imagery to construct solid building geometry and then segment facades and features within facades e.g. windows. To do this they used 'pixel-wise stereo matching to generate 3D point cloud from oblique images'. They then identified dominant planes, segmenting based on a repetition principle and a line fitting approach. Next they used gradient of mutual information for similarity (Dalal and Trigg, 2005) and the Line Segment Detector (LSD) to partition. They state that the approach is robust to low resolution images and weak texture due to the redundancy provided by oblique images from many angles. But, their method needs repetitive structures to work.''},
  creationdate = {2015.06.12},
  editors      = {T. Fuse and M. Nakagawa},
  keywords     = {building facade},
  month        = {5},
  owner        = {ISargent},
  year         = {2015},
}

@Article{ZhengZYIZ2015,
  author       = {Bo Zheng and Yibiao Zhao and Joey Yu and Katsushi Ikeuchi and Song-Chun Zhu},
  title        = {Scene Understanding by Reasoning Stability and Safety},
  journaltitle = {International Journal of Computer Vision},
  year         = {2015},
  comment      = {This is so obvious, but its the first time I've encountered it: this research automatically models objects from pointclouds by incorporating our knowledge about how gravity affects the physical world: http://www.stat.ucla.edu/~ybzhao/research/physics/. In summary, segmentation is applied to the point cloud, the segments are turned into volume objects and these are then combined using the assumption 'if it should fall, but doesn't, its joined on'. The video isn't great but it sort of works.},
  keywords     = {3D modelling},
  owner        = {ISargent},
  creationdate    = {2015.01.28},
}

@Article{ZhouX10,
  author       = {HuaRong Zhou and DuNing Xiao},
  journaltitle = {Journal of Arid Land},
  title        = {Ecological function Regionalization of fluvial corridor landscapes and measures for ecological regeneration in the middle and lower reaches of the Tarim River, Xinjiang of China},
  pages        = {123-132},
  volume       = {2},
  comment      = {Paper about landscape ecological regionlization aka ecological function regionalization. Seems a bit repetitive but I may not be understanding the subtleties of the topic. References Bailey76 and Bailey83 who ``first put forward the concept of ecological regionlization''. Teasingly, the in introduction, ``other regionalization'' which differs from landscape ecological regionalization is mentioned. I would like to know more about these others.States that a landscape ecological function area is a hierarchical system of regionlization, dividing the study area in landscape type area, landscape sub-area, landscape plot and landscape factor plot. References a paper by Zhou, turson and Tang, 2001 about 'division of urban landscape functonal districts...' but doesn't include it in text - the paper ZhouTT01 appears to be in chinese.},
  creationdate = {2014.03.19},
  keywords     = {ImageLearn, landscape, regionalization},
  owner        = {ISargent},
  year         = {2010},
}

@Article{ZhouTT01,
  author       = {Zhou, H R and Turson, H and Tang, P},
  title        = {Division of urban landscape functional districts and its ecological control in Urumqi, Xinjiang, China.},
  journaltitle = {Arid Land Geography},
  year         = {2001},
  volume       = {24},
  number       = {4},
  pages        = {314-320},
  url          = {http://www.alljournals.cn/view_abstract.aspx?pcid=E62459D214FD64A3C8082E4ED1ABABED5711027BBBDDD35B&cid=869B153A4C6B5B85&jid=8D670C0CA915F2CDD665049400ADA844&aid=571408EE806FE7AA&yid=14E7EF987E4155E6&from_type=1},
  comment      = {Paper appears to be Chinese. referenced (but not commented on) in ZhouX10.},
  keywords     = {ImageLearn, landscape, regionalization},
  owner        = {ISargent},
  creationdate    = {2014.03.19},
}

@Article{ZhouZHL12,
  Title                    = {Multi-instance multi-label learning},
  Author                   = {Zhi-Hua Zhou and Min-Ling Zhang and Sheng-Jun Huang and Yu-Feng Li},
  Year                     = {2012},
  Number                   = {1},
  Pages                    = {2291--2320},
  Volume                   = {176},

  Abstract                 = {In this paper, we propose the MIML (Multi-Instance Multi-Label learning) framework where an example is described by multiple instances and associated with multiple class labels. Compared to traditional learning frameworks, the MIML framework is more convenient and natural for representing complicated objects which have multiple semantic meanings. To learn from MIML examples, we propose the MimlBoost and MimlSvm algorithms based on a simple degeneration strategy, and experiments show that solving problems involving complicated objects with multiple semantic meanings in the MIML framework can lead to good performance. Considering that the degeneration process may lose information, we propose the D-MimlSvm algorithm which tackles MIML problems directly in a regularization framework. Moreover, we show that even when we do not have access to the real objects and thus cannot capture more information from real objects by using the MIML representation, MIML is still useful. We propose the InsDif and SubCod algorithms. InsDif works by transforming single-instances into the MIML representation for learning, while SubCod works by transforming single-label examples into the MIML representation for learning. Experiments show that in some tasks they are able to achieve better performance than learning the single-instances or single-label examples directly.},
  Journaltitle             = {Artificial Intelligence},
  Keywords                 = {Machine Learning, Bag of Instances, Classification},
  Owner                    = {ISargent},
  creationdate                = {2013.09.27}
}

@Article{ZhuY1998,
  author    = {Changqing Zhu and Xiaomei Yang},
  title     = {Study of remote sensing image texture analysis and classification using wavelet},
  doi       = {10.1080/014311698214262},
  eprint    = {http://dx.doi.org/10.1080/014311698214262},
  number    = {16},
  pages     = {3197-3203},
  url       = {http://dx.doi.org/10.1080/014311698214262},
  volume    = {19},
  comment   = {Reference for texture operators in remote sensing},
  journal   = {International Journal of Remote Sensing},
  keywords  = {ImageLearn, Feature Extraction},
  owner     = {ISargent},
  creationdate = {2017.05.29},
  year      = {1998},
}

@Article{ZiouT1998,
  author    = {Djemel Ziou and Salvatore Tabbone},
  title     = {Edge detection techniques: An overview},
  journal   = {International Journal of Pattern Recognition and Image Analysis},
  year      = {1998},
  number    = {4},
  pages     = {537--559},
  comment   = {A survey of a number of different edge detection methods.},
  keywords  = {ImageLearn, Feature Extraction},
  owner     = {ISargent},
  creationdate = {2017.05.29},
  volum     = {8},
}

@Misc{CooteFM2010,
  editor           = {Andrew Coote and Steven Feldman and Robin McLaren},
  title            = {{AGI} Foresight Study: The {UK} Geospatial Industry in 2015},
  creationdate     = {2016.09.20},
  modificationdate = {2023-12-06T08:02:40},
  month            = {5},
  owner            = {ISargent},
  year             = {2010},
}

@Book{OGCCityGML12,
  title        = {OGC City Geography Markup Language (CityGML) En-coding Standard},
  editor       = {Gerhard Gr\''oger and Kolbe, Thomas H. and Claus Nagel and Karl-Heinz H\''afele},
  publisher    = {Open Geospatial Consortium},
  url          = {http://www.opengis.net/spec/citygml/2.0},
  volume       = {OGC 12-019},
  comment      = {''7.5 Dictionaries and code lists for enumerative attributes Attributes, which are used to classify objects, often have values that are restricted to a number of discrete values. An example is the attribute roof type, whose attribute values typically are saddle back roof, hip roof, semi-hip roof, flat roof, pent roof, or tent roof. If such an attribute is typed as string, misspellings or different names for the same notion obstruct interoperability. In CityGML such classifying of attributes are specified as CodeLists and implemented by GML3 Dictionaries (c.f. Cox et al. 2004). Such a structure enumerates all possible values of the attribute in an external file, assuring that the same name is used for the same notion. In addition, the transla-tion of attribute values into other languages is facilitated. Dictionaries and code lists may be extended or rede-fined by users. They can have references to existing models.'' Also gives roof types in XML from SIG3D roof types schema http://www.sig3d.org/codelists/standard/building/2.0/_AbstractBuilding_roofType.xml: flat roof, monopitch roof, skip pent roof, gabled roof, hipped roof, half-hipped roof, mansard roof, pavilion roof, cone roof, copula roof, shed roof, arch roof, pyramidal broach roof, combination of roof forms. No description of diagrams for any of these.},
  creationdate = {2013.11.07},
  institution  = {Open Geospatial Consortium},
  keywords     = {roof type, 3DCharsPaper},
  month        = {4},
  owner        = {isargent},
  type         = {OpenGIS Encoding Standard},
  year         = {2012},
}

@Book{Hoffman2014,
  title     = {The Psychology of Expertise: Cognitive Research and Empirical AI},
  year      = {2014},
  editor    = {Robert R. Hoffman},
  publisher = {Psychology Press},
  comment   = {Could be an interesting read. Hoffman seems to have got into this field of through remote sensing - specifically when applying to the US Army Engineers Topographic Laboratory to train in aerial interpretation they asked him to help them build an expert system (he didn't know about expert systems either).},
  keywords  = {ImageLearn, psychology},
  owner     = {ISargent},
  creationdate = {2015.06.30},
}

@Online{CarnegieMellonStatistics,
  title            = {Carnegie Mellon University Probability and Statistics},
  url              = {https://oli.cmu.edu/jcourse/workbook/activity/page?context=c09cf67880020ca601e42944841e08bb},
  comment          = {Contains excellent diagrams showing the 4 stages of statistics: converting data into useful information.

May have been \url{http://wiki.penson.io/images/big-picture.png} from \url{http://wiki.penson.io/courses/cmu-stats.html} which shows the four phases of understanding a population from which a data sample is taken (phase 1):
1. producing data
2. exploratory data analysis
3. probability
4. inference},
  creationdate     = {2013.10.16},
  modificationdate = {2023-12-24T13:05:22},
  owner            = {ISargent},
}

@Misc{RedditDeepDream2015,
  title        = {DeepDream},
  year         = {2015},
  howpublished = {Web Page},
  url          = {https://www.reddit.com/r/deepdream/comments/3cawxb/what_are_deepdream_images_how_do_i_make_my_own/},
  comment      = {Why are there dogs and eyes in machine dreams? Because there is a bias towards these in the training data.},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  creationdate    = {2016.08.31},
}

@Misc{Colah2014,
  year      = {2014},
  url       = {http://colah.github.io/posts/2014-07-Conv-Nets-Modular/},
  comment   = {Excellent support to KrizhevskySH12 explaining convolution nets},
  keywords  = {convolution networks, ImageLearn, machine learning},
  owner     = {ISargent},
  creationdate = {2014.07.15},
}

@Manual{OSMMOPO,
  Title                    = {OS MasterMap Topography Layer User Guide},
  Note                     = {Last accessed 11 November 2015},
  Organization             = {Ordnance Survey},
  Year                     = {2014},

  Institution              = {Ordnance Survey},
  Keywords                 = {3DCharsPaper},
  Owner                    = {ISargent},
  Publisher                = {Ordnance Survey},
  creationdate                = {2014.11.14},
  Url                      = {http://www.ordnancesurvey.co.uk/docs/user-guides/os-mastermap-topography-layer-user-guide.pdf}
}

@Periodical{Broadleaf13a,
  Title                    = {Broadleaf},
  Year                     = {2013},
  Number                   = {81},
  Organization             = {Woodland Trust},

  Owner                    = {ISargent},
  creationdate                = {2013.12.03}
}

@Misc{CMHLC08,
  title     = {Charlotte-Mecklenburg Historic Landmarks Commision - Roof Types},
  year      = {2008},
  url       = {http://www.cmhpf.org/kids/Guideboox/RoofTypes.html},
  comment   = {Gives roof types as sketch diagrams: gable, cross-gable, flat, mansard, hipped, cross-hipped, pyramidal, shed, saltbox, gambrel.},
  owner     = {isargent},
  creationdate = {2013.11.07},
}

@Article{MassaroARSSLRH2017,
  author    = {Emanuele Massaro and Chaewon Ahn and Carlo Ratti and Paolo Santi and Rainer Stahlmann and Andreas Lamprecht and Martin Roehder and Markus Huber},
  title     = {The Car as an Ambient Sensing Platform [Point of View]},
  journal   = {Proceedings of the {IEEE}},
  year      = {2017},
  volume    = {105},
  issue     = {1},
  owner     = {ISargent},
  creationdate = {2017.09.29},
}

@Article{MishkinM2015,
  author    = {Dmytro Mishkin and Jiri Matas},
  title     = {All you need is a good init},
  url       = {http://arxiv.org/abs/1511.06422},
  volume    = {abs/1511.06422},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/MishkinM15},
  comment   = {Proposes Layer-sequential unit-variance (LSUV) initialization for deep network weights which shows promising results in experiments using different activation functions and network architectures.},
  journal   = {CoRR},
  keywords  = {deep learning, initialisation, MLStrat Training},
  owner     = {ISargent},
  creationdate = {2017.06.07},
  year      = {2015},
}

@InProceedings{LiCH2009,
  author    = {Yunpeng Li and D. J. Crandall and D. P. Huttenlocher},
  title     = {Landmark classification in large-scale image collections},
  booktitle = {2009 IEEE 12th International Conference on Computer Vision},
  year      = {2009},
  month     = {9},
  pages     = {1957-1964},
  doi       = {10.1109/ICCV.2009.5459432},
  comment   = {Labelling images based on geotag - finding peaks that correspond to landmarks - and learn models of the landmarks using interest point descriptors. Also use textual tags and dates givenon photo sharing sites.},
  issn      = {1550-5499},
  keywords  = {Web sites;image classification;object recognition;support vector machines;image classification;landmark classification;large-scale image collections;multiclass support vector machine;object recognition;photo-sharing Web sites;vector-quantized interest point descriptors;Computer science;Computer vision;Facebook;Image classification;Internet;Large-scale systems;Object recognition;Support vector machine classification;Support vector machines;Testing},
  owner     = {ISargent},
  creationdate = {2017.10.11},
}

@TechReport{BrightComputing2017,
  title       = {Building a Deep Learning Environment for your Organization: CONSIDERATIONS FOR A ROBUST SOLUTION},
  institution = {Bright Computing},
  year        = {2017},
  comment     = {Really just a way of getting Bright's name out there - although it doesn't actually pitch Bright Computing in the body of the document. Very basic document, explains what deep learning is, what its used for, that you need libraries/frameworks, softwware, hardware and  architecture, and you'll need to think about how all this is managed. Doesn't provide any major insights but not an unpleasant read. Has some use cases.},
  owner       = {ISargent},
  creationdate   = {2017.10.18},
}

@TechReport{Gutierrez2017,
  author       = {Daniel D Gutierrez},
  institution  = {insideBIGDATA},
  title        = {InsideBIGDATA Guide to Deep Learning \& Artificial Intelligence},
  comment      = {About the intersection and coupling of AI and HPC. ``In the final analysis, the success of organizations to fully capitalize on these technologies may boil down to finding enough skilled workers. Many companies are working to determine how to recruit the right talent and build the right organization to succeed in an AI-driven economy. Maybe some practitioners who grew up doing machine learning didn't grow up with an HPC background. The solution may be to sit down and learn it yourself, or maybe partner with people that have this skillset.''. ``An interesting statistic involving the use of AI for speech recognition shows that using 10x more data can lower relative error rates by 40%. HPC''. Contains results of a survey of an 'audience' about opinions on artificial intelligence/machine learning/deep learning. Reponses indicated a very high expectation of impact of AI/ML/Dl on their work, that very many were already evaluating the tech, Some of the other statistics, such as those on the intersection of AI/ML/Dl and HPC are less impressive. Talks about how GPU is essential - no mention of alternatives (this is sponsored by nVidia). Has some use cases.},
  creationdate = {2017.10.18},
  keywords     = {AI, transformation, MLStrat Programme},
  owner        = {ISargent},
  year         = {2017},
}

@Article{GonzalezGarciaMF2018,
  author           = {Abel Gonzalez-Garcia and Davide Modolo and Vittorio Ferrari},
  date             = {2018},
  journaltitle     = {International Journal of Computer Vision},
  title            = {Do Semantic Parts Emerge in Convolutional Neural Networks?},
  doi              = {10.1007/s11263-017-1048-0},
  url              = {https://link.springer.com/article/10.1007/s11263-017-1048-0},
  comment          = {''Some previous works (Zeiler and Fergus 2014; Simonyan et al. 2014) have suggested that semantic parts do emerge in CNNs, but only based on looking at some filter responses on a few images. Here we go a step further and perform two quantitative evaluations that examine the different stimuli of the CNN filters and try to associate them with semantic parts. First, we take advantage of the available  ground-truth part location annotations in the PASCAL-Part dataset (Chen et al. 2014) to count how many of the annotated semantic parts emerge in a CNN. Second, we use human judgements to determine what fraction of all filters systematically fire on any semantic part (including parts that might not be annotated in PASCAL-Part).''

''as suggested by (Simon et al. 2014; Simon and Rodner 2015; Xiao et al. 2015), a single semantic part might emerge as distributed across several filters.''

''how many semantic parts emerge in CNNs?'' ``We present an extensive analysis on AlexNet (Krizhevsky et al. 2012) finetuned for object detection (Girshick et al. 2014). Results show that 34 out of 105 semantic parts emerge. This is a modest number, despite all favorable conditions we have engineered into the evaluation and all assists we have given to the network. This result demystifies the impressions conveyed by (Zeiler and Fergus 2014; Simonyan et al. 2014) and shows that the network learns to associate filters to part classes, but only for some of them and often to a weak degree. In general, these semantic parts are those that are large or very discriminative for the object class (e.g., torso, head, wheel). Finally, we analyze different network layers, architectures, and supervision levels.We observe that part emergence increases with the depth of the layer, especially when using deeper architectures such as VGG16 (Simonyan and Zisserman 2015). Moreover, emergence decreases when the network is trained for tasks less related to object parts, e.g. scene classification (Zhou et al. 2014).''

''what fraction of all filters respond to any semantic part?'' ``On average per object class, 7\% of the filters correspond to semantic parts (including several filters responding to the same semantic part). About 10\% of the filters systematically respond to other stimuli such as colors, subregions of parts or even assemblies of multiple parts.''

''Finally, we also investigate how discriminative network filters and semantic parts are for recognizing objects. We explore the possibility that some filters respond to 'parts' as recurrent discriminative patches, rather than truly semantic parts. We find that, for each object class in PASCAL-Part, there are on average 9 discriminative filters that are largely responsible for recognizing it. Interestingly, 40\% of these are also semantic according to human judgements, which is a much greater proportion than the 7\% found when considering all filters. The overlap between which filters are discriminative and which ones are semantic might be the reason why previous works (Zeiler and Fergus 2014; Simonyan et al. 2014) have suggested a stronger emergence of semantic parts, based on qualitative visual inspection.''

Some useful summaries of other papers.  ``Zhou et al. (2015) show that the layers of a network learn to recognize visual elements at different levels of abstraction (e.g. edges, textures, objects and scenes). Most of these works make an interesting observation: filter responses can often be linked to semantic parts (Zeiler and Fergus 2014; Simonyan et al. 2014; Zhou et al. 2015). These observations are however mostly based on casual visual inspection of few images (Zeiler and Fergus 2014; Simonyan et al. 2014). Zhou et al. (2015) is the only work presenting some quantitative results based on human judgements''

Explain feature maps and receptive fields for each filter. Use regression (regressing ground-truth part bounding-boxes on activations, I think) to determine the actual bounding box of the part resulting in the local maximum peak in the feature map.  ``In general, the receptive field of high layers is significantly larger than the part ground-truth bounding-box...Moreover, while the receptive field is always square,
some classes have other aspect ratios (e.g. legs). Finally, the response of a filter to a part might not occur in its center, but at an offset instead'' - this final point only makes sense if its the 'peak' response of a filter not occuring in the centre - ?

Evaulate the filters as detectors of object parts by comparing the ``stimulus detections'' (bounding box computed using the regression approach within the receptive field region (use non-maxima suppression to remove duplicates). A correct detection has an intersection-over-union >-0.4. ``use Average Precision (AP) to
evaluate the filters as part detectors, following the PASCAL VOC (Everingham et al. 2010) protocol?},
  creationdate     = {2017.10.18},
  journal          = {International Journal of Computer Vision},
  keywords         = {TopoNet Hack, visualisation, deep learning, hierarchical, toponet metrics, MLStrat Discovery, explainability},
  modificationdate = {2023-01-26T10:08:50},
  owner            = {ISargent},
  year             = {2017},
}

@Article{RouetLeducHLBHJ2017,
  author       = {Rouet-Leduc, Bertrand and Hulbert, Claudia and Lubbers, Nicholas and Barros, Kipton and Humphreys, Colin J. and Johnson, Paul A.},
  title        = {Machine Learning Predicts Laboratory Earthquakes},
  doi          = {10.1002/2017GL074677},
  issn         = {1944-8007},
  note         = {2017GL074677},
  number       = {18},
  pages        = {9276--9282},
  url          = {http://dx.doi.org/10.1002/2017GL074677},
  volume       = {44},
  comment      = {Set up laboratory earthquakes and develop ML tools (decision trees) to predict time to failure of the fault. Based on instantaneous analysis of signal rather than analysing time series. Discover a predictive signal that is not recorded in live earthquakes (I think that's what they're saying) so intend to ``progressively scale from the laboratory to the Earth by applying this approach to Earth problems that most resemble the laboratory system''.},
  creationdate = {2017.10.18},
  journal      = {Geophysical Research Letters},
  keywords     = {Forecasting, Machine learning, Earthquake interaction, forecasting, and prediction, Dynamics and mechanics of faulting, machine learning, earthquake prediction, laboratory earthquakes, acoustic signal identification, earthquake precursors},
  year         = {2017},
}

@Article{ZhouT2017,
  author        = {Yin Zhou and Oncel Tuzel},
  title         = {{VoxelNet}: End-to-End Learning for Point Cloud Based 3D Object Detection},
  eprint        = {1711.06396},
  adsurl        = {https://arxiv.org/abs/1711.06396},
  archiveprefix = {arXiv},
  comment       = {The question of processing point cloud data such that it can be learned from remains open, and one of the more difficult topics of future ImageLearn research. This paper from Apple suggests a way that irregular point clouds can be regularised into voxels, each attributed with local shape information. Neat.},
  journal       = {ArXiv e-prints},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition, point cloud, 3D, machine learning},
  month         = {11},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2017.11.23},
  year          = {2017},
}

@InProceedings{YanM2017,
  author       = {Lisa Yan and Nick McKeown},
  booktitle    = {SIGCOMM'17},
  title        = {Learning Networking by Reproducing Research Results},
  comment      = {Paper looking at what happens when students on a particular course are tasked with taking a relevant paper and reproducing the results. TMP write up here: https://blog.acolyer.org/2017/10/27/learning-networking-by-reproducing-research-results/},
  creationdate = {2017.12.21},
  owner        = {ISargent},
  year         = {2017},
}

@InProceedings{DongMSW2017,
  author    = {Yuxiao Dong and Hao Ma and Zhihong Shen and Kuansan Wang},
  title     = {A Century of Science: Globalization of Scientific Collaborations, Citations, and Innovations},
  booktitle = {KDD2017},
  year      = {2017},
  comment   = {Paper looking at how scientific publishing has changed in a century. Exponential growth in number of papers, number of authors. Also international collaborations increasingly common. TMP review is here: https://blog.acolyer.org/2017/10/06/a-century-of-science-globalization-of-scientific-collaborations-citations-and-innovations/},
  owner     = {ISargent},
  creationdate = {2017.12.21},
}

@InProceedings{RedmonDGF2016,
  author    = {Joseph Redmon and Santosh Divvala and Ross Girshick and Ali Farhadi},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {You Only Look Once: Unified, Real-Time Object Detection},
  pages     = {779--788},
  comment   = {Many object detection approaches (e.g. the R-CNN series) have two stages - image CNN and region proposal. YOLO is a one-stage detection. Using GoogLeNet-inspired architecture with 24 convolutional layers followed by 2 fully connected layers. Instead of the inception modules used by GoogLeNet, we simply use 1x1 reduction layers followed by 3x3 convolutional layers, similar to Lin et al. (2013). Input is 448x448 (instead of 224 x 224), The final output is the 7 x 7 x 30 tensor of class probabilities and bounding box coordinates. Fast YOLO uses a neural network with fewer convolutional layers (9 instead of 24) and fewer filters in those layers. Other than the size of the network, all training and testing parameters are the same between YOLO and Fast YOLO. On PASCAL VOC 2007: comparing the performance and speed of fast detectors. Fast YOLO is the fastest detector on record for PASCAL VOC detection and is still twice as accurate as any other real-time detector. YOLO is 10 mAP more accurate than the fast version while still well above real-time in speed.},
  keywords  = {Deep Learning, object detection, localisation, MLStrat Object},
  owner     = {ISargent},
  creationdate = {2018.05.16},
  year      = {2016},
}

@Article{LandrieuS2018,
  author       = {Loic Landrieu and Martin Simonovsky},
  title        = {Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs},
  url          = {https://arxiv.org/abs/1711.09869},
  comment      = {Alternative method to dealing with point clouds that doesn't use voxels. Geometrically homogeneous partition: creates 'superpoints' using an unsupervised approach to finding geometrically simple yet meaningful shapes. These superpoints then simplify the pointcloud resulting in a graph of superpoints. This graph is then convolved: ``Deep learning algorithms based on graph convolutions can then be used to classify its nodes using rich edge features facilitating long range interactions.''},
  creationdate = {2018.05.16},
  journal      = {arXiv:1711.09869v2 [cs.CV]},
  keywords     = {3D, remote sensing, MLStrat},
  owner        = {ISargent},
  year         = {2018},
}

@Article{GuDG2017,
  author           = {Tianyu Gu and Brendan Dolan-Gavitt and Siddharth Garg},
  title            = {{BadNets}: Identifying Vulnerabilities in the Machine Learning Model Supply Chain},
  url              = {https://arxiv.org/abs/1708.06733},
  comment          = {The Morning Paper considered this here: https://blog.acolyer.org/2017/10/13/badnets-identifying-vulnerabilities-in-the-machine-learning-model-supply-chain/: ``demonstrate two attack vectors: (i) if model training is outsourced, then it's possible for a hard to detect and very effective backdoor to be `trained into' the resulting network, and (ii) if you are using transfer learning to fine tune an existing pre-trained model, then a (slightly less effective) backdoor can be embedded in the pre-trained mode''},
  creationdate     = {2018.06.08},
  journal          = {ArXiv},
  keywords         = {Deep Learning, TopoNet, MLStrat Training, attacks},
  modificationdate = {2022-05-04T21:08:12},
  owner            = {ISargent},
  year             = {2017},
}

@InProceedings{BaylorGoogleEtAl2017,
  author       = {Denis Baylor and Eric Breck and Heng-Tze Cheng and Noah Fiedel and Yu Foo, Chuan and Zakaria Haque and Salem Haykal and Mustafa Ispir and Vihan Jain and Levent Koc and Chiu Yuen Koo and Lukasz Lew and Clemens Mewald and Akshay Naresh Modi and Neoklis Polyzotis and Sukriti Ramesh and Sudip Roy and Steven Euijong Whang and Martin Wicke and Jarek Wilkiewicz and Xin Zhang and Martin Zinkevich},
  booktitle    = {KDD 2017},
  date         = {August 13-17},
  title        = {TFX: A TensorFlow-Based Production-Scale Machine Learning Platform},
  location     = {Halifax, NS, Canada},
  url          = {http://www.kdd.org/kdd2017/papers/view/tfx-a-tensorflow-based-production-scale-machine-learning-platform},
  abstract     = {Creating and maintaining a platform for reliably producing and deploying machine learning models requires careful orchestration of many components|a learner for generating models based on training data, modules for analyzing and validating both data as well as models, and nally infrastructure for serving models in production. This becomes particularly challenging when data changes over time and fresh models need to be produced continuously. Unfortunately, such orchestration is often done ad hoc using glue code and custom scripts developed by individual teams for specic use cases, leading to duplicated eort and fragile systems with high technical debt.

We present TensorFlow Extended (TFX), a TensorFlow-based general-purpose machine learning platform implemented at Google. By integrating the aforementioned components into one platform, we were able to standardize the components, simplify the platform conguration, and reduce the time to production from the order of months to weeks, while providing platform stability that minimizes disruptions. We present the case study of one deployment of TFX in the Google Play app store, where the machine learning models are refreshed continuously as new data arrive. Deploying TFX led to reduced custom code, faster experiment cycles, and a 2\% increase in app installs resulting from improved data and model analysis.},
  comment      = {The Morning Paper summary can be found here: https://blog.acolyer.org/2017/10/03/tfx-a-tensorflow-based-production-scale-machine-learning-platform/

Google's hosting of a production-scale machine learning platform.},
  creationdate = {2018.06.08},
  owner        = {ISargent},
  yera         = {2017},
}

@Article{ZhouKLOT2015,
  author       = {Bolei Zhou and Aditya Khosla and Agata Lapedriza and Aude Oliva and Antonio Torralba},
  title        = {Object Detectors Emerge in Deep Scene {CNNs}},
  url          = {https://arxiv.org/abs/1412.6856},
  comment      = {''ImageNet, an object-centric dataset'' - useful to emphasise that ImageNet is centred on objects. Experiments with the Places dataset. Points out that the functional parts of an object (e.g. eyes, nose in face) are not necessarily important for visual recognition. Discusses the looser relationship between parts in a scene representation, as compared to an object representation and that other representations can emerge in scenes such as textures, bag-of-words and parts based models and ObjectBank (refs given for all). ``The main contribution of this paper is to show that object detection emerges inside a CNN trained to recognize scenes, even more than when trained with ImageNet....we show that the same network can do both object localization and scene recognition in a single forward pass''. Train AlexNet on Places dataset. To identify the diferences in the type of images preferred at the diferent layers of each network they look at images with the highest activations for each layer.  To better understand the nature of the learned representations they simplify images by first segmenting them and then removing image segments until the classification of the image fails, thus identifying which segments are vital for recognition. Also consider the 'empirical' size of the receptive field by removing small parts of the input (11 by 11 pixels) randomly and recording the impact that these changes have the activations at the units - i.e. those removals that have no impact are outside of the empirical receptive field. To understand the 'meaning' of units the group used Amazon Mechanical Turk to crowdsource labels by supplying groups of images to the worker to identify the images that don't fit in the group (included 3 results that shouldn't fit in the group to ensure the quality of the output) and, where possible, supply a label for the group. Demonstrate that with depth, the labels grouped as 'simple elements and colours' decrease, as do 'texture materials' but 'region or surface', 'object part', 'object' and 'scene' all increase. Also show that object localization is possible using internal units of the trained network (and thus a separate network/training is not required).},
  creationdate = {2018.06.08},
  journal      = {arXiv:1412.6856},
  keywords     = {Deep Learning, TopoNet, TopoNet Metrics, MLStrat Discovery},
  owner        = {ISargent},
  year         = {2015},
}

@TechReport{GeoBuiz2018,
  institution  = {Geospatial Media and Communications},
  title        = {GeoBuiz 2018 Report Geospatial Industry Outlook and Readiness Index},
  url          = {https://geobuiz.com/geobuiz-2018-report.html},
  abstract     = {Key Findings

    During 2013 to 2017, the market grew at an estimated 11.5%, and is forecast to grow at CAGR of 13.6\% between 2017-2020
    Impact of geospatial industry has grown at an even higher rate (21%) indicating the multiplier effect of the geospatial technologies on the economy and the society
    Big data, cloud computing and artificial intelligence are seen as the most significant drivers for the geospatial industry
    Driven largely by sharp expansion of the user base, Asia pacific is expected to edge out North America as the largest market region with a market share of 32.6\% by 2020
    The Countries Geospatial Readiness ranking is led by the USA, followed by the UK, Germany, Singapore and the Netherlands being ranked 2nd, 3rd, 4th and 5th respectively
    The industry in China and India is continuing to expand due to an exponential rise in the number of technical and scientific research centers, aero-space domain strengths and national programs for startups},
  comment      = {Summary ppt is on the BC\&I drive under management.
The report ranks the UK 2nd in the world for its geospatial readiness and examines the key trends impacting the geospatial industry globally. It states that geospatial readiness is a key enabler of digital innovation and highlights that geospatial technologies have contributed US$2.3bn to the global economy in 2017.},
  creationdate = {2018.06.12},
  owner        = {ISargent},
  year         = {2018},
}

@InProceedings{XueLWL2018,
  author       = {Xue, Lei and Liu, Chang and Wu, Yunqiang and Li, H},
  booktitle    = {ISPRS TC III Mid-term Symposium “Developments, Technologies and Applications in Remote Sensing”},
  date         = {7-10 May},
  title        = {Semantic Segmentation of Convolutional Neural Network for Supervised Classification of Multispectral Remote Sensing},
  doi          = {https://doi.org/10.5194/isprs-archives-XLII-3-2035-2018},
  url          = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLII-3/2035/2018/},
  volume       = {XLII-3},
  address      = {Beijing, China},
  comment      = {use U-Net for geospatial application but it is very unclear what they have done. Seems to be 8 bands at one point...},
  creationdate = {2018.06.25},
  journal      = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  keywords     = {Deep learning, remote sensing, segmentation, MLStrat DLRS},
  owner        = {ISargent},
  year         = {2018},
}

@Article{BuscombeR2018,
  author    = {Daniel Buscombe and Andrew C. Ritchie},
  title     = {Landscape classification with deep neural networks},
  issue     = {7},
  url       = {https://www.mdpi.com/2076-3263/8/7/244},
  volume    = {8},
  comment   = {Reviewed this article before publication: 

The authors propose a method for segmenting remote sensing data into geomorphic classes then couples a simple deep convolutional neural network (DCNN) with a condition random field (CRF) method.

Motivation given for this approach is that the geoscience community often lacks the access to tools for training many DCNN models. In particular, as mentioned in the discussion (which perhaps should be in the introduction) that DCNN-based segmentation approaches tend to require particularly complex architectures in order to perform 'upscaling' to achieve the same resolution at the output as at the input.

The CRF graphical model defines the probability of the class of a given pixel based on its spectral similarity and spatial distance to other pixels and their expected classes, as well as ensuring smoothness across the scene (i.e. limiting small isolated regions of classes). This method will reclassify any previously classified pixels that have a class probability of less than a given value.

I understand that the segmentation is achieved using the following 6 steps:
1) A user-interactive tool enables the manual delineation of regions in the input image of specific classes
2) The CRF method is used to estimate the class for every the pixel within the image
3) Training and evaluation data sets are created by selecting tiles from the image that contain a proportion of pixels that is greater than a given threshold
4) The training tiles are used to fine-tune/retrain a DCNN
5) The retrained DCNN is used to to classify a portion of the image
6) The CRF method is used to estimate a class for every pixel in the image

General comments:
A very clearly written paper, especially the overview in the introduction. Worthy of publication with the clarification of points itemised below.

Points to clarify:
1. Any of the above that I have misunderstood
2. Line 77, replace Imagnet with ImageNet
3. Lines 115-117, the sentence 'Much work in leaning with graphical models...' needs a reference.
4. In 2.2, w.r.t. the user-defined class regions - are these 'exhaustive' i.e. must the user delimit the entire region within the chunk that pertains to the class or can these be 'exemplative' i.e. simply demonstrating some of the region in the chunk that pertains to the class?
5. Section 2.3 requires an over-riding statement to clarify that retraining is used such as 'we used the labelled tiles to retrain a DCNN'
6. Line 238 are the '1000 training steps' training epochs or something else?
7. The term 'fine-tuning' is used in some literature in the way that 'retraining' is used in this paper. This may need clarifying, particularly where 'fine-tuning' is used in 3.1 to refer to the tuning of hyperparameters.
8. With DCNN retraining - how much of the network is tuned? Is it all the weights or just the final layer or...?
9. Please provide more information about the architecture of MobileNetV2.
10. In the caption for Tables 1 and 2, I do not know to what out-of-calibration' refers
11. In 3.1, results are given for whole image tile classification. How much of the above 6 steps is used, if any, in this process? My understanding is that classes are already available for these tiles and so this would simply require the retraining of the DCNN against the pre-defined classes
12. If the training sets already have redefined classes, how are these used as part of the 6 steps? Are they the 'pre-defined set of classes' in Section 2.2?
13. Line 358 - combining classes is an interesting approach to improving accuracy - surely the classes are the desired outcome rather than the accuracy?
14. Testing against ground truth would be necessary to be able to reliably claim the accuracy statements since it appears that no data is obtained from the field and that most training data is aquired using an unvalidated approach (generation based on a sparse set of hand-derived examples).
15. Figure 10d, please consider replacing either red or green with another colour to aid differentiation by readers with common forms of colour-blindness},
  journal   = {Geosciences},
  keywords  = {deep learning, remote sensing, Segementation, MLStrat Segmentation},
  owner     = {ISargent},
  creationdate = {2018.06.25},
  year      = {2018},
}

@InProceedings{LiSSS2018,
  author       = {Y. Li and M. Sakamoto and T. Shinohara and T. Satoh},
  booktitle    = {Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci},
  title        = {Object Detection from Mms Imagery Using Deep Learning for Generation of Road Orthophotos},
  url          = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLII-2/573/2018/},
  volume       = {XLII-2},
  comment      = {MattR says: ``Research seeks to mask out temporary road objects that skew orthophotos of road environments. Mobile-mapping conducted, implementing a VaS (vehicle and its shadow) detection algorithm to locate regions of vehicles and their shadows from a mobile mapping system. This is executed in a Faster R-CNN to achieve high accuracy of detection of the VaS region. Objects are detected directly in an input image, followed by correction of the classification and region of the candidates to improve the accuracy of the result. Ortho is done in post-processing so there was no requirement for high processing speeds during capture. Maximum recall was reportedly high (0.96), with an intersection over union of > 0.7.  Algorithm was robust to varying vehicle shape (occlusions and shadow direction). Significant improvements to ortho-mosaic quality''},
  creationdate = {2018.06.25},
  keywords     = {deep learning, remote sensing},
  owner        = {ISargent},
  year         = {2018},
}

@InProceedings{LiLOVPKH2018,
  author       = {F. Li and M. Lehtom\''{a}ki b and S. Oude Elberink and G. Vosselman and E. Puttonen and A. Kukko and J. Hyypp\''{a}},
  booktitle    = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  title        = {Pole-like Road Furniture Detection in Sparse and Unevenly Distributed Mobile Laser Scanning Data},
  url          = {https://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/IV-2/185/2018/isprs-annals-IV-2-185-2018.pdf},
  volume       = {IV-2},
  comment      = {MattR says: ``Research employs a mobile laser scanner (Velodyne sensor) to produce a sparse and unevenly distributed point cloud for automatically detecting pole-like road-side furniture. Researched is pitched in an autonomous-driving future focusing on the inherent requirement to capture and recognise these features due to their essential traffic functionalities. Methodology does not rely on consistent point density nor a uniform point distribution. Rough classification is carried out to identify three classes (building, ground and vegetation) to then remove buildings and trees to aid processing time and avoid erroneous identification. Horizontal point-cloud slicing, in combination with cylinder masking is proposed to identify and extract features. Slice diameter, as well as diameter-height ratio is considered when thresholding. Two test sites used, one using IGN's Stereopolis II system on a 0.45km road scene. 0.86 detection completeness was achieved on features greater than 0.5 metres in height. Most error derived from connected objects who reside extremely close to building facades.''},
  creationdate = {2018.06.25},
  keywords     = {Street view imagery},
  owner        = {ISargent},
  year         = {2018},
}

@InProceedings{LaupheimerTHS2018,
  author       = {Dominik Laupheimer and Patrick Tutzauer and Norbert Haala and Marc Spicker},
  booktitle    = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  title        = {Neural Networks for the Classification of Building Use from Street-view Imagery},
  url          = {https://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/IV-2/177/2018/isprs-annals-IV-2-177-2018.pdf},
  volume       = {IV-2},
  comment      = {MattR says: ``Research seeks to classify terrestrial images (sourced from Google StreetView) of building facades into five different utility classes (commercial, hybrid, residential, specialUse and underConstruction) using convolutional neural networks. Images linked to 2D cadastral data provided by City Survey Office Stuttgart (consisting of polygons attributed with semantic information for each building. Four networks were investigated (VGG16/19, ResNet50 and InceptionV3) with the highest overall accuracy attained by InceptionV3 with 64\% across the 5-class test set. Best performing class (residential) achieves 98.57\% accuracy. High intra-class variance of the likes of specialUse proved to give bad seperability from other classes. Paper overlaid results on to images to produce class activation maps to interpret and localise learned features within input images. Used class activation maps to find what causes the network to fire.''},
  creationdate = {2018.06.25},
  keywords     = {Street level imagery, MLStrat DLRS},
  owner        = {ISargent},
  year         = {2018},
}

@InProceedings{GriffithsB2018,
  author       = {David Griffiths and Jan Boehm},
  booktitle    = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  title        = {Rapid Object Detection Systems, Utilising Deep Learning and Unmanned Aerial Systems ({UAS}) for Civil Engineering Applications},
  url          = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLII-2/391/2018/isprs-archives-XLII-2-391-2018.pdf},
  volume       = {XLII-2},
  abstract     = {With deep learning approaches now out-performing traditional image processing techniques for image understanding, this paper accesses the potential of rapid generation of Convolutional Neural Networks (CNNs) for applied engineering purposes. Three CNNs are trained on 275 UAS-derived and freely available online images for object detection of 3m2 segments of railway track. These includes two models based on the Faster RCNN object detection algorithm (Resnet and Incpetion-Resnet) as well as the novel onestage Focal Loss network architecture (Retinanet). Model performance was assessed with respect to three accuracy metrics. The first two consisted of Intersection over Union (IoU) with thresholds 0.5 and 0.1. The last assesses accuracy based on the proportion of track covered by object detection proposals against total track length. In under six hours of training (and two hours of manual labelling) the models detected 91.3\%, 83.1\\% and 75.6\\% of track in the 500 test images acquired from the UAS survey Retinanet, Resnet and Inception-Resnet respectively. We then discuss the potential for such applications of such systems within the engineering field for a range of scenarios.},
  comment      = {MattR says: ``David Griffiths: Rapid object detection using deep learning. UAVs + CNNs for real-time detection''},
  creationdate = {2018.06.25},
  keywords     = {deep learning, remote sensing},
  owner        = {ISargent},
  year         = {2018},
}

@InProceedings{ChenWGYHJW2018,
  author       = {Kaiqiang Chen and Michael Weinmann and Xin Gao and Menglong Yan and Stefan Hinz and Boris Jutzi and Martin Weinmann},
  booktitle    = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  title        = {Residual Shuffling Convolutional Neural Networks for Deep Semantic Image Segmentation Using Multi-modal Data},
  url          = {https://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/IV-2/65/2018/isprs-annals-IV-2-65-2018.pdf},
  volume       = {IV-2},
  abstract     = {In this paper, we address the deep semantic segmentation of aerial imagery based on multi-modal data. Given multi-modal data composed of true orthophotos and the corresponding Digital Surface Models (DSMs), we extract a variety of hand-crafted radiometric and geometric features which are provided separately and in different combinations as input to a modern deep learning framework. The latter is represented by a Residual Shuffling Convolutional Neural Network (RSCNN) combining the characteristics of a Residual Network with the advantages of atrous convolution and a shuffling operator to achieve a dense semantic labeling. Via performance evaluation on a benchmark dataset, we analyze the value of different feature sets for the semantic segmentation task. The derived results reveal that the use of radiometric features yields better classification results than the use of geometric features for the considered dataset. Furthermore, the consideration of data on both modalities leads to an improvement of the classification results. However, the derived results also indicate that the use of all defined features is less favorable than the use of selected features. Consequently, data representations derived via feature extraction and feature selection techniques still provide a gain if used as the basis for deep semantic segmentation.},
  comment      = {MattR says: ``Kaiqiang Chen: Residual shuffling CNN for image segmentation.''},
  creationdate = {2018.06.25},
  keywords     = {deep learning, image segmentation, MLStrat Segmentation},
  owner        = {ISargent},
  year         = {2018},
}

@InProceedings{SchnabelBDJ2018,
  author       = {Tobias Schnabel and Paul N. Bennett and Susan T. Dumais and Thorsten Joachims},
  booktitle    = {WSDM'18},
  date         = {February 5-9, 2018},
  title        = {Short-Term Satisfaction and Long-Term Coverage: Understanding How Users Tolerate Algorithmic Exploration},
  doi          = {https://doi.org/10.1145/3159652.3159700},
  address      = {Marina Del Rey, CA, USA},
  comment      = {About recommender systems. Use Amazon Mechanical Turk to elicit feedback on satisfaction with different amounts of recommendations in particular trade off between maximising satisfaction by using knowledge of user's interests and discovering additional interests (exploration). Small amounts of exploration are tolerated well, but there is a super-linear drop-off in satisfaction as exploration increases.},
  creationdate = {2018.07.11},
  keywords     = {Recommender systems},
  owner        = {ISargent},
  year         = {2018},
}

@Article{VeitWB2016,
  author           = {Andreas Veit and Michael Wilber and Serge Belongie},
  title            = {Residual Networks Behave Like Ensembles of Relatively Shallow Networks},
  url              = {https://dl.acm.org/doi/10.5555/3157096.3157158},
  comment          = {Great short paper investigating aspects of information flow through residual networks. Residual netwoks introduce identity skip-connections that bypass residual layers. These have enabled networks that are two orders of magniture deeper than previous models. However, ``biological systems can capture complex concepts within half a dozen layers''. This paper observes that removing single layers from residual networks does not noticeably affect their performance, in contrast to traditional architectures. This paper finds that residual networks can be seen as collections of many paths and the effective paths are relatively shallow. ``Residual networks can be seen as a special case of highways networks'' in which data flows equally through both paths. Contains a nice short overview of hierarchical approaches. Also figures of unravelled view of residual block. Paths in residual network have varying lengths, the distribution centres around a mean of n/2 where n is the numer of modules. By showing how much gradient is induced on the first layer through paths of different lengths they show that paths longer than about 20 modules are generally too long to contribute noticeable gradient during training. Similar to SrivastavaGS2015 they find that short paths are more important. Lesion study demonstrated that removing layers didn't affect performance much, although removing downsampling layers had slightly more negative impact. Also found that the structure of a resifual network can be changed at test time without affecting the performance. Error increases smoothly with the removal of layers, indicating that they behave like ensembles. I think this paper challenges the assumption of hierarchical representations.},
  creationdate     = {2018.07.11},
  journal          = {arXiv:1605.06431v2},
  keywords         = {ImageLearn, deep learning, architecture, MLStrat Milestones},
  modificationdate = {2023-10-31T10:40:18},
  owner            = {ISargent},
  year             = {2016},
}

@Article{HintonSKSS2012,
  author       = {Hinton, Geoffrey E and Nitish Srivastava and Alex Krizhevsky and Ilya Sutskever and Salakhutdinov, Ruslan R},
  title        = {Improving neural networks by preventing co-adaptation of feature detectors},
  comment      = {From VeitWB2016 ``Hinton et al. [7] show that dropping out individual neurons during training leads to a network that is equivalent to averaging over an ensemble of exponentially many networks.''},
  creationdate = {2018.07.11},
  journal      = {arXiv:1207.0580},
  keywords     = {deep learning, architecture},
  owner        = {ISargent},
  year         = {2012},
}

@Article{SrivastavaGS2015,
  author       = {Srivastava, Rupesh Kumar and Klaus Greff and Jürgen Schmidhuber},
  title        = {Highway networks},
  comment      = {From VeitWB2016  ``observe empirically that the gates commonly deviate from ti() = 0:5. In particular, they tend to be biased toward sending data through the skip connection; in other words, the network learns to use short paths''},
  creationdate = {2018.07.11},
  journal      = {arXiv:1505.00387},
  keywords     = {deep learning, architecture},
  owner        = {ISargent},
  year         = {2015},
}

@InProceedings{MorcosBRB2018,
  author           = {Morcos, Ari S. and Barrett, David G.T. and Rabinowitz, Neil C. and Matthew Botvinick},
  booktitle        = {ICLR},
  title            = {On the Importance of Single Directions for Generalization},
  url              = {https://arxiv.org/abs/1803.06959},
  comment          = {There are networks that memorize data well and there are networks that train well. I'm not sure if they are mutually exclusive, but we do want networks that generalise well. This paper looks at something called 'single directions' which I think means that specific inputs follow single paths through a network in determines that where these are present in a training/trained network the network is more likely to be memorizing the data or not generalising well. Therefore, they suggest that detecting single directions, using ablations - that is resetting node outputs to single values unrelated to the input (zero works better here than the average for the data) - indicates how well the network is avoiding single directions. Suggest this can be used to trigger early stopping i.e. when single directions are being detected, or something. Also suggest that specific approaches to stop single directions would improve training. Batch normalisation seems to help here, but is not explicitly for this purpose. Dropout is also a good candidate although they find that it does not appear to regularise for single directions past the dropout fraction. Stuff about class/concept selectivity - do parts of the network respond equally no matter what the class or are they selective? Measure this in two ways - using a measure of selectivity and measure of mutual information. ``Interestingly, we found that while networks trained without batch normalization exhibited a large fraction of feature maps with high class selectivity (and dead feature maps), the class selectivity of feature maps in networks trained with batch normalization was substantially lower. In contrast, we found that batch normalization increases the mutual information present in feature maps. These results suggest that batch normalization actually discourages the presence of feature maps with concentrated class information and rather encourages the presence of feature maps with information about multiple classes, raising the question of whether or not such highly selective feature maps are actually beneficial.'' In summary, having nodes that are selective to specific classes may harm generalisation - this paper then possits that ``methods for understanding neural networks based on analyzing highly selective single units, or finding optimal inputs for single units, such as activation maximization (Erhan et al., 2009) may be misleading''. But surely, being selective to specific classes in the target data is not the same as being selective to different concepts in the input, or is it?},
  creationdate     = {2018.07.11},
  journal          = {arXiv:1803.06959},
  keywords         = {deep learning, TopoNet, ImageLearn, visualisation, training, TopoNet Metrics, MLStrat Metrics, explainability},
  modificationdate = {2022-04-05T09:41:49},
  owner            = {ISargent},
  year             = {2018},
}

@InProceedings{AlmutairiCD2018,
  author       = {Nawal Almutairi and Frans Coenen and Keith Dures},
  booktitle    = {AI-2018: The Thirty-eighth SGAI International Conference},
  title        = {Secure Third Party Data Clustering Using \phi-Data: Multi-User Order Preserving Encryption and Super Secure Chain Distance Matrices},
  location     = {Cambridge, UK},
  comment      = {Overview:

This paper describes a method of data transform and encryption that reduces the computation and communication overhead incurred by the data owner and improves, over existing transformation methods, results of data clustering applied to the data. The original data are transformed using a chain distance metric to produce a chain distance matrix (CDM), which is encrypted to produce a secure CDM (SCDM). This encryption is achieved by applying a multi-user order preserving encryption (MUOPE) scheme. Several SCDMs are then bound together to form a super SCDM (SSCDM). The binding can be horizontal (appending new records with the same features) or vertical (appending new features) and permits similarity between records to be calculated. Clustering can then be applied to these data using the proposed secure DBSCAN (SDBSCAN) algorithm which requires encrypted parameters used by the MUOPE scheme. 

The paper introduced the concept of phi-data, which allows data to be securely shared and mined without additional involvement of the data owner.

Comparisons are made for clustering efficiency and accuracy (against DBSCAN) and an analysis of security and scalability is performed.

If any of the above is contrary to the intention of the paper, it may be worth considering suitable edits.

Comments:

As is probably obvious from the above, this paper takes me out of my field of knowledge but is very interesting to read and extremely well written. I have learned a lot! The paper definitely meets standards required for publication - my only question is does it meet the scope for the conference? That is, the paper describes a possible enabler for artificial intellingence but does not specifically mention any AI approaches.

I would have been interested in learning more about any disadvatages or unknowns to be investigated with this approach.

Specific edits:

Page 2, Line 2 ``secure'' in ``to secure computation'' is probably unnecessary
Page 3, line 10 replace ``effects'' with ``affecting''
Page 4, line 8 replace ``an HE scheme'' with ``a HE scheme''
Please increase the font size in all the figures
Please add a space between the figure number 5.1 in section 5
Please remove the newline after the double-quote around ``virtual'' in section 6},
  creationdate = {2018.08.01},
  owner        = {ISargent},
  year         = {2018},
}

@InProceedings{VoxW2018,
  author       = {Vox, Jan P. and Frank Wallhoff},
  booktitle    = {AI-2018: The Thirty-eighth SGAI International Conference},
  title        = {Human Motion Recognition using 3D-Skeleton-Data and Neural Networks},
  location     = {Cambridge, UK},
  comment      = {Overview:

Applies a NN to the recognition of human motion from Kinect data. Motions are classified into 16 exercises and the data set comprises 21 subjects performing each exercise 10 times. 

The network input comprises position data for parts of the skeleton (I presume joints) and the angles at specific joints, all normalised to a reference skeleton. Variance analysis was used to select 52 of the possible 89 features, to be used as network inputs.

Different network architectures are tested alongside learning rates with an optimum being networks with 100 neurons, 3 hidden layers and a learning rate of 0.001, achieving accuracies of over 90%.

Comments:

If any of the above is contrary to the intention of the paper, it may be worth considering suitable edits.

It is unclear how the position data are derived - I believe this is described in [7] but this should be briefly described in the current paper because it is unclear whether a manual or automatic process is used, or how reliable that process is.

It is also unclear on what data the different network configurations were tested - this should be a separate validation/dev data set but this doesn't appear to exist.

There is not enough detail of the subject being classified such as figures describing the data or the variance of the motion within and between subjects. Perhaps this is fully covered in [7] but some further details would give this work more meaning.

Conversely, quite a lot of space is devoted to the choice of network configuration. Whilst this is important, it could be more summaried, thus allowing more space for understanding about the data. Further, I would question whether a similar amount of effort was devoted to the configuration of the SVM in the previous work? What is it about the NN that allows it to perform better than the SVM? It is a fair comparison?

On the whole, the paper describes clearly the work undertaken and the outcomes - excepting the two 'unclear' aspects above, which need addressing. It is worthy of publication as a poster but would be strengthened by greater depth of understanding of the subject being classified and of why a NN is better than a SVM.

Specific edits:
Section 1, 1st para: Change ``In this work it is investigated...'' to ``This work investigates''
Section 4, 1st para: Change ``the position data to from a'' to ``the position data to form a''
Section 4, 1st para: Change ``figured in Fig. 1'' to ``shown in Fig. 1''
Please change the thousand ``.'' delimiter to ``,'' for the number of instances
Section 5, 1st line: Change ``an NN'' to ``a NN''
Section 5, 2nd para: Change ``With less number of instances'' to ``with fewer instances''
Please consider the final sentence, I am not sure what you are trying to say.},
  creationdate = {2018.08.01},
  owner        = {ISargent},
  year         = {2018},
}

@InProceedings{SmithKYL2018,
  author       = {Smith, Samuel L. and Pieter-Jan Kindermans and Chris Ying and Le, Quoc V.},
  booktitle    = {ICLR 2018},
  title        = {Don't decay the learning rate, increase the batch size},
  url          = {https://openreview.net/pdf?id=B1Yy1BxCZ},
  comment      = {Google Brain, of course. The title says it all. “Combining these strategies, we train Inception-ResNet-V2 on ImageNet to 77\% validation accuracy in under 2500 parameter updates, using batches of 65536 images. We also exploit increasing batch sizes to train ResNet-50 to 76.1\% ImageNet validation set accuracy on TPU in under 30 minutes. Most strikingly, we achieve this without any hyper-parameter tuning”. Usually a larger batch size works a lot better},
  creationdate = {2018.08.14},
  keywords     = {TopoNet, Deep learning, hyperparameters, MLStrat Training},
  owner        = {ISargent},
  year         = {2018},
}

@Article{WangWQV2018,
  author       = {Limin Wang and Zhe Wang and Yu Qiao and Van Gool, Luc},
  title        = {Transferring Deep Object and Scene Representations for Event Recognition in Still Images},
  number       = {2-4},
  pages        = {390-409},
  volume       = {126},
  abstract     = {This paper addresses the problem of image-based event recognition by transferring deep representations learned from object and scene datasets. First we empirically investigate the correlation of the concepts of object, scene, and event, thus motivating our representation transfer methods. Based on this empirical study, we propose an iterative selection method to identify a subset of object and scene classes deemed most relevant for representation transfer. Afterwards, we develop three transfer techniques: (1) initialization-based transfer, (2) knowledge-based transfer, and (3) data-based transfer. These newly designed transfer techniques exploit multitask learning frameworks to incorporate extra knowledge from other networks or additional datasets into the fine-tuning procedure of event CNNs. These multitask learning frameworks turn out to be effective in reducing the effect of over-fitting and improving the generalization ability of the learned CNNs. We perform experiments on four event recognition benchmarks: the ChaLearn LAP Cultural Event Recognition dataset, the Web Image Dataset for Event Recognition, the UIUC Sports Event dataset, and the Photo Event Collection dataset. The experimental results show that our proposed algorithm successfully transfers object and scene representations towards the event dataset and achieves the current state-of-the-art performance on all considered datasets.},
  comment      = {Working with the ChaLearn LAP Cultural Event Recognition dataset. Pass these images through networks trained to recognise objects (ImageNet) and scenes (Places) to get a probability of the images containing names objects and scenes. Then develop a method to use this probability to classify the event at which the image was taken.

''Recently, supervised CNN representations have been shown to be effective for a variety of visual recognition tasks (Girshick et al. 2014; Oquab et al. 2014; Chatfield et al. 2014; Sharif Razavian et al. 2014; Azizpour et al. 2015). Sharif Razavian et al. (2014) proposed to treat CNNs as generic feature extractors, yielding an astounding baseline for many visual tasks.''},
  creationdate = {2018.08.15},
  journal      = {International Journal of Computer Vision},
  keywords     = {transfer learning, deep learning, object, scene, event, MLStrat Transfer},
  owner        = {ISargent},
  year         = {2018},
}

@Article{BalntasLVM2017,
  author    = {Vassileios Balntas and Karel Lenc and Andrea Vedaldi and Krystian Mikolajczyk},
  title     = {HPatches: A benchmark and evaluation of handcrafted and learned local descriptors},
  journal   = {arXiv:1704.05939v1 [cs.CV]},
  year      = {2017},
  comment   = {Local image descriptors, specifically those used for image matching in scene reconstruction, can be derived manually or using deep learning. Evaluation of these descriptors is contradictory so the authors introduce a benchmarking dataset HPatches, conmprising pairs of image patches that represent the same feature from different views in order to determine how good a single or set of descriptor(s) is for identifying matched locations.},
  keywords  = {image matching},
  owner     = {ISargent},
  creationdate = {2018.08.15},
}

@Article{LencV2018,
  author       = {Karel Lenc and Andrea Vedaldi},
  title        = {Understanding Image Representations by Measuring Their Equivariance and Equivalence},
  doi          = {https://doi.org/10.1007/s11263-018-1098-y},
  comment      = {''In full generality, a representation \phi is a function mapping an image x to a vector \phi(x) \element \Real^d and our goal is to establish important statistical properties of such functions. We focus on two such properties. The first one is equivariance, which looks at how the representation output changes upon transformations of the input image . We demonstrate that most representations, including HOG and most of the layers in deep neural networks, change in a easily predictable manner with geometric transformations of the input. We show that such equivariant transformations can be learned empirically from data and that, importantly, they amount to simple linear transformations of the representation output. In the case of convolutional networks, we obtain this by introducing and learning a new transformation layer. As a special case of equivariance, by analyzing the learned equivariant transformations we are also able to find and characterize the invariances of the representation. This allows us to quantify geometric invariance and to show how it builds up with the representation depth.''

''The second part of the manuscript investigates another property, equivalence, which looks at whether different representations, such as different neural networks, capture similar information or not. In the case of CNNs, in particular, the non-convex nature of learning means that the same CNN architecture may result in different models even when retrained on the same data. The question then is whether the resulting differences are substantial or just superficial. To answer this question, we propose to learn stitching layers that allow swapping parts of different architectures, rerouting information between them. Equivalence and coverage is then established if the resulting ``Franken-CNNs'' perform as well as the original ones''

''Our aim is not to propose yet another mechanism to learn invariances (Anselmi et al. 2016; Bruna and Mallat 2013; Huang et al. 2007) or equivariance (Dieleman et al. 2016; Schmidt and Roth 2012b), but rather a method to systematically tease out invariance, equivariance, and other properties that a given representation may have.''

''Although the performance of representations has improved significantly as a consequence of [deep learning approaches], we still do not understand them well from a theoretical viewpoint; this situation has in fact deteriorated with deep learning, as the complexity of deep networks, which are learned as black boxes, has made their interpretation even more challenging. In this paper we aim to shed some light on two important properties of representations: equivariance and equivalence.''

''Yosinski et al. (2014), the authors study the transferability of CNN features between different tasks by retraining various parts of the networks.''

Whilst I understand the principal of what they are trying to achieve, and I think it may be useful for understanding how good our trainined deep networks are, I don't understand how they achieve it. Results are quite interesting - but I'll have to re-read them.},
  creationdate = {2018.08.15},
  journal      = {International Journal of Computer Vision},
  keywords     = {TopoNet Metrics, representation learning, MLStrat Metrics},
  owner        = {ISargent},
  year         = {2018},
}

@Unpublished{ZhangWSLXXXX,
  author       = {Bin Zhang and Cunpeng Wang and Yonglin Shen and Yueyan Liu},
  title        = {Fully Connected Conditional Random Fields for High Resolution Remote Sensing Land Use/Land Cover Classification with Convolutional Neural Networks},
  comment      = {My review: 

This paper proposes an approach to the land cover classification of remote sensing (RS) data that combines two sets of features with a fully-connected conditional random fields (FC-CRF) to reduce the noisiness of the classified output.

The first feature set, ``FCNN'', comprises the values extracted from the final layer of a CNN that has been fine-tuned using 3-band RS imagery, which is treated as the set of probabilities of the input belonging to each land cover class. Fine-tuning has been chosen to address the problem of having only a small training data set.

The second feature set, ``HC'', comprises values derived by training a support vector machine (SVM) with more layers of RS data and a digital surface model (DSM) against the same set of land cover classes, which are also treated as a set of probabilities that the input belongs to each land cover class.

Confusion matrices and overall accuracy are then assessed for different combinations of feature sets - FCNN, HC and a concatenation of FCNN and HC - and different classification methods - either training a SVM with the chosen feature set or applying the probabilities to a FC-CRF. Overall accuracies fall within the range 78-85%. The authors conclude that the best approach is to combine bother feature sets and apply FC-CRF.

The authors present an approach to RS land cover classification that has merits but is not fully justified either in its introduction for by its results. For example, no justification of using the 'hand-crafted' features is given, just the statement in the abstract ``it is necessary to utilize some hand-crafted features''. Whilst the results do show that the best method was the combination of feature sets (FCNNMF-FCCRFs), I am not convinced that the differences in overall accuracy are significant given, I assume, a small test size. Figure 4 also does not show any discernible improvement with any of the methods.

Such differences that area apparent could be achieved by adjusting hyperparameters prior to training the CNN or SVM. I feel this work needs a more robust justification which may be achieved by demonstrating the additional information contributed by each feature set, applying the method to a larger publicly-available data set or demonstrating a more thorough training approach using all methods. Please also supply details of the training, validation and test data used.

Further changes required for clarity:

The four classes are land cover (possibly excepting ``farmland'') and so I feel it is rather glorifying to consider this LULC classification. I cannot understand which class set is used - lines 119-122 list one set but a different set are used when describing the results. Also, it is not clear why either class set is selected. Why are ``farmland'' and ``water'' in the same class?

 The naming convention of the different methods, whereby a combination of abbreviations is used, is practically unreadable and a better convention or way of presenting is required.

'Hand-crafted' is a phrase more commonly used to describe features extracted for approaches such as SIFT and HOG. However, I don't think you mean this. It is not clear, but I have assumed that you simply mean the pixel values taken from the multispectral data.

 I needed to re-read several times to determine that the FCNN features were taken directly from the final layer of a fine-tuned CNN and that HC features were taken directly from the prediction of the SVM. Please state this more clearly.

By ``cascaded'', I think you mean ``concatenated''. This is much more understandable.

Line 137 - I don't know what ``several synthetic image bands'' are

Whilst the English is good, it needs improvement to be ready for publication. For example, phrases such as line 11 ``is a hotspot'', line 36 ``when the spatial resolution'' and line 170 ``but neurons are learnable convolutions shared at each image location'' don't really make sense.

There is a change of tense in the paragraph lines 222-228 and there may be others in the manuscript.

There are some glaring typos that are repeated throughout the manuscript including ``radom (fields)'' and ``imperious (surfaces)'' (a phrase I did enjoy!)

 The addition of ``s'' after many of the abbreviations is confusing - particularly in ``CNNs'' and ``CRFs''. Sometimes this is a plural and sometimes its seems to indicate possession i.e. ``CNN's parameters''. In many cases, the authors would be better saying something like ``the fine-tuning of the CNN'', ``CNN architecture'' and ``An introduction the FC-CRF approach''.

 Subscripts are missing, such as line 175 $x_i$. Also, please ensure that the same font is used for symbols in equations as is used in the main text.

The two sentences, lines 217-220 don't seem to add to this section, I'm not sure how they are relevant.

 Please check for hyperbole, such as ``impressive'' in line 27. This isn't scientific.

 Many of the references contain an itinerant ``[J]'' or [C]'' which needs removing.


To the editors:
This work overlaps with work that I am involved in and so this may be considered a conflict of interest. However, I have endeavoured to review this paper fairly. I believe there is merit in this work but it needs to be presented much more clearly and the justification and results need to be much more robust, stated in the comments above.},
  creationdate = {2018.08.16},
  journal      = {Remote Sensing},
  keywords     = {remote sensing},
  notes        = {Manuscript ID: remotesensing-346108, Type: Article, Number of Pages: 13},
  owner        = {ISargent},
  year         = {2018},
}

@InBook{NgMLY2018Section8,
  author    = {Andrew Ng},
  booktitle = {Machine Learning Yearning},
  title     = {8 Establish a single-number evaluation metric for your team to optimize},
  comment   = {This section talks about creating a single number metric to optimise when iterating training of ML algorithms.},
  keywords  = {toponet metrics, MLStrat Metrics},
  owner     = {ISargent},
  creationdate = {2018.08.17},
  year      = {2018},
}

@InBook{NgMLY2018Section9,
  author    = {Andrew Ng},
  booktitle = {Machine Learning Yearning},
  title     = {9 Optimizing and satisficing metrics},
  comment   = {Introduces the idea of satisficing metrics - whereby a threshhold allows the team to determine if a criterion has been met or not, in addition to an optimising metric that demonstrates if improvements are being made.},
  keywords  = {Metrics, MLStrat Metrics},
  owner     = {ISargent},
  creationdate = {2018.08.17},
  year      = {2018},
}

@Manual{McGarigal2015,
  author    = {Kevin McGarigal},
  title     = {FRAGSTATS HELP},
  year      = {2015},
  date      = {21 April 2015},
  url       = {http://www.umass.edu/landeco/research/fragstats/documents/fragstats.help.4.2.pdf},
  comment   = {Detailed discussion of landscape defnitions, metrics and probably much more. Mainly from the point of view of ecology but why not give this wider application? Sections:
BACKGROUND
Introduction
What Is a Landscape?
Classes of Landscape Pattern
Patch-Corridor-Matrix Model
The Importance of Scale
Landscape Context
Perspectives on Categorical Map Patterns
Scope of Analysis
Levels of Heterogeneity
Patch-based Metrics
Surface Metrics
Structural Versus Functional Metrics
Limitations in the Use and Interpretation of Metrics},
  keywords  = {landscape},
  owner     = {ISargent},
  creationdate = {2018.08.17},
}

@Article{SharifRazavianASC2014,
  author       = {Sharif Razavian, Ali and Hossein Azizpour and Josephine Sullivan and Stefan Carlsson},
  title        = {{CNN} Features off-the-shelf: an Astounding Baseline for Recognition},
  comment      = {From WangWQV2018 ``Sharif Razavian et al. (2014) proposed to treat CNNs as generic feature extractors, yielding an astounding baseline for many visual tasks''.},
  creationdate = {2018.08.22},
  journal      = {arXiv:1403.6382 [cs.CV]},
  owner        = {ISargent},
  year         = {2014},
}

@TechReport{KaplanPJ2016,
  author           = {Russell Kaplan and Raphael Palefsky-Smith and Liu Jiang},
  title            = {Visualizing and Understanding Stochastic Depth Networks},
  url              = {https://web.stanford.edu/class/cs331b/2016/projects/kaplan_smith_jiang.pdf},
  abstract         = {In this paper,  we understand,  analyze,  and visualize Stochastic Depth Networks,  an architecture introduced in March
of 2016.  Stochastic Depth Networks have enjoyed interest as a result of their significant reduction in training time while
beating the then state of the art in accuracy.  However, while Stochastic Depth Networks have delivered exceptional results,
no academic paper has sought to understand the source of their performance or their limitations.  In providing an analysis
of Stochastic Depth Networks' representations, error types, and strengths and weaknesses, we conduct seven experiments:
t-SNE on layer activations, weight activations, maximally activated images, guided backpropagation, dead neuron counting,
robustness to input noise, and linear classifier probes. By specifically comparing and contrasting Stochastic Depth Networks
with Fixed Depth Networks (standard residual networks), we discover that Stochastic Depth Networks have a faster training
time, a lower test error, similar clustering of data, and more strongly differentiated weight activatio},
  creationdate     = {2018.08.24},
  keywords         = {visualisation, toponet hack},
  modificationdate = {2022-11-29T11:52:40},
  owner            = {ISargent},
  year             = {2016},
}

@Article{ChattopadhyaySHB2018,
  author           = {Aditya Chattopadhyay and Anirban Sarkar and Prantik Howlader and Balasubramanian, Vineeth N},
  title            = {Grad-CAM++: Generalized Gradient-based Visual Explanations for Deep Convolutional Networks},
  url              = {https://arxiv.org/pdf/1710.11063.pdf},
  comment          = {Another paper looking at the contribution of different parts of an image to its CNN classification result

the main point of interest here is the suggestion that explanations can be used to train a shallower 'student' network with evidence that this is better than the original model},
  creationdate     = {2018.08.24},
  journal          = {arXiv:1710.11063},
  keywords         = {deep learning, visualisation, toponet hack},
  modificationdate = {2023-01-26T10:02:14},
  owner            = {ISargent},
  year             = {2018},
}

@Article{HuR2017,
  author       = {Peiyun Hu and Deva Ramanan},
  title        = {Finding Tiny Faces},
  comment      = {Paper with loads of insights into the effect of scale on detection. ``While training detectors for large objects is straightforward, the crucial challenge remains training detectors for small objects. We show that context is crucial, and define templates that make use of massively-large receptive fields (where 99\% of the template extends beyond the object of interest)''. ``We provide an in-depth exploration of scale-variant templates, which have been previously proposed for pedestrian detection[17]''. Brief review of other work considering context in finding instances of objects in images. ``Our model superficially resembles a regionproposal network (RPN) trained for a specific object class instead of a general “objectness” proposal generator [18]. The important differences are that we use foveal descriptors (implemented through multi-scale features), we select a range of object sizes and aspects through cross-validation, and our models make use of an image pyramid to find extreme scales.''. ``We propose a simple yet effective framework for finding small objects, demonstrating that both large context and scale-variant representations are crucial. We specifically show that massively-large receptive fields can be effectively encoded as a foveal descriptor that captures both coarse context (necessary for detecting small objects) and high-resolution image features (helpful for localizing small objects). We also explore the encoding of scale in existing pre-trained deep networks, suggesting a simple way to extrapolate networks tuned for limited scales to more extreme scenarios in a scale-variant fashion. Finally, we use our detailed analysis of scale, resolution, and context to develop a state-of-the-art face detector that significantly outperforms prior work on standard benchmarks.''},
  creationdate = {2018-11-21},
  institution  = {Robotics Institute, Carnegie Mellon University},
  journal      = {arXiv:1612.04402v2},
  keywords     = {Deep Learning, Spatial Scale, ImageLearn},
  owner        = {ISargent},
  year         = {2017},
}

@Article{Girshick2015,
  author       = {Ross Girshick},
  title        = {Fast {R-CNN}},
  url          = {https://arxiv.org/abs/1504.08083},
  comment      = {Speed up R-CNN by moving the convnet to before the region proposals.
From: https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e:
''The approach is similar to the R-CNN algorithm. But, instead of feeding the region proposals to the CNN, we feed the input image to the [VGG] CNN to generate a convolutional feature map. From the convolutional feature map, we identify the region of proposals and warp them into squares and by using a RoI pooling layer we reshape them into a fixed size so that it can be fed into a fully connected layer. From the RoI feature vector, we use a softmax layer to predict the class of the proposed region and also the offset values for the bounding box.

The reason “Fast R-CNN” is faster than R-CNN is because you don't have to feed 2000 region proposals to the convolutional neural network every time. Instead, the convolution operation is done only once per image and a feature map is generated from it.''},
  creationdate = {2018-11-21},
  institution  = {Microsoft Research},
  keywords     = {Deep Learning, object detection, localisation, MLStrat Object},
  owner        = {ISargent},
  year         = {2015},
}

@Article{RedmonF2016,
  author       = {Joseph Redmon and Ali Farhadi},
  title        = {{YOLO9000}: Better, Faster, Stronger},
  url          = {https://arxiv.org/abs/1612.08242},
  comment      = {Development of YOLO. ``YOLO suffers from a variety of shortcomings''...''compared to Fast R-CNN shows that YOLO makes a significant number of localization errors. Furthermore, YOLO has relatively low recall compared to region proposal-based methods. Thus we focus mainly on improving recall and localization while maintaining classification accuracy''. ``with YOLOv2 we want a more accurate detector that is still fast ... we simplify the network and then make the representation easier to learn''. ``By adding batch normalization on all of the convolutional layers in YOLO we get more than 2\% improvement in mAP''. Also finetune the original YOLO with full resolution ImageNet ``This high resolution classification network gives us an increase of almost 4\% mAP''. Instead of predicting coordinates of bounding boxes, like Faster R-CNN, YOLOv2 predicts offsets and confidences for anchor boxes at every pixel. The achor box dimensions are no longer hand-picked but chosen using k-means clustering. This results in 5 different dimensions of bounding boxes, the dimensions of which depend on the data set. The introduction of anchor boxes necessitated a change in the input image resolution.  

''Most detection frameworks rely on VGG-16 as the base feature extractor [17]. VGG-16 is a powerful, accurate classification network but it is needlessly complex...The YOLO framework uses a custom network based on
the Googlenet architecture [19]. This network is faster ... However, it's accuracy is slightly worse than VGG-16...We propose a new classification model to be used as the base of YOLOv2...Our final model, called Darknet-19, has 19 convolutional layers and 5 maxpooling layers''.

''YOLOv2 is state-of-the-art and faster than other detection systems across a variety of detection datasets. Furthermore, it can be run at a variety of image sizes to provide a smooth tradeoff between speed and accuracy.
YOLO9000 is a real-time framework for detection more than 9000 object categories by jointly optimizing detection and classification''.},
  creationdate = {2018-11-21},
  institution  = {University of Washington, Allen Institute for AI},
  keywords     = {Deep Learning; Object Detection, MLStrat Object},
  owner        = {ISargent},
  year         = {2016},
}

@TechReport{MoD_HumanMachine2018,
  author       = {{Ministry of Defence}},
  institution  = {Ministry of Defence},
  title        = {Joint Concept Note 1/18: Human-Machine Teaming},
  url          = {https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/709359/20180517-concepts_uk_human_machine_teaming_jcn_1_18.pdf},
  comment      = {Report on the affect of AI in military applications, particuarly wrt remote and autonmous systems (RAS) (e.g. UAVs).

Chapter 1 - Context
Chapter 2 - The evolution of remote and automated systems
Chapter 3 - Impacts on conflict
Chapter 4 - Human-machine teaming
Deductions and insights
Annex A - Understanding assessments of autonomy

''Artificial intelligence (AI) will transform war fighting.  Pursuing it will be non-negotiable.  Full exploitation of the potential of AI will be constrained by what can be assured.''

''The likely exponential nature of robotic and AI evolution suggests there will be three overlapping phases: augment; parallel; and supersede.''

The OODA loop: Observe, Orient, Decide, Act - AI can work in any of these aspects.

Cana machine do it? ``A useful rule of thumb when considering how well machines can be applied to a task is to understand how readily the activity can be codified.  The clearer the rules, metrics and recognition features a task has, the higher the likelihood that a machine can be optimised to undertake the task.  This is leading to surprising outcomes: roles traditionally considered to be challenging and that are often highly paid that involve data sorting or deterministic analysis like accounting, insurance estimation, legal documentation reviews and medical 
diagnostics, are proving to be automatable.  Whereas waiting on tables or care assistance for the elderly - often much lower wage attracting roles - are proving difficult to automate.  The last jobs to be automated in society will not simply be those of highly paid professionals.  Actions that we as humans struggle to comprehend will be very difficult to codify and ultimately automate.''

What computers and humans are good at: ``Broadly, computer algorithms are good at sorting and searching through large amounts of structured data (for example, text and document processing, people and enterprise information, and genetics), doing deterministic analysis (for example, counting, classifying and game playing), and producing predictable mechanical interactions (for example, manufacturing, flying and driving).  Computer algorithms are not as good at understanding complex unstructured data (for example, images, acoustics and environment structure or context), doing non-deterministic analysis (for example, road scene understanding or 
predicting human behaviour), and undertaking dexterous actions (for example, fine manipulation requiring touch and pressure feedback or handling deformable objects).  Despite these being more challenging fields for machines, it must be understood that machines are increasingly outperforming humans at some of 
these challenging tasks, including image recognition.  They do not suffer from concentration lapses, or fatigue, assuming access to a constant power supply.''

Lots to do: ``High-quality live and synthetic collective training and above all experimentation with AI systems will be essential for us to learn how to optimise our ability to create effective human-machine teams.''

Reassuring: ``The Ministry of Defence must continue to be proactive in considering legal, ethical and public concerns surrounding the use of robotics and AI.  ``},
  creationdate = {2018-11-22},
  owner        = {ISargent},
  year         = {2018},
}

@InProceedings{GirshickDDM2014,
  author       = {Ross Girshick and Jeff Donahue and Trevor Darrell and Jitendra Malik},
  booktitle    = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
  title        = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  doi          = {10.1109/CVPR.2014.81},
  pages        = {580-587},
  url          = {https://ieeexplore.ieee.org/document/6909475},
  comment      = {The R-CNN paper.
From: https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e:

''To bypass the problem of selecting a huge number of regions, Ross Girshick et al. proposed a method where we use selective search to extract just 2000 regions from the image and he called them region proposals. Therefore, now, instead of trying to classify a huge number of regions, you can just work with 2000 regions. These 2000 region proposals are generated using the selective search algorithm which is written below.

Selective Search:
1. Generate initial sub-segmentation, we generate many candidate regions
2. Use greedy algorithm to recursively combine similar regions into larger ones 
3. Use the generated regions to produce the final candidate region proposals 

R-CNN

These 2000 candidate region proposals are each warped into a square and fed into a pre-trained AlexNet convolutional neural network that produces a 4096-dimensional feature vector as output. The CNN acts as a feature extractor and the output dense layer consists of the features extracted from the image and the extracted features are fed into an SVM to classify the presence of the object within that candidate region proposal. In addition to predicting the presence of an object within the region proposals, the algorithm also predicts four values which are offset values to increase the precision of the bounding box. For example, given a region proposal, the algorithm would have predicted the presence of a person but the face of that person within that region proposal could've been cut in half. Therefore, the offset values help in adjusting the bounding box of the region proposal.''},
  creationdate = {2018-12-12},
  journal      = {arXiv:1311.2524v5},
  keywords     = {Deep learning, object detection, localisation, MLStrat Object},
  owner        = {ISargent},
  year         = {2014},
}

@Article{RenHGS2016,
  author       = {Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun},
  title        = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
  comment      = {Speed up Fast R-CNN by replacing VGG with ResNet and a separate network to predict region proposals.
From: https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e:
''Shaoqing Ren et al. came up with an object detection algorithm that eliminates the selective search algorithm and lets the network learn the region proposals.

Similar to Fast R-CNN, the image is provided as an input to a [ResNet] convolutional network which provides a convolutional feature map. Instead of using selective search algorithm on the feature map to identify the region proposals, a separate network is used to predict the region proposals. The predicted region proposals are then reshaped using a RoI pooling layer which is then used to classify the image within the proposed region and predict the offset values for the bounding boxes.''},
  creationdate = {2018-12-12},
  journal      = {arXiv:1506.01497v3},
  keywords     = {Deep learning, object detection, localisation, MLStrat Object},
  owner        = {ISargent},
  year         = {2016},
}

@Article{ChenG2017,
  author    = {Xinlei Chen and Abhinav Gupta},
  title     = {An Implementation of Faster RCNN with Study for Region Sampling},
  comment   = {Adapted Faster R-CNN from Caffe into Tensorflow and made some simplifications along the way. Short paper, may be worth studying if tweaks to object detection networks are being considered.},
  journal   = {https://arxiv.org/abs/1702.02138},
  keywords  = {Deep Learning, object detection, localisation, MLStrat Object},
  owner     = {ISargent},
  creationdate = {2018-12-12},
  year      = {2017},
}

@Online{HintonCapsulesLecture,
  author       = {Geoffrey Hinton},
  title        = {How to force unsupervised neural networks to discover the right representation of images},
  url          = {https://www.youtube.com/watch?v=So87FUzY0wM},
  comment      = {Easy to digest lecture from Hinton explaining why capsules (try to ignore the terrible editing).

In brain: mental image of an entity has a viewpoint - pose and scale. 
We can recognise things from many poses. But need to use mental rotation to compare left-right mirror image etc. Therefore, we represent things as separate views and can only determine the pose by mentally rotating the representations. 
''the way standard CNNs deal with spatial variance is completely hopeless.'' 

They don 't deal with scale and orientation because ``then you'd need to grid a 4D space''

NN do not use a structural design. Argues that computer vision ought to be inverse graphics. NNs are not inverse graphics. 

CN s are ``doomed'' because sub-sampling removes precise relationship between parts that are required for recognition. CNN are trying to find spatial invariance too soon-too early in the network. Ideally, The network should be equivariant - change the network by the same amount the input change. 

There is :
Place-coded equivariance: when more object and diff neurons are activated
Rate-coded equivariance: when same neurons are activated by different amount

Argues for capsules, each containing a few 100 Neurons . Each capsule outputs several numbers that define: × location,  y location, orientation, scale and probability for the visual entity.

They are more like steerable filter than a CNN's match templates. Steerable filters will take all the outputs and summarise rathan than rather than just the max output. 

How to train capsules:
Pairs of images related by a known transform. The target is to reconstruct 2nd image when transform is applied to the pose parameters of the capsule.
Calls this a ``Transformative auto encoder''

1st level of capsules asks: what is the entity? what is its pose?

Must'n't throw away pose information because this is essential throughout pathway.

''what'' pathway is determined by the relative ``wheres'' of its parts.},
  creationdate = {2018-12-12},
  keywords     = {Deep learning;Scale},
  owner        = {ISargent},
}

@Conference{BMVATLCV2017,
  date         = {2017-01-25},
  title        = {BMVA Symposium: Transfer Learning (TL) in Computer Vision (CV) - TLCV},
  comment      = {My understanding prior to this event was that transfer learning was largely restricted to taking a trained network, preferably a trained deep network, lopping off the last layer or two and sticking on an SoftMax network, SVM or similar, to apply the network to a new problem. Apparently, this is called DeCAF: Deep Convolutional activation features explain domain.

The event showed me that my understanding was very simplistic and there is already a considerable amount of work about transfer learning before deep networks are considered. This field is especially important for problems where this isn't much training data - in this case a machine is trained for some problem but then used for another. How it is used, however, may not be as simple as 'lopping off the last layers'. In an early presentation, from Teo de Campos, I learned of methods to transform the data so that the source and target domain are more similar but separation between classes remains large.

There was also a good deal of discussion about multi-task learning, in which a machine is trained to do several things from the same data, such as estimate the age and sex of a person in a photograph. This may or may not be part of transfer learning, depending on the author's perspective.

Zero-shot and one-shot learning were also discussed a lot - in this case the machine that has been trained in one domain gets no or 1 example from another domain. The zero-shot case does my head in a bit.
Another area that struck me was that of active learning, whereby, using some criterion, the machine choses which example to train with next. This is, I believe, a form of curriculum learning.

One very interesting presentation from Tim Hospidales discussed using attribute vectors to define the domain. These are part of the machine and become particularly useful if they provide additional information. The example given was when classifying animals - the attributes could indicate whether the animal has fur, its colours, etc. Therefore, if this machine is trained to identify tigers and pandas, because some attributes are shared it may be transferred to identifying leopards. 

A nice example of value of transfer learning was given for robotics by Tom Hospidales. It is possible to train a robotic arm to throw a ball into a cup and a ball on a string into a cup using reinforcement learning (by which the machine is given feedback on how well it met its target). However, it is not possible to train it to throw a ball on a string into a cup on the robotic arm using this method. Supervised learning much be used first, However, if the robot has first learned the two problems (throwing the ball into the cup and throwing the ball with the string) it is then able to learn how to do the hard task.

It occurs to me that each block of imagery is a new domain. We could or should be using solar azimuth and zenith and the time of year as well as camera characteristics as attribution when working with aerial imagery.

There was some good evidence that CNNs are not doing very well in replicating the activity of the brain, in their inability to interpret drawings and paintings. Some nice work was presented by Peter Hall that discussed recognition across domains from photos and art and the different types of art. It seems that CNNs are really not very good at this but a Multi-labelled Graph approach does work.

The final paper from Yongxin Yang was beyond my tired brain but I did pick up that he'd done some very clever stuff with “27 lines of code in TensorFlow”.
Isabel Sargent},
  creationdate = {2019-01-24},
  keywords     = {Machine Learning, Transfer Learning},
  owner        = {ISargent},
}

@InProceedings{HofferA2015,
  author    = {Elad Hoffer and Nir Ailon},
  booktitle = {ICLR 2015},
  title     = {Deep Metric Learning Using Triplet Network},
  url       = {https://arxiv.org/abs/1412.6622},
  comment   = {Propose a triplet network whereby 3 connected networks are trained using triplets of training examples. Each triplet contains an anchor, a positive exampel which is similar and a negative example which is disimilar. The loss function is based on a dissimilarity measure which should be high for the comparison between the anchor and the negative example and low for the comparison with the positive example.

Suggest an approach to 'unsupervised training' that bases the loss function on the spatial location of the image patch i.e. nearby patches are similar.  Alternatively use the temporal domain to define similarity.},
  keywords  = {Machine Learning, deep learning, DLRS, MLStrat Training},
  owner     = {ISargent},
  creationdate = {2019-03-06},
  year      = {2015},
}

@Article{SprinksHBM2019,
  author       = {J. Sprinks and R. Houghton and S. Bamford and J. G. Morley},
  date         = {2019-04-07},
  title        = {Planet Four: Craters—Optimizing task workflow to improve volunteer engagement and crater counting performance},
  comment      = {''Overall, the results of this study support the findings of previous related human factors research when considering volunteer engagement, with preference given to greater autonomy and variety suggesting that interfaces that incorporate these factors can provide an intrinsic motivation to take part. In terms of volunteer performance, again the influence of task workflow design factors differs depending on the performance measure concerned. The live nature of this study has additionally revealed the delicate balance between volunteer engagement and performance—reinforcing the importance that VCS developers and science teams consider the analysis required, the amount needed, and the prospective size of their volunteer community when considering a citizen science approach.''},
  creationdate = {2019-04-30},
  journal      = {Meteoritics \& Panetary Science},
  keywords     = {Crowdsourcing, Social Science, MLStrat Experts},
  owner        = {ISargent},
  year         = {2019},
}

@Article{SiuK2018,
  author    = {Siu, Caitlin R. and Murphy, Kathryn M.},
  date      = {24 April 2018},
  title     = {The development of human visual cortex and clinical implications},
  doi       = {https://doi.org/10.2147/EB.S130893},
  pages     = {25--36},
  volume    = {10},
  comment   = {Nice table showing A summary of the development of key visual perceptual milestones across the life span. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5937627/figure/f1-eb-10-025/},
  keywords  = {Vision, MLStrat Intro},
  owner     = {ISargent},
  creationdate = {2019-05-07},
  year      = {2018},
}

@Article{HongZ2020,
  author    = {Liang Hong and Meng Zhang},
  title     = {Object-oriented Multiscale deep features for Hyperspectral image classification},
  issue     = {14},
  volume    = {41},
  comment   = {My review before publication:

I have understood the proposed approach as follows:

Three grandularities of image segmentation are applied to hyperspectral imagery. For each resulting segment/object, a series of features (I do not know how many) are extracted using hard-coded approaches that produce values for the texture, shape and spectral aspects of the objects. My guess is that these features are extracted using all the pixels that fall within the objects but this is not clear. These features are concatenated into an input feature array for training a neural network. I have been unable to establish how the classes are allocated to each object for training.

This approach is compared to 12 other approaches with 3 different datasets obtaining a better result using the proposed approach than for those other approaches.

I have fundamental concerns about the approach, which may be down to my misunderstanding of how it is being implemented. Therefore, this paper requires either a considerable re-write to address this misunderstanding, or rework to address the following concerns.

Concerns:

My primary concern is the use of a 1-D convolutional layers on, essentially, unordered data. Convolutions are useful for data which are n-dimensional series within which the adjacency of values has meaning. For instance, is a sentence, the order of the words provides meaning; in an image, the pixels provide context to their neighbours. In these examples, if the order is changed, the meaning is lost. In the method of processing the image data presented in this paper, the order is not relevant - a spectral mean followed by a measure of contrast has the same meaning if it were 'stacked' the other way around. In this case, the convolutional layers should be replaced by fully-connected layers.

Other concerns:

A key attribute of deep neural networks is that they learn to extract the most meaningful features from the presented data. It is expected that texture and spectral features would be 'learned' by a CNN if these are relevant to the problem. Therefore it is unclear, and not explained, why this benefit of CNNs is being ignored and instead the texture and spectral features are being hardcoded before the neural network is applied.

It is difficult to understand the results because so many methods are being compared. There is little justification of the choices of methods being compared and I have struggled to understand the differences between the methods. As far as I can tell, none of the methods are are 'standard' CNN - why is this?

It is unclear how much effort has been put into parameter tuning of each method. The result that the proposed method apparently outperforms the others, at least according to some criteria, could be down to better parameter tuning. 

It is somewhat controversial to refer to a 5-layer network as 'deep'.

Other choices have been made without a clear justification, such as the use of tanh activation.

Other comments:

An excellent description of the segmentation approach.

Interesting analysis on scale effect - this aspect should be pursued.

The writing needs a lot of tidying up and the ordering of the paper could also be clearer. 
Please pay attention to singular and plural nouns.
Put spaces around parentheses.
Use 'the Salinas dataset'.
The architecture/parameters of the 1-D CNN setup would be better as a table.
Figure 8 should not be line plots - there is no progression from one datum to the next.
There are quite a few errors that will need fixing once the paper is in better shape.
A sentence beginning 'Some studies have shown...' or similar should end with at least one reference.

Improvement of the work:

I would be very interested to see how well the multi-scale features performed as inputs to a multi-layer perceptron and compare this to a non-object-based CNN with the same number of tunable parameters. Perhaps this could also be compared to an object-based CNN such as a segmentation network or one other state-of-the-art approach for hyperspectral data.},
  journal   = {International Journal of Remote Sensing},
  keywords  = {Machine Learning, Hyperspectral Imagery},
  owner     = {ISargent},
  creationdate = {2019-06-12},
  year      = {2020},
}

@WWW{Brownlee2019,
  author       = {Jason Brownlee},
  title        = {How to Calculate McNemar's Test to Compare Two Machine Learning Classifiers},
  url          = {https://machinelearningmastery.com/mcnemars-test-for-machine-learning/},
  comment      = {useful test for comparing two different ml classifiers using a single dataset},
  creationdate = {2019-06-26},
  keywords     = {Machine Learning, Testing, MLStrat Metrics},
  owner        = {ISargent},
  year         = {2018},
}

@Article{JozdaniJCXXXX,
  author       = {Shahab E. Jozdani and Brian A. Johnson and Dongmei Chen},
  title        = {Comparing deep neural networks, ensemble classifiers, and support vector machine for object-based urban land use/cover classification},
  comment      = {return to this paper for comments on XGBoost , working with multiple scales, key parameters for SVM and XGBoost, balancing training data with SMOTE

Paper describes research comparing the land use/land cover ability of 10 examples of classifiers, which fall into 3 catergories: deep neural networks, ensemble classifiers and support vector machines. By comparing classification accuracy for two different data sets - of different scene complexities - the conclusion is drawn that given the problem, a multi-layer perceptron performs best, that autoencoders as feature extractors do not improve classification results and that traditional machine learning is still relevant. 


The paper is well written and structured and will need only a minimum of re-writing. Further, the study is well executed and the conclusions reasonable - however this paper could be considerably improved with some changes to the experiments and their justification (see section 1 below). With the suggested improvements, this paper would stand as a good reference for future works deciding which classifier they chose for LULC problems, and specifically why more 'traditional' ML techniques are still relevant - i.e. in the case that large volumes of data are still unavailable. Regarding data volume, there already evidence that given enough data, any classifier will perform well (BankoB01) so the question being posed (which is the classifier of choice) is precipitated by the poor data volumes experience in this field. The conclusion that applying convolutional techniques in a GEOBIA context is not yet mature is quite correct.


1. The following improvements would make this an exceptional paper:

1.1 I would have liked there to be a little more understanding and reasoning behind the choice of classifiers and the use of the chosen classifiers. Specifically:
1.1.a one of the issues in the field of remote sensing is the difficulty in obtaining labelled data. This is an excellent reason for trying pre-training using unsupervised techniques such as autoencoders. 
1.1.b the chosen autoencoders are not particularly deep, and in fact the encoder is shallower than the MLP, and so it is not surprising they they do not perform as well.
1.1.c only the Resnet-50 could be considered to be a deep network. The others are very shallow, even in comparison to early 'deep' networks, and so should not be referred to as deep neural networks or DMLP. 
1.1.d Resnet-50, was chosen to fill the CNN role is an interesting choice since some have likened its architecture to an ensemble (VeitWB2016) of shallower networks.
1.1.e you have not chosen to try either a convolutional autoencoder or a pre-trainined (e.g. using ImageNet) deep network both as feature extractors in the way that you apply the shallow autoencoders. This could be a very powerful aspect to your work.

1.2 It would be useful and interesting to have more of a discussion of features in the context of LULC. There are many hard-coded features available and described in 3.2. However, these are only for the non-convolutional methods. It is believed, and partially demonstrated, that deep networks will learn the most useful features during training - from edge and colour detectors, to shape and texture, to potentially combinations that have been claimed to have more 'semantic' meaning in other fields of computer vision such as face detection. The distinction between hard-coded and learned features is worth making in this paper.


2. Changes required:

2.1 The description of neural networks (section 2.3) is a bit weak. It would be better to describe NN as a series of layers of neurons (or nodes - since they really aren't like brain cells) - specifically an input layer, 0 (in the case of a linear model) or more hidden layers and an output layer. The neurons in the layers are connected by weights and it is these (not the 'neuron parameters') that are adjusted iteratively to optimise the model.

2.2 Whilst the excellent work of ``Xavier and Yoshua'' (sic) was important in the development of deep neural networks (section 2.3) in that they determined the better activation functions (although sadly relu wasn't tested) identified a better strategy for weight initialisation, it is usually recognised that this is one of a number of developments that removed the barrier to using DNNs - including increasing data volume, increasing computational power.

2.3 It is not the depth of the network that increases the possiblity of overfitting but its ability to fit more complex models. This is subtly different to the statement at line 191 - which implies that it is the depth not the complexity that leads to overfitting.

2.4 Strictly, autoencoders are not applied to reduce the dimensionality of the data (think: overcomplete autoencoders) but to learn a representation of the data that is best for reconstructing the data - and is thus likely to be more meaningful than the original values.

2.5 The description of the stacked autoencoder is unclear. From Figure 3  I understand that there are 2 encoding layers followed by 2 decoding layers. However, your description says that multiple encoders are stacked together (which indicates an encode-decode-encode-decode structure, which is admittedly a bit odd).

2.6 You give a good description of the variational autoencoder - but it needs a bit more elaboration about the 'reconstruction process'.

2.7 The sentence in lines 247-8 ``A CNN hierachically mimics the humans' visual cortex'' could arguably be replaced with ``CNNs are very simplistic analogies of the mammalian visual cortex'' or removed altogether.

2.8 The observation (line 260) that CNNs only accept fixed-size image patches is identical to the issue that MLPs, SVM, RF, etc. only accept the number of features that they are trained on. You have observed that you can create CNNs with different number of bands and the shape of the patch can also be changed. I also do not understand how this leads to ignoring landcover boundaries. CNNs tend to be very good at finding edges but if boundaries are weak then they are less likely to be detected - but this is not because of the patch size.

2.9 Please clarify how the autoencoders are used (lines 365-72). After training, the encoder layers are used as a feature extractor from the input data into a classifier. Please clarify that the encoder layers are ALWAYS fine-tuned after fitting to the fully-connect classification layer.

2.10 Please elaborate the statistical difference between all the classifiers - this is an important aspect of the work. This should be referred to in the conclusions for example in lines 533-34.

2.11 Shadow is neither a land use or land cover. Please justify the use of this class or remove it.

2.12 The results/discussion/conclusion would be improved with some description of the spatial pattern of the different classified results. Are some more 'noisy' than others? Are some better at defining straight-edge features? etc...

2.13 In line 445 you comment that the BT model continues to degrade with increasing complexity of the image but as I understand there are only two images with different complexity. Therefore, you cannot draw this conclusion - or have I missed another test of BT against image complexity?

2.14 Please describe the geographic distribution of the test data because this effects the outcome - were the test areas dispersed among the training areas or in a set-aside region of each image?


3. Edits required
line: improvement required
94: please summarise the features in the 181-dimensional feature space?
117: what do you mean by 'pre-training procedure' and 'specific regularization' in this context?
144: in what context was Breiman experimenting (i.e. data/problem)...
145: ...and specifically what are these variables? Is this applicable to the current problem?
146: see above - please give context and applicability
631: please correct the authors names!

Table 1: Its worth including the CNN in this table and having a further column stating either the number of tunable parameters or some other measure of network complexity.
Table 2: correct REA/SEA/VEA (AE). Also please correct the F1-score for RAE

Figures 6-9, 11-14 - please indicate which is the predicted and which the actual axis. Also, consider how you can change the shading to better indicate the proportion of examples in the class - e.g. water appears poorly classified, but there are very few exaples

check citation format on lines (should it be 'et al.' with 4 names?)
96 (x2), 98, 295

4. Text replacements required
line: replace-> with
DMLP-> MLP
55: pixel-by-pixel-> pixel-wise
62: divide the image into non-overlapping semantically similar-> group pixels into semantically similar non-overlapping
110: full-fledged-> mature
111: wall-to-wall-> comprehensively
112: other before in a comprehensive manner.-> other.
114: experimentally compared thoroughly-> thoroughly experimentally compared
118: above-> previous
142: an RF model-> a RF model
175: An MLP-> A MLP
182: tangential effect-> effect 
190: no any solid-> not any solid
216: Applying few neurons in the coding layer-> reducing the number of neurons that output from the encoding layer
216: squeeze feature space-> reduce dimensionality
218-9: helps us use neurons as many numbers as input data-> permits the same or more neurons in successive layers as their are input features
219-24: <please re-write because this is somewhat confused>
231: two hidden-> three hidden
233: VAE-> variational autoencoder (VAE)
249: features as well-> features
249-50: To put it simply, in-> In
253: connected to other-> connected to all other
370: pretrained layers-> encoder layers
374: lower-> encoder
386: lower-> worse
387: lowest-> worst
399: becomes larger-> become larger
402: no any difference-> no difference
407: SEA-> SAE
437: almost led to the similar conclusions-> led to similar conclusions
437: In this regard, the-> The
439: However,-> Again
440: worse-> worst
500: not be always-> not always be

5. References required
line: references are required for the statements
111: ...full-fledged in several studies
113: popular ensemble model for the classifictation of remote sensing images
122: previous studies showing the importance of multi-scale mapping
123: similar studies using single-scale comparison
133: decision tree
135: bagging tree
233: VAE

Refs},
  creationdate = {2019-06-26},
  journal      = {Remote Sensing},
  keywords     = {Machine Learning, Land use, Land cover},
  owner        = {ISargent},
  year         = {XXXX},
}

@Article{BonafiliaYGB2019,
  author       = {Derrick Bonafilia and David Yang and James Gill and Saikat Basu},
  title        = {Building High Resolution Maps for Humanitarian Aid and Development with Weakly- and Semi-Supervised Learning},
  comment      = {Use open street map to train resnets with patches of imagery - labels taken from OSM.  because there are problems with the labels (misalighment with imagery for instance) they call this weakly supervised. 

Also use trained network to find examples of no building by making predictions of the presence of buildings and setting a threshold (optimised based on the F1 score) to  ensure the expected labelling error is less than 1\% (we set  to maximize the f1 score subject to the constraint that our FN=PN < :01).

Transfer trained model to segmenting DeepGlobe data ``We then train a modified version of the DLinkNet-34 model that won 2018's DeepGlobe challenge on this data''

''we found overfitting to negatively affect the performance of the 34 and 50 layer Resnets and our best performing model was the 18 layer Resnet''},
  creationdate = {2019-07-25},
  keywords     = {TopoNet, Machine Learning, MLStrat Transfer},
  owner        = {ISargent},
  year         = {2019},
}

@Article{KriegeskorteMRKBETB2008,
  author       = {Nikolaus Kriegeskorte and Marieke Mur and Ruff, Douglas A. and Roozbeh Kiani and Jerzy Bodurka and Hossein Esteky and Keiji Tanaka and Bandettini, Peter A.},
  date         = {2008-12-26},
  title        = {Matching Categorical Object Representations in Inferior Temporal Cortex of Man and Monkey},
  comment      = {From video \url{https://www.youtube.com/watch?v=_-D4S0x5AFc} ``Our working hypothesis is the (Inferior temporal) IT may emply featuers that are optimised for distinguising conventional categories - that would suggest its not enough to do unsupervised learning and just represent natural shapes but behaviourally relevant divisions might inform the selection of those basis functions in IT''},
  creationdate = {2019-07-30},
  journal      = {Neuron},
  keywords     = {Unsupervised learning},
  owner        = {ISargent},
  year         = {2008},
}

@Article{KriegeskorteMB2008,
  author    = {Nikolaus Kriegeskorte and Marieke Mur and Peter Bandettini},
  title     = {Representational Similarity Analysis - Connecting the Branches of Systems Neuroscience},
  journal   = {Frontiers in System Neuroscience},
  year      = {2008},
  date      = {2008-05-19},
  comment   = {Describes the use of representation similarity analysis or  representational dissimilarlity matrices by which the measure of similarity of brain response between two stimuli is placed in a matrix. When comparing the matrices for humans and monkeys, they show similar patterns of finding, for example, faces have low dissimilarity from each other. Demonstrated in \url{https://www.youtube.com/watch?v=_-D4S0x5AFc}},
  keywords  = {Toponet Metrics; Quality},
  owner     = {ISargent},
  creationdate = {2019-07-30},
}

@Article{DoerschGE2016,
  author       = {Carl Doersch and Abhinav Gupta and Efros, Alexei A.},
  date         = {2016-01-16},
  title        = {Unsupervised Visual Representation Learning by Context Prediction},
  number       = {arXiv:1505.05192},
  url          = {https://arxiv.org/abs/1505.05192},
  comment      = {Train a CNN to predict the position of one patch relative to the previous patch. ``we show that the learned ConvNet can be used in the R-CNN framework and provides a significant boost over a randomly-initialized ConvNet, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations''},
  creationdate = {2019-07-30},
  journal      = {arXiv},
  keywords     = {Deep learning, TopoNet Training, MLStrat Training},
  owner        = {ISargent},
  year         = {2015},
}

@InProceedings{HigginsMPBGBML2017,
  author    = {Irina Higgins and Loic Matthey and Arka Pal and Christopher Burgess and Xavier Glorot and Matthew Botvinick and Shakir Mohamed and Alexander Lerchner},
  booktitle = {ICLR 2017 conference},
  title     = {beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework},
  url       = {https://openreview.net/forum?id=Sy2fzU9gl},
  comment   = {Variational autoencoders work by sampling from a distribution at the autoencoder bottleneck. The mean and standard deviation of the distribution are what is learned. This beta-VAE (explained well https://www.youtube.com/watch?v=9zKuYvjFFS8) or disentangled autoencoder force the nodes in the latent distribution to be uncorrelated by applying an addition term in the loss which weighs the scale divergence in the loss function.
https://deepmind.com/research/publications/beta-VAE-Learning-Basic-Visual-Concepts-with-a-Constrained-Variational-Framework},
  keywords  = {TopoNet Training, unsupervised, deep learning, MLStrat Unsupervised},
  owner     = {ISargent},
  creationdate = {2019-07-30},
  year      = {2017},
}

@Article{RuleBZAHKMNRPR2019,
  author    = {Adam Rule and Amanda Birmingham and Cristal Zuniga and Ilkay Altintas and Shih-Cheng Huang and Rob Knight and Niema Moshiri and Nguyen, Mai H. and Rosenthal, Sara Brin and Fernando Pérez and Rose, Peter W.},
  title     = {Ten simple rules for writing and sharing computational analyses in Jupyter Notebooks},
  journal   = {PLoS Compututational Biology},
  year      = {2019},
  date      = {2019-07-25},
  volume    = {15},
  number    = {7},
  url       = {https://doi.org/10.1371/journal.pcbi.1007007},
  comment   = {Focussed on the use of Notebooks like Jupyter - how to do research into these in a way that ensure reproducibility.
Rule 1: Tell a story for an audience - e.g. in your code
Rule 2: Document the process, not just the results - what you did, why you chose that parameter, etc.
Rule 3: Use cell divisions to make steps clear - too many too small cells are confusion, as are very long cells, find a balance
Rule 4: Modularize code - turn repeated steps into functions and classes, turns functions and classes that are used by multiple notebooks into modules and libraries
Rule 5: Record dependencies - for example in Conda save a yaml file of the libraries
Rule 6: Use version control - so that you can roll back when it goes wrong
Rule 7: Build a pipeline - when you've designed your process create a new notebook that is the pipeline
Rule 8: Share and explain your data - make available some data that allows the process/pipeline to be run
Rule 9: Design your notebooks to be read, run, and explored - do all the above, plus use a public code repo, readme files, etc.
Rule 10: Advocate for open research - ask others to test out what you've produced, try other's notebooks, advocate},
  keywords  = {Sustainable Software},
  owner     = {ISargent},
  creationdate = {2019-08-07},
}

@Article{FlemingS2019,
  author       = {Roland W. Fleming and Katherine R. Storrs},
  title        = {Learning to see stuff},
  doi          = {https://doi.org/10.1016/j.cobeha.2019.07.004},
  pages        = {100--108},
  volume       = {Volume 30},
  comment      = {''Highlights
•    Unsupervised deep learning is a powerful framework for studying visual perception.
•    Natural images are structured by `latent variables' (e.g. lighting, reflectance).
•    Learning to encode and predict image structure discovers statistical regularities.
•    These regularities teach the brain about the outside world.
•    Neural networks may reveal cues the brain uses to represent complex materials.''

Starting for the viewpoint of human perception and that humans are able to visually estimate ``Many visual properties—such as how faded denim appears, the ripeness of a pear, or the gracefulness of a ballet dancer—are hard to define in physical terms'' despite having no 'supervision' on such things ``learn how to estimate distal properties from image data, but also what to estimate in the first place''. ``How do we learn to see the outside world? It cannot be primarily through supervised learning because we never get detailed information about the true state of the world. ``

''We suggest that perception of complex material and object properties does not arise primarily through densely supervised learning, nor indeed through estimating predefined physical quantities. Rather, perceptual representations emerge through learning to encode and predict the visual input as accurately and efficiently as possible. This may seem like a paradoxical claim, yet we propose that the best way to learn how to infer the distal stimulus (i.e. properties of the outside world) is to get really good at describing the proximal stimulus.''

Gives some basics on deep learning and machine learning theory. Also outlines several different unsupervised learning frameworks that are worth investigating.

Potentially useful for adding justification for use of unsupervised approaches, using bahavioural science.},
  creationdate = {2019-08-17},
  journal      = {Current Opinion in Behavioral Sciences},
  keywords     = {Unsupervised, deep learning, Behavioural science, Neuroscience, vision, MLStrat Training},
  owner        = {ISargent},
  year         = {2019},
}

@Article{SauveBS2016,
  author       = {S\'{e}bastien Sauv\'{e} and Sophie Bernard and Pamela Sloan},
  date         = {2016-01-01},
  title        = {Environmental sciences, sustainable development and circular economy: Alternative concepts for trans-disciplinary research},
  pages        = {48--56},
  url          = {https://www.sciencedirect.com/science/article/pii/S2211464515300099},
  volume       = {17},
  comment      = {''we explore three concepts - “environmental sciences”, “sustainable development” and “circular economy” that have come into use to express scientific concerns and efforts to protect the environment. Our focus on these three concepts is guided by their prevalence in tackling environmental challenges as well as the transdisciplinary nature of each one. ``

Sustainable development: Meeting the needs of the present withoutcompromising the ability of future generations to meet their own needs. Core concept is a Societal objective (based on inter-generational sustainability)

Circular economy: Production and consumption of goodsthrough closed loop material flows that internalize environmental externalities linked to virgin resource extraction and the generation of waste (including pollution). Core concept: Model of production and consumption

Linear economy: By opposition to the circular economy, production and consumption of goods that (partially) ignore environmental externalities linked to virgin resource extraction and the generation of waste andpollution. Core concept: Model of production and exchange

Weak sustainability offers a more flexible approach. Weak sustainability proposes that natural capital can be at least partially substituted, allowing a given level of production to be maintained with the input of less and less natural capital and more and more man-made capital. 

The circular economy aims to decouple prosperity from resource consumption, i.e., how can we consume goods and services and yet not depend on extraction of virgin resources and thus ensure closed loops that will prevent the eventual disposal of consumed goods in landfill sites. Production and consumption also have associated “contamination transfers” to the environment at each step. In that sense, the circular economy is a movement towards the weak sustainability described earlier. 

A significant difference between the circular economy and the linear economy is that sustainable development, when applied through the linear model of production, may emphasize waste reduction, recycling and reduction of pollution, focusing mainly on the downstream processes of production and consumption. The result can be that, in “linear” sustainable development initiatives, products that are recuperated through recycling efforts are too often orphaned; value chains are simply not in place and there are few actors who are ready to use waste as raw materials for new production. 

Due to the fact that (as outlined above) experts do not agree on the notion of sustainable development, the ways that circular economy, linear economy and sustainable development are linked and compared can differ significantly. In particular, some specialists in environmental sciences perceive “sustainable development” as a set of initiatives that have been implemented within a linear thinking, thus for them sustainable development and linear economy have become inseparable. The circular economy therefore offers a solution where sustainable development, when implemented in a linear economy model of production, is perceived as a failure.},
  creationdate = {2019-08-27},
  journal      = {Environmental Development},
  keywords     = {Environment, Semantics},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2016},
}

@Article{Bendell2018,
  author       = {Jem Bendell},
  date         = {2018-07-27},
  title        = {Deep Adaptation: A Map for Navigating ClimateTragedy},
  comment      = {Paper that could not find a journal - now one of the most downloaded papers of 2018. Outlines the current physical science in terms of observations of temperature, sea level, ocrean acidification, crop yields. Also discusses descriptions of consequences of these impacts - ``starvation, destruction, migration, disease and war''. Goes on to discuss how some may see these descriptions as ``overly dramatic'' and that ``it would not be unusual to feel a bit affronted, distrubed or saddeneded by the arguments''. Further, that some believe that the public should not be presented with this information, for various reasons. Bendell presents 4 insights: 
1) people respond to data in terms of what it means to them, not necessary what the data present, 
2) ``bad news and extreme scenarios impact human psychikigy''
3) ``sometimes people can express a paternalistic relationship between themsleves as environmental experts and other people whom they categorise as ``the public'' 
4) Hopelessness and its related emotions of dismay and dispair are understandbly feared but wrongly assumed to be netirely negative - the reality is that through these emotions comes a new approach to living - courage, raidcal hope, and draws parallels in the expreince of Native Americans being moved to reservations. 
Then draws conclusions of the impact of this on human society. Concludes that we need to develop not only resilience, but relinquish (assets, behaviours and beliefs which make matters worse) and restoration (rediscovering attitudes and approaches to life and organisation from our pre-hydrocarbon era.

Resilience: ``How do we keep what we really want to keep?''
Relinquishment: ``What do we need to let go of in order to not make matters worse?''
Restoration: ``What can we bring back to help us with the coming difficulties and tragedies?''

Also, I love this paragraph on neo-liberal econoticsin the West's response to environmental issues: Hyper-individualistic: ``switching off lightbulbs or buying sustainable furniture, rather than promoting political action as engaged citizens''
Market-fundamentalist: carbon-cap and trade system rathen than exploring what more governemnt intervention could achieve
Incemental: ``celebrating small steps forward such as a company publishing a sustainability report rather than strategies designed for a speed and scale of change suggested by the science''
Atomistic: ``climate action as a separate issues from the governance or markets, finance and banking, rather than exploring what kind of economic system could permit or enable sustainability''

''the end of the idea that we can either solve or cope with climate change''
''At one point in early 2018, temperature recordings from the Arctic were 20 degrees Celsius above the averate for that date (Watts, 2018)''.
''The World Bank reported in 2018 that countries needed to prepare for over 100 million internally displaced people due to the effects of climate change (Rigaud et al., 2018)''.
''It is a responsible act to communicate this analysis now and invite people to support each other myself included, in exploring the implications, including the psycholocial and spiritual implications''.
''take a time to step back, to consider ``what if'' the analysis in these pages is true, to allow yourself to grieve, and to overcome enough of the typical fears we all have, to find meaning in new ways of being and acting''.

''...removing carbon from the atmosphere with machines...the current technology needs to be scaled by a factor of 2 million within 2 years, all powered by renewables, alongside massive emission cuts , to reduce the amount of heating alraedy locked into the system (Wadhams, 2018)...biological capture appear far more promising...planting trees, restoring soils used in agriculture and growing seagrass and kelp''.

''Foster argues that implicative denial is rife within the environmental movement, from dipping into a local Transition Towns initiative, signing online petitions, or revouncing flying, threre are endless ways for people to be ``doing somehting'' without seriously confronting the reality of climate change''

''And yet, I always come around to the same conclusion - we do not know. Ignoring the future because it is unlikley to matter might backfire. ``Running for the hills'' - to creat our own eco-community - might backfire. But we definitely know that continuing to work in the ways we have done until now is not just backfiring - it is holding the gun to our own heads''



https://www.youtube.com/watch?v=DAZJtFZZYmM&vl=en},
  creationdate = {2019-08-31},
  journal      = {IFLAS Occasional Paper 2},
  keywords     = {Environment},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2018},
}

@Article{KleresA2017,
  author           = {Jochen Kleres and \^{A}sa Wettergren},
  date             = {2017},
  journaltitle     = {Social Movement Studies},
  title            = {Fear, hope, anger, and guilt in climate activism},
  number           = {5},
  pages            = {507--519},
  url              = {https://www.researchgate.net/publication/318307076_Fear_hope_anger_and_guilt_in_climate_activism},
  volume           = {16},
  comment          = {Analyse the four emotions of fear, hope, anger and guilt to understand how activists manage them to both mobilize (externally) and sustain (internally) their activism.

Use interviews with activists

Previous work has found that 
Communual embeddedness shields against sliding from disappointment into the passivitiy of ongoing dispair (McGeer 2004). 
Fear of climate change attibuted to the self's incapacity to prevent it may result in withdrawal, while considering someone else responsiblity may result in anger (Barbalet 1998).
Hope undergirds but may also result from taking action in a self-perpetuating spiral (McGeer 2004).
May also be fear of inaction - fear may collapse hope but hope can mitigate fear (Miller et al 2009, others)

The IPCC report's poor public resonance can be attributed to the social organisation of denial effectively neutralising climate fears (Norgaard 2011)

''In this global arena, the looming fear of climate change is contained by neoliberal and consensus-orientated technological mitigations and adaptations, seeking hope in solutions 'outside one-self' Ojala, 2012)''

Conclusion is that in the Global North, fear, mitigated by hope, lead to action, which helps increase hope. However, in the Global South, extreme fear coupled with blame results in anger which leads to action - hope is not much part of this story.},
  creationdate     = {2019-08-31},
  keywords         = {Environment},
  modificationdate = {2024-05-07T09:14:45},
  owner            = {ISargent},
  priority         = {prio1},
}

@InBook{Tahmasebi2018,
  author    = {Pejman Tahmasebi},
  booktitle = {Handbook of Mathematical Geosciences},
  title     = {Multiple Point Statistics: A Review},
  doi       = {https://doi.org/10.1007/978-3-319-78999-6_30},
  pages     = {613--643},
  publisher = {Springer, Cham},
  url       = {https://link.springer.com/chapter/10.1007/978-3-319-78999-6_30},
  comment   = {Reviews the recent important developments in multiple point statistics, a branch of geostatistics, from the perspective of geology.

Kriging produces overly smooth ersults and cannot represent the heterogeneity of real-world phenomena. An alternative, stochastic simulation, using various similation methods, can provide several (many?) equi-probable representations, which can be use to characterise uncertainty over space. Simulations are often based on distributions or variograms which cannot represent real-world structures well. Alternatively, multiple point statistics use a 'training image' to provide the required data.

Chapter continues with detailed discriotion of different types of MPS approaches before looking specifically at its use in geostatistics.

The training image can be of any type, ranging from an image to statistical properties in space and time.},
  keywords  = {Geostatistics},
  owner     = {ISargent},
  creationdate = {2019-09-04},
  year      = {2018},
}

@Article{MeinshausenEtAl2011,
  author       = {Malte Meinshausen and S. J. Smith and K. Calvin and J. S. Daniel and M. L. T. Kainuma and J-F. Lamarque and K. Matsumoto and S. A. Montzka and S. C. B. Raper and K. Riahi and A. Thomson and G. J. M. Velders and van Vuuren, D.P. P.},
  title        = {The RCP greenhouse gas concentrations and their extensions from 1765 to 2300},
  doi          = {https://doi.org/10.1007/s10584-011-0156-z},
  pages        = {109:-213},
  abstract     = {We present the greenhouse gas concentrations for the Representative Concentration Pathways (RCPs) and their extensions beyond 2100, the Extended Concentration Pathways (ECPs). These projections include all major anthropogenic greenhouse gases and are a result of a multi-year effort to produce new scenarios for climate change research.   We combine a suite of atmospheric concentration observations and emissions estimates for greenhouse gases (GHGs) through the historical period (1750-2005) with harmonized emissions projected by four different Integrated Assessment Models for 2005-2100. As concentrations are somewhat dependent on the future climate itself (due to climate feedbacks in the carbon and other gas  cycles), we emulate median response characteristics of models assessed in the IPCC Fourth Assessment Report using the reduced-complexity carbon cycle climate model MAGICC6. Projected `best-estimate' global-mean surface temperature increases (using inter alia a climate sensitivity of 3°C) range from 1.5°C by 2100 for the lowest of the four RCPs, called both RCP3-PD and RCP2.6, to 4.5°C for the highest one, RCP8.5, relative to pre-industrial levels. Beyond 2100, we present the ECPs that are simple extensions of the RCPs, based on the assumption of either smoothly stabilizing concentrations or constant emissions: For example, the lower RCP2.6 pathway represents a strong mitigation scenario and is extended by assuming constant emissions after 2100 (including net negative CO2 emissions), leading to CO2 concentrations returning to 360 ppm by 2300. We also present the GHG concentrations for one supplementary extension, which illustrates the stringent emissions implications of attempting to go back to ECP4.5 concentration levels by 2250 after emissions during the 21st century followed the higher RCP6 scenario. Corresponding radiative forcing values are presented for the RCP and ECPs.},
  comment      = {Paper on understanding how the Representative Concentration Pathways pan out to 2100 and beyond (using the Extended Concentration Pathways).},
  creationdate = {2019-10-07},
  journal      = {Climatic Change},
  keywords     = {Climate Change, Environment},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2011},
}

@Article{BurkeWCHLO2018,
  author    = {K. D. Burke and J. W. Williams and M. A. Chandler and A. M. Haywood and D. J. Lunt and B. L. Otto-Bliesner},
  date      = {2018-12-26},
  title     = {Pliocene and Eocene provide best analogs for near-future climates},
  doi       = {https://doi.org/10.1073/pnas.1809600115},
  number    = {52},
  pages     = {13288--13293},
  volume    = {115},
  comment   = {Paper looking at how projected global temperatures match past climates. Takes proxies for past temperatures (5 models), contemporary temperature measures and the range of temperture outcomes for 4 representative Concentration Pathways and creates and really useful plot with them all together. Demonstrates how projections are similar to temperatures in the pliocene.},
  journal   = {PNAS},
  keywords  = {Climate Change, Paleoclimate, Environment},
  owner     = {ISargent},
  priority  = {prio1},
  creationdate = {2019-10-07},
  year      = {2018},
}

@Conference{SocRSE2019,
  author       = {Graham Lee},
  booktitle    = {The Fourth Conference of Research Software Engineering},
  title        = {(Research Software) Engineering is not Research (Software Engineering)},
  url          = {https://www.cs.ox.ac.uk/projects/RSE/rse_post/rseconf_talk/},
  comment      = {Write up of this talk can be found on the url link. Discusses how to create meaningful goals for research software engineering. Starts with looking at Agile design principles (\url{http://agilemanifesto.org/principles.html}). Some of these are relevant but others less so e.g. ``Our highest priority is to satisfy the customer through early and continuous delivery of valuable software'', ``Deliver working software frequently…Working software is the primary measure of progress'', ``Business people and developers must work together daily throughout the project'' largely because they can be impractical in a research setting.

Those principles that are applicable include ``At regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behaviour accordingly.'', ``Agile processes promote sustainable development. The sponsors, developers, and users should be able to maintain a constant pace indefinitely.'', ``Build projects around motivated individuals. Give them the environment and support they need, and trust them to get the job done.''

Describes alternatives that have been developed since these Agile principles were created: Software Craftsmanship, DevOps, lean StartUp and wonders whether something akin to Lean Research can be designed. 

''We have different values in research software engineering, and to identify the practices that we should engage in, we have to enumerate those values and the principles that will support them.''},
  creationdate = {2019-10-10},
  keywords     = {Sustainable Software},
  owner        = {ISargent},
  year         = {2019},
}

@Article{Conway1968,
  author       = {Melvin E. Conway},
  title        = {How Do Committees Invent?},
  comment      = {From summary on web page ``Any organization that designs a system (defined more broadly here than just information systems) will inevitably produce a design whose structure is a copy of the organization's communication structure. This turns out to be a principle with much broader utility than in software engineering, where references to it usually occur.''

When two things that have the same structure (e.g. design committee and thing designed) is called a homomorphism.

Nice discussion from The Morning Paper: https://blog.acolyer.org/2019/12/13/how-do-committees-invent/},
  creationdate = {2019-12-16},
  journal      = {Datamation magazine},
  keywords     = {Social Science},
  owner        = {ISargent},
  year         = {1968},
}

@Article{LampertNH2014,
  author       = {Christoph H. Lampert and Hannes Nickisch and Stefan Harmeling},
  title        = {Attribute-Based Classification for Zero-Shot Visual Object Categorization},
  doi          = {10.1109/TPAMI.2013.140},
  issue        = {3},
  pages        = {453 --465},
  volume       = {36},
  comment      = {Zero-shot learning (ZSL) - branch of transfer learning - using a pre-trained model to predict an unseen class, without any further training. Usually this is achieved by training the original model in such a way that it can apply to unseen classes. This paper suggests two approaches using attribute-based classification, that is, classification that is achieved by first classifying the attributes of a class. In this case, the classes are animals and attributes would be colour, pattern, habitate featuers, body parts. The two approaches are Direct attribute prediction (DAP) and Indirect attribute prediction (IAP).

In DAP, an interim stage between input and class prediction defines the attributes for the classes - graphically a hidden layer(s?) within the model would imply attributes. These attributes are learned either from attribute annotations of each input image or from an attribute vector that is defined to correspond to the class label (e.g. if the class is 'zebra' then attribute stripey would be set to 1). The class labels are then inferred from the values of these attribute. Because there is a deterministic dependance assumed between class and attributes, the relationship between class and attribute is fixed (hard-coded?), rather than learned. 

In IAP, the class label is first learned and the attributes then determined from this using a fixed relationship. At this stage, graphically, the model's hidden layer is the class label and the output layer is the attributes. At test time, further fixed relationship from attributes to unseen classes is put in place - making the attribute layer a further hidden layer and the output layer is the unseen classes.

''It assumes that each class has a description vector''

''To our knowledge, no previous approach allows the direct incorporation of human prior knowledge. Also all the [methods we have found in the literature] require at least some training examples of the target classes and cannot handle completely new objects'' 

Some interesting papers reviewed, including cognitive science such as how human judgements are influenced by characteristic object attributes.

Results are 'promising'},
  creationdate = {2019-12-20},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords     = {Deep Learning, Zero Shot Learning},
  owner        = {ISargent},
  year         = {2014},
}

@Article{ZhangS2015,
  author        = {Ziming Zhang and Venkatesh Saligrama},
  title         = {Zero-Shot Learning via Semantic Similarity Embedding},
  eprint        = {1509.04767},
  url           = {http://arxiv.org/abs/1509.04767},
  volume        = {abs/1509.04767},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/ZhangS15d},
  comment       = {Somewhat complex description of what I suspect is a fairly simple approach to zero-shot learning. I have read and re-read but cannot unpick some fundamental aspects of this paper. At it's highest level, classes are expressed as a set of attributes for which a transform is learned to a new ``semantic similarity embedding'' (SSE) space; Images are transformed to a new embedding by first passing them through a trained deep network (the deeper the better, apparently) and then a transform is learned to the SEE. This transform improves the separation between seen classes - resulting in a set of features that express each class. These features are, however ``guided by ... attribute vectors and indeed preserve affinities between classes in the attribute space''. I think that the embeddings from attributes (which are availabile with the data set or are hand-labelled, I think) to the SSE and from the data to the SSE are jointly learned. I'm sure it's all in the paper...},
  creationdate  = {2018-08-13 16:48:47 +0200},
  journal       = {CoRR},
  keywords      = {deep learning, MLStrat Transfer},
  owner         = {ISargent},
  year          = {2015},
}

@Article{PradhanASTA2020,
  author       = {Biswajeet Pradhan and Husam A. H. Al-Najjar and Maher Ibrahim Sameen and Ivor Tsang and Alamri, Abdullah M.},
  title        = {Unseen Land Cover Classification from High-Resolution Orthophotos Using Integration of Zero-Shot Learning and Convolutional Neural Networks},
  issue        = {10},
  volume       = {12},
  comment      = {reviewed Jan 1st 2020 (remotesensing-675557)
From <https://susy.mdpi.com/user/review/review/10460422/znM68OGE> before publication:

This paper describes an approach to Zero Shot Learning (ZSL) for remote sensing data that uses two shallow convolutional networks and Word2Vec to transfer image data into the (Word2Vec) semantic embeddings and then to land cover class. ZSL is possible because the semantic embedding can be derived for new classes for which the model has not been trained. K-nearest neighbour (KNN) is then used to determine whether outputs from unseen data are nearest to classes for which the model has been train or to, I assume, an exemplar from the unseen class(es).

This is an interesting pilot project that sets out a template for future work into ZSL with remote sensing data. However, it has some flawed reasoning and/or needs some reworking to enable the reader to understand what has been performed.

I am surprised at the accuracy achieved for the unseen classes given the method, can you explain this please? This seems high because CNN-1 is only extracting, at best, edge and colour information, and at very low dimensionality (32) - the mapping from this in 3-layer CNN-2 to the deeply semantic Word2Vec is ambitious. Can you demonstrate how good the prediction of Word2Vec is? If, instead, you were to training a 3 or 4- layer network to recognise all but one of the landcover classes and then used KNN from the final dense layer in the same way as you have in this paper (i.e. - leave out Word2Vec embedding - the final dense output is your embedding) - how do the results compare? This would make a more complete paper.

Why do you not train CNN-1 to predict Word2Vec embedding? Again, this would be an interesting comparative approach.

How are you predicting a class for every pixel (e.g. Figure 7)? This would require an image patch to be created for every pixel since you do not indicate that you are using a segmentation approach. If you are creating a patch for every pixel, are you not training with similar data as you use for testing - e.g. training and testing patches overlap. This could account for the high accuracy scores. Please clarify this.

The conclusions are good and interesting. They would sit better with the paper if the results in the paper were not so lauded - i.e. transforming remote sensing imagery to a new embedding and using this to predict seen and unseen classes is a solid approach to ZSL in remote sensing data, however there is a lot that can be improved from the presented approach that should improve the results. I am particularly interested in the outcome of the work creating a Wordd2Vec embedding that is more suitable for remote sensing applications.

Needs clarity:I cannot determine the target on which CNN-1 is trained. This layer extracts 'features' from the imagery but I cannot identify what the classification at the softmax is. This is fundamental to the understanding of this paper.

In section 3.3 it is claimed that CNNs ``automatically learn feature maps'' - this is incorrect. The weights in CNNs are adjusted during training to result in learned 'filters' or 'nodes', the outputs of which during the feedforward stage are the feature maps. In other words, it is the 'filters' or 'nodes' that are equivalent to you 'handmade filters'.

I don't believe the paper explains how KNN is used to determine the locality for unseen classes and so  I have inferred the method. If I have inferred correctly, by using an exemplar to determine whether unseen data are nearest to an unseen class, this is in fact one-shot learning - the KNN is being 'trained' using a single example.

Section 2 needs better descriptions of ZSL and the papers cited. The three main steps (89-91) are unclear - (ii) is not a step - and more discussion of method of citation [36] - [41] would add meaning to your paper. You can then refer better to these papers when describing your method - what does it borrow, what does it need to do differently?

Please check the notation in section 2.4. I don't believe that X, line 141 (feature space) is the same as X, line 144 (testing samples).

I'm not sure why you say the 2nd dense layer of CNN-2 is untrainable - please explain.

Figure 5 is weird - these are vectors derived from Word2Vec not 1-dimensional series. Please find a more intuitive way of plotting (line plots indicate that there is meaning in the line between one value and the next, there is not with these data) for example by re-projecting the vectors in a lower dimensional space. Of additional value here would be to include nearby and co-occurrent words (in Word2Vec) space so that readers can have a sense of the word concepts that are embedded within Word2Vec.

Literature:The discussion of OBIA at the start seems spurious because it does not relate to the rest of the paper at all (and is very poorly written). A mention of OBIA, alongside the many other techniques used to classify remote sensing imagery may be justified, but not to the detail given here.

Conversely, the discussion of CNNs is relevant but is too superficial. There is a good body of work applying CNNs to remote sensing both in pixel-based and segmentation approaches. Further, since you use dense layers, you should also talk about multi-layer perceptron architectures and the pros and cons of these.

Something that is often discussed when applying CNNs to imagery is how each consecutive layer in a network can represent higher concepts. For example, the first convolutional layer will learn filters for edge and colour opponent features, later layers will represent shapes and later still may represent possibly more 'semantic' concepts (e.g. body parts, wheels, faces - where these are sufficiently present in the training data). This would be a useful addition to this paper - you can then identify how much your shallow networks can represent within the imagery (i.e. very simple edge features and colours but nothing more). Without this discussion, your paper seems rather naïve of the limitations of CNNs.

Language:
The writing needs some improvement. The very first section up to line 48 is excellent but then there is a lot of ambiguity in the writing

Ambiguous phrases include (but are not limited to):
60: ``comprised in the requirement''
87: ``constructed appropriate progress''
88: ``leveraging semantic information''
105: ``remarks a further forbidding challenging setting''
111: ``testing in realistic nature''
111: ``flexible context''
236: ``kernel'' - I think you mean the target for training CNN-2

Please avoid hyperbole, it adds nothing to scientific writing, e.g.:52: ``remarkably''
103: ``extremely''
116: ``considerably''


To the editors:
This paper is not ready for publishing and needs considerable adjustment to the writing to be considered. If the writing fully clarified the approach (including better linking it to other works) then it may be ready for publication. However, I also think the rationale is poor and feel this is more the level of a 'letter' - I cannot make a judgement on whether this is a good enough level for this journal. I have suggested in my paragraphs 3 and 4 two additional studies that could make this a stronger paper. Still, I have some misgivings about the high accuracies achieved by this approach, this needs some justification (my paragraphs 3 and 5).},
  creationdate = {2020-01-01},
  journal      = {Remote Sensing},
  keywords     = {Deep Learning, CNN, Zero-Shot Learning, Land Cover, Land Use, pre-training, remote sensing, MLStrat DLRS},
  owner        = {ISargent},
  year         = {2020},
}

@Article{Bainbridge1983,
  author       = {Lisanne Bainbridge},
  title        = {Ironies of automation},
  number       = {6},
  url          = {https://www.ise.ncsu.edu/wp-content/uploads/2017/02/Bainbridge_1983_Automatica.pdf},
  volume       = {19},
  comment      = {The problem with automation is that there is always a need for human intervention, e.g. monitoring systems to identify failures, because automatic monitoring, could go wrong (alarms on alarms). The trouble here is that humans are very poor at monitoring and cannot keep up attention for long. ``Perhaps the final irony is that it is the most successful automated systems, with rare need for manual intervention, which may need the greatest investment in human operator training… I hope this paper has made clear both the irony that one is not by automating necessarily removing the difficulties, and also the possibility that resolving them will require even greater technological ingenuity than does classic automation.'' Review is available here https://blog.acolyer.org/2020/01/08/ironies-of-automation/.},
  creationdate = {2020-01-08},
  journal      = {Automatica},
  keywords     = {Artificial Intelligence, Automation, employment},
  owner        = {ISargent},
  year         = {1983},
}

@Article{BauZSZTFT2018,
  author        = {David Bau and Jun{-}Yan Zhu and Hendrik Strobelt and Bolei Zhou and Joshua B. Tenenbaum and William T. Freeman and Antonio Torralba},
  title         = {{GAN} Dissection: Visualizing and Understanding Generative Adversarial Networks},
  eprint        = {1811.10597},
  url           = {http://arxiv.org/abs/1811.10597},
  volume        = {abs/1811.10597},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1811-10597},
  comment       = {Visualising what GANs have learned by first characterising the units and then measuring the causal relationship.

Characterising by dissection:
By upsampling and thresholding the feature maps for all units and find the concept, from the segmented output image, that has the highest spatial agreement with that thresholded feature map (using intersection over union). Each unit can be labelled with the concept with which it has the highest IoU.

Measuring causal relationships using intervention:
By ablating (removing) or inserting units, it is possible to measure how much a unit contributes to the presence of a concept in the generating image by comparing the presence and absence of the concept in the resulting image after ablation/insertion. Note that this can be applied to single units but that they have found that objects tend to depend on more than one unit.

Use method not only to determine how GANs produce realistic images, but also to find units that are responsible for failures such as artifacts (and thus ablate these units).

Code for this is avaiable https://github.com/CSAILVision/GANDissect (The code depends on python 3, Pytorch 4.1).

A summary has been created here https://blog.acolyer.org/2019/02/27/gan-dissection-visualizing-and-understanding-generative-adversarial-networks/},
  journal       = {CoRR},
  keywords      = {Visualiation, Discovery, MLStrat Discovery},
  owner         = {ISargent},
  creationdate     = {2018-11-30 12:44:28 +0100},
  year          = {2018},
}

@Article{MingCZLZSQ2017,
  author        = {Yao Ming and Shaozu Cao and Ruixiang Zhang and Zhen Li and Yuanzhe Chen and Yangqiu Song and Huamin Qu},
  title         = {Understanding Hidden Memories of Recurrent Neural Networks},
  journal       = {CoRR},
  year          = {2017},
  volume        = {abs/1710.10777},
  eprint        = {1710.10777},
  url           = {http://arxiv.org/abs/1710.10777},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1710-10777},
  comment       = {paper about visualising what a RNN has learned - mainly applies to text understanding.},
  owner         = {ISargent},
  creationdate     = {2018-08-13 16:49:08 +0200},
}

@Book{Tucker2019,
  author       = {Christopher Tucker},
  title        = {A Planet of 3 Billion},
  comment      = {See review in stanfords website http://www.stanfords.co.uk/productreviews/313246

A stunning validation of the importance of geography to humanity's survival

In my experience, debates about global climate and ecological issues, can get particularly tetchy on the topic of pop­ulation. Often, someone will state that the problem is quite simply one of global human overpopulation, only to be roundly shot-down by someone else pointing to the difference between the environmental impact of 'western' individuals and those in developing parts of the world - evidence that it is the culture's consumption and emissions that are to blame. Of course, the reality is somewhere between these two positions. However, entitling a book to directly advocate a specific global population goal seems a risky strategy; I can think of some ardent environ­mentalists who would dismiss this book without opening it, which is a shame, because this book offers some very striking insights into the depth and breadth of the problems that humanity has visited on planet Earth. In fact, the discussion of population size is addressed only in the latter parts of ``P3B''.

The early chapters of P3B deal, quite stunningly in my opinion, with the impact of humanity over time - starting in pre-history - and over space, discussing industrialisation and emissions (including noise). I thought I knew pretty all there was to know about the enormity of humanity's impact on out home planet, but Tucker tells the story so comprehensively and incisively that I found myself quite overwhelmed at times.

After some fascinating maps, the book goes on to discuss the different ways of defining ecoregions - so that we know what we have lost and still have, and what we must protect and restore - as well as considering connectivity between and through these regions - so that we can begin to evaluate the impacts on biogeography that natural and human commications facillitate. It is only after all these deep insights that P3B begins to discuss the knotty issue of how to determine Earth's carrying capacity. Thus, I believe that even those who disagree that population is part of our environmental challenge, would gain deep insights from P3B.

The value of 3 billion for Earth's carrying capacity is arrived at given the assumption that we would bring all of humanity to a moderate level of health and comfort. A ballpark having been established, P3B then rattles through some short, valuable chapters on how to reduce global populations, what economics, politics and global leadership could (must) look like under population de-growth. For those with a geographical leaning there is a call to take part in global data collection so that more detailed analysis is possible to understand humanity's impact over history and geography, and begin to create more detailed maps of how we move to living more sustainably on Earth. Finally, based on his estimate of the Earth's carry is capacity, Tucker prints letters sent to religious, political and corporate leaders asking for their involvement in the debate (I would love to read any replies).

An excellent book. The first half is an eloquent and paced journey, the second felt that it needed a little editing to match the delivery of the first. Must be read by anyone who thinks they knew everything about how our species has impacted the planet, and especially by geographers looking for validation of their subject.},
  creationdate = {2020-01-08},
  keywords     = {Climate, Environment, Social Science, Population},
  owner        = {ISargent},
  year         = {2019},
}

@TechReport{CCCEconometrics2019,
  author           = {Cambridge Econometrics},
  institution      = {Committee on Climate Change},
  title            = {A consistent set of socioeconomic dimensions for the CCRA3 Evidence Report research projects},
  comment          = {Description of choices made and data produced for future scenarios of the following for the UK:
- Population
- GDP
- GVA
- Employment
- Labour productivity
- Land use
- Expenditure on R\&D
- Energy generation by technology
- Average household size},
  creationdate     = {2020-01-23},
  keywords         = {Climate Change, Environment, Social Science},
  modificationdate = {2022-11-26T15:59:40},
  owner            = {ISargent},
  priority         = {prio1},
  year             = {2019},
}

@Book{AllenCAT2019,
  author       = {Paul Allen and Laura Blake and Peter Harper and Alice Hooker-Stroud and Philip James and Tobi Kellner and Tom Barker and Sonya Bedford and Anna Francis and Pete Cannell and Hazel Graham and Stephen Graham and Louise Halestrap and Tanya Hawkes and Richard Hawkins and Suzanne Jeffery and Jaise Kuriakose and Trystan Lea and Lucy Neal and Jonathan Neale and Eurgain Powell and Andrew Simms and Amanda Smith and Ruth Stevenson and Judith Thornton and Catriona Toms and Julia Wallond and Chloe Ward},
  title        = {Zero Carbon Britain: Rising to the Climate Emergency 2019 updates},
  comment      = {Summary
This report is an feat of excellent research, carbon calculations and crystal-clear writing. It is full of statistics and essential information and outlines the issue -  that we are in a climate and ecological emergency - and how we can address it in the UK. The report explains different ways of considering our carbon budget and chooses a compromise between ignoring all our past and 'exported' carbon and accounting for it all. It also recognises that we face many other issues such as biodiversity loss, species extinction and also our own wellbeing.  

Given that it seems impossible to achieve what is needed to prevent catastrophic climate scenarios if we use current economic and political approaches, the Zero Carbon Britain approach to bridging the ``physics-politics gap'' is to imagine the desired future and then identify how we get there. Some clear rules and assumptions are set out, including the use of only technology that is already available, that solutions are long-term and sustainable and that people would like to continue eating meat, albeit to a lesser degree.

The main area that the scenario covers are improving energy efficiency, increasing the use of renewables (but not nuclear), reducing emissions from buildings, industry, waste and agriculture, and carbon capture by changing land-use and creating bio-char. Interesting aspects of the scenario are the 'storage' of energy in times of overproduction (e.g. windy weather) such as by create to methane for use in existing gas-powered infrastructure.

The clear outcome of this scenario is not just a reduction in UK's CO2 emissions but also an improvement in other factors such as biodiversity, food security, job creation and physical (and probably mental) health. Certainly there is still even more detail that needs drilling down into, but this is truly the most practical solution to the climate and ecological emergency that I have yet encountered. My reaction is to roll up my sleeves and ask ``where do we start?'' and so fortunately the final chapter discusses how we need to act as a nation and as organisations to make this scenario a reality. 

AllwoodEtAl2019 say that ZCB has ``much more optimistic assumptions about the deployment rates of renewable generation technologies, especially very early stage technlogy for producing liquid fuels from biomass''

Carbon budgets
P25, P111 Meinshausen estimated that to have a 80\% chance of remaining below 2 degrees, we have less then 1.4GtCO2 to spend between 2000 and 2050. we have already spent around half of this. 
P26, P112 Current UK emissions, if assumed to be 'our share' give us less than 50\% chance of remaining below 2 degrees.
P27 Bridging the physics-politics gap
P42 Emissions from imported goods has increased by 28\% since 1997
P99 Would need a forest the size of 2 UKs to balance the GHGs currently being emitted
P134 2016 emissions, depending on framing. Emissions from international aviation and shipping, those related to consumption of imports and land-use change abroad for UK food consumption may be equivalent to emissions currently being accounted for.

Paleoclimate
P13 
GHG higher than in 800k years
Increasing 10x faster than the last deglaciation

Future climates
P16 4 degrees scenarios

Sea-level
P14 rising about 3cm/decade

Resources
P126 Embodied water in goods

Waste
P76 4\% of UK GHG emissions are from waste management. 
P77 The majority from of waster emissions are from landfill although this has massively decreased due to CH4 capture and diversion of waste from landfill.

Agriculture
P36 Around half of cropland for livestock feed.
p37 Around half of land area of UK for livestock. 
P83 More analysis is needed to identify which farming and production parctises (in the ZCB scenario) are most appropriate to regions of UK.
P84 Details of GHG emissions from agriculture.

Land management
P36 1\% C emissions due to clearing land for urbanisation
P104 Losses from peatland are probably higher than we previously thought but peatland restoration is a big opportunity to capture more than 4 MtCO2 a year.
P104 Biochar is stable for 1000 years.

Aviation
P31 P48 Aviation is about 23\% of transport energy use or 7\% of UK GHG emissions (in 2017). However, because emissions high in atmosphere have greater impact on radiative forcing we have to add an effective GHG factor of 1.6. (Lee 2010 in https://www.icao.int/environmental-protection/Documents/EnvironmentReport-2010/ICAO_EnvReport10-Ch1_en.pdf)

Wellbeing
P23 our reported happiness has very little (at most 10%) to do with our material or energy accumulation (above a crucial subsistence threshold)
P23 increasing inequality 3x increase in share of income going to richest 1\% households in 4 decades. Bottom 3rd received almost nothing.
P85 We eat too much food in the wrong balance resulting in obesity, heart and circulatory problems, strokes, type 2 diabetes
P89-92 Detail of new diet under ZCB scenario
P128 NEF Happy Planet Index

Economics
P118 ZCB scenario is affordable. Simon Wren-Lewis: ``no one in a 100 years' time who suffers the catastrophe and irreversible impact of climate change is going to console themselves that at least they did not increase the national debt. Humanity will not come to an end of we double debt to GDP ratios, but it could if we fail to combat climate change''
P119 Green New Deal

Multi-solving
P119 War-time effort
P120 we need
	1. National, cross sector, CEE action plan
	2. Interdisciplinary skills
	3. Budgets not siloed
	4. Link up with different levels of authority
	5. Long term thinking
P122 Adaptation needs to be green and blue, not grey
P128 There will be a lot of jobs created
P143 ``policy must demand that all institutions immediately withdraw their support from the fossil fuel industry - be that investments, sponsorships, subsidies or permits''
P146-8 How to develop your group's action plan. Delivery upwards, downwards, sideways and inwards.
P151 To create a positive future, we must imagine it},
  creationdate = {2020-01-25},
  keywords     = {Climate Change, Net Zero, Carbon, Mitigation, Environment},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2019},
}

@Manual{A4Q_AITesting2019,
  author       = {{Alliance for Qualification}},
  title        = {AI and Software testing Foundaton Syllabus},
  version      = {1.0},
  comment      = {Syllabus covering ``what is AI'' and details about how it could/should be tested. Training to create ``AI and Software Testing Foundation certified testers''. Resource for engineering departments with potentially little experience of AI.  Should result in:
:ability to perform test analysis, test design, test implementation, test execution and test completion activities for a system that intergrates one (or more) AI-based components
ability to support and evaluate AI testing activites and approaches within an organisation
Three main sections:
1 - Key aspects of AI
2 - Testing AI systems
3 - Using AI support testing
Usefrom from section 2 onwards. Well worth returning to periodically to consider how we are doing in terms of our code and approaches.},
  creationdate = {2020-01-30},
  keywords     = {Artificial Intelligence, Software testing, Software Sustainabiility, MLStrat Metrics},
  owner        = {ISargent},
  year         = {2019},
}

@TechReport{MorenoSF2016,
  author           = {Camila Moreno and Daniel Speich Chassé and Lili Fuhr},
  date             = {2016},
  institution      = {Heinrich Böll Foundation},
  title            = {Carbon Metrics - Global abstractions and ecological epistemicide},
  url              = {https://www.boell.de/en/2015/11/09/carbon-metrics},
  comment          = {Carbon must not become the new GDP. Metrics are part of colonialism and capitalism. Great overview of history of GDP and problems with GDP as a measure of economy because it is a model that doesn't fit everywhere and it makes many things invisible, oversimplified and overcertain.
''A history of environmental policy as the history of forgotten alternatives has not yet been written.''

''What are the implications, if «carbon» becomes the accounting unit of society? What are the implications for dealing with the crisis of nature? Does it foster or hinder a turnaround in policy and mentality?''

''GDP hogs the limelight like an all-powerful autocrat, bathing the money economy in its glare and consigning the non-economic values to darkness.''
''Is there a risk of a similar trajectory - from innovation via custom to frustration - if «carbon» is made the negative measure of prosperity for all societies?''

''Is climate change more important and more urgent than the loss of biodiversity, the degradation of arable soils, or the depletion of fresh water? Can any of these phenomena even be considered in isolation from each other?''

''the way we describe and frame a problem very much predetermines the kinds of solutions and answers that we can consider''

''It is important to keep in mind that the products of just ninety private companies, state-owned enterprises and government-run industries (including the biggest producers of coal, oil, gas and cement) are responsible for two thirds of global emissions to the atmosphere since the beginning of industrialization (www.carbonmajors.org)''

''green growth strategies try to take a short cut to solving the environmental crises by relying on one single measurable unit. Carbon metrics are a scale for environmental injustice; they are thought to offer a universal lens to see the world and the problems we face (as we live in a CO2 society) and anchor a consistent indicator for environmental degradation; and they are thought to offer a policy tool to change the world.''

''The assumed objectivity of counting environmental and economic global issues (be it carbon or GDP) can not be detached from the risk of over-simplifying complex issues, thus making opaque - or even invisible - major issues of power.''

Likens carbon metrics to calorie counting as well as GDP.

''global abstractions need to be assessed in epistemological (i.e. cognitive) and political terms as well as with regard to their respective historical contexts.''

''The apprehension of reality in calculable units lies at the core of the way in which we think today. It is the reason why we can frame - and reduce - key political issues of our times in terms of a calculation of costs and benefits, and, for example, talk about climate change in terms of the «price of inaction» (and the opportunities and profits of action), or translate a major ecological crisis into the management of carbon units.''

''Each gas has distinct values according to its Global Warming Potential (GWP) or Global Temperature change Potential (GTP)''

''common metric such as the «CO2 equivalent» allows us to put emissions of all greenhouses gases on a commensurable scale. Ideally, the same equivalent CO2 emissions would produce the same climate effect, regardless of which gases contribute to that equivalent CO2 and irrespective of the geo-social circumstances of its emission.''

*''Undoubtedly, the environmental crisis we face is real and deeply serious. But it is also multidimensional and highly complex in the way it influences the interdependent interactions that constitute the delicate and intricate web of planetary life.''

''Is it assumed that all these multidimensional aspects of the man-made environmental challenge facing us not only correlate, but can also be tackled and solved by simply addressing the concentration of carbon dioxide (CO2) in parts per million in the atmosphere?''

''The Pope clearly voiced opposition to «the buying and selling of carbon credits» because in his view it leads to a «new form of speculation which will not help reduce the emission of polluting gases worldwide». The Pope has been criticized for making this point, because carbon trade stands as the economist's favored path to change''

''While we see carbon as a new metric making its way into all dimensions of social life, we observe the emergence of a new commodity in the form of «carbon rights».'' - have the same impact on world history as ``the enclosure of formally communual land into private holding of land titles''

''when Europe was in disarray due to the devastation caused by World War Two....the reconstruction of whole national economies captured global political imagination in a way that is comparable to today's focus on carbon.''
''Counting carbon simplifies this challenge and gives politicians the illusion that they can do something against environmental degradation''

''The immediate phase post-1945 produced [four] innovations .... 
1. novel institutions of global politics such as the United Nations founded in San Francisco in 1945 ...
2. Before the rise of international organizations, global political debate was the realm of smart diplomats who were trained in diplomatic protocol, in legal studies and international law. After World War Two, technical experts, for example in agriculture, health or education, started to accompany the national diplomatic delegations at international meetings ...
3. turning all political questions into economic issues [e.g. Keynes] ... «economic imperialism» ... is conquering all neighboring disciplines and currently also colonizes all concerns about the environment ...
4. post-1945 ... the shift of political issues into a quantitative mode.''
''The measurement of carbon emissions is being objectified since the 1992 Rio Conference''

Excellent history of GDP (page 29)
''During the Early Modern Epoch the secular European intellectual elites began to separate their immediate sensual experience of the natural environment from an analytical approach to nature. This move has been termed a «great bifurcation» that became the foundation of the modern sciences. Its core was the laboratory.''

''So there are always two trajectories involved in modern science: one is the isolation part, which means the shutting of the laboratory's doors, the reduction of complexity and the creation of invisibility. The other part is the innovation that brings new insights to the fore.''

''Social scientists took some time to catch up with this hard epistemic model''

''The widely held assumption that numbers are the «hard facts» of the real world needs to be refuted.''

Economic models don't work everywhere, e.g. Phyllis Dean found that areas were largely dominated by subsistence production and barter trade. And in other areas it is difficult to separate production and consumption.

''the «valuation» of environmental system services might lead to a more sustainable global economy. But it will also prolong capitalist exploitation by allowing those in power to accumulate newly established «carbon rights» and control over ecosystems in the Global South''

''«technical» recommendations about the economic benefits of investment and adoption of low-carbon technologies, such as those attributed to biofuels, served as a key trigger for the land grabbing boom that followed the release and impact of the report, with consequences on land price speculation, evictions, expansion of monocultures, .hunger, etc''

''The failure to recognize the different ways of knowing by which people across the globe run their lives and provide meaning to their existence is termed «cognitive injustice», or «epistemicide» (Boaventura de Souza Santos).''..global social justice is not possible without global cognitive justice''

''How can we debunk the myth that we can have «zero-net emissions» accountancy (an urgent matter in light of the Paris Agreement.)?''
''How can we dismantle the growth mantra? How can we regulate those industries that destroy and pollute and how can we prevent their lobbyists from simply buying political decisions in their favor?''

''We need to challenge our mental infrastructures''},
  creationdate     = {2020-02-19},
  editor           = {The Heinrich Böll Foundation},
  keywords         = {Carbon Metrics,Social Science,Environment,economics,policy,thesis},
  modificationdate = {2024-08-26T19:59:22},
  owner            = {ISargent},
  priority         = {prio1},
  series           = {Publication Series Ecology},
  volume           = {42},
  year             = {2016},
}

@Article{ClarksonEtAl2016,
  author       = {Clarkson, M.O. and Kasemann, S.A. and Wood, R. and Lenton, T.M. and Daines, S.J. and Richoz, S. and Ohnemueller, F. and Meixner, A. and Poulton, S.W. and Tipper, E.T.},
  title        = {Ocean Acidification and the Permo-Triassic Mass Extinction},
  url          = {http://hdl.handle.net/10871/20741},
  comment      = {''During the second extinction pulse, however, a rapid and large injection of carbon caused an abrupt acidification event that drove the preferential loss of heavily calcified marine biota.''
''The Permian Triassic Boundary (PTB) mass extinction, at ~ 252 million years ago (Ma), represents the most catastrophic loss of biodiversity in geological history, and played a major role in dictating the subsequent evolution of modern ecosystems (1).''
''The first occurred in the latest Permian [Extinction Pulse 1 ( EP1)] and was followed by an interval of temporary recovery before the second pulse (EP2), which occurred in the earliest Triassic''
''Models of PTB ocean acidification suggest that a massive and rapid release of CO2 from Siberian Trap volcanism acidified the ocean''
''The carbon release required to drive the observed acidification event must have occurred at a rate comparable with the current anthropogenic perturbation, but exceeds it in expected magnitude''},
  creationdate = {2020-02-19},
  journal      = {Science},
  keywords     = {Extinction, environment, Ocean Acidification},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2016},
}

@Article{RolnickEtAl2022,
  author           = {Rolnick, David and Donti, Priya L. and Kaack, Lynn H. and Kochanski, Kelly and Lacoste, Alexandre and Sankaran, Kris and Ross, Andrew Slavin and Milojevic-Dupont, Nikola and Jaques, Natasha and Waldman-Brown, Anna and Luccioni, Alexandra Sasha and Maharaj, Tegan and Sherwin, Evan D. and Mukkavilli, S. Karthik and Kording, Konrad P. and Gomes, Carla P. and Ng, Andrew Y. and Hassabis, Demis and Platt, John C. and Creutzig, Felix and Chayes, Jennifer and Bengio, Yoshua},
  date             = {2022},
  journaltitle     = {ACM Computing Surveys},
  title            = {Tackling Climate Change with Machine Learning},
  doi              = {10.1145/3485128},
  issn             = {0360-0300},
  number           = {2},
  url              = {https://dl.acm.org/doi/full/10.1145/3485128},
  volume           = {55},
  abstract         = {Climate change is one of the greatest challenges facing humanity, and we, as machine learning (ML) experts, may wonder how we can help. Here we describe how ML can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by ML, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the ML community to join the global effort against climate change.},
  address          = {New York, NY, USA},
  articleno        = {42},
  comment          = {Authors include Yoshua Bengio.
I read this version: http://arxiv.org/abs/1906.05433

Detailed yet simplistic paper giving areas that Ml may be applied to climate change. Some examples are not necessarily problems for ML. However, it is an excellent resource for easy overview of the issues and araes needing solutions and it recognises the need for collaboration across disciplines. Divides the space into:

1 Electricity systems
Enabling low-carbon electricity      
Reducing current-system impacts    
Ensuring global impact   
2 Transportation
Reducing transport activity    
Improving vehicle efficiency  
Alternative fuels \& electrification  
Modal shift    
3 Buildings and cities
Optimizing buildings    
Urban planning    
The future of cities    
4 Industry
Optimizing supply chains   
Improving materials 
Production \& energy   
5 Farms \& forests
Remote sensing of emissions 
Precision agriculture   
Monitoring peatlands 
Managing forests   
6 Carbon dioxide removal
Direct air capture 
Sequestering CO2   
7 Climate prediction
Uniting data, ML \& climate science    
Forecasting extreme events    
8 Societal impacts
Ecology  
Infrastructure   
Social systems   
Crisis  
9 Solar geoengineering
Understanding \& improving aerosols  
Engineering a planetary control system  
Modeling impacts  
10 Individual action
Understanding personal footprint    
Facilitating behavior change  
11 Collective decisions
Modeling social interactions  
Informing policy     
Designing markets   
12 Education  
13 Finance},
  creationdate     = {2020-02-19},
  issue_date       = {February 2023},
  keywords         = {Environment, Climate Change, Machine Learning},
  modificationdate = {2022-12-11T17:44:36},
  month            = {2},
  numpages         = {96},
  owner            = {ISargent},
  publisher        = {Association for Computing Machinery},
  year             = {2022},
}

@InProceedings{AmershiEtAl2019,
  author           = {Saleema Amershi and Andrew Begel and Christian Bird and Robert DeLine and Harald Gall and Ece Kamar and Nachiappan Nagappan and Besmira Nushi and Thomas Zimmermann},
  booktitle        = {International Conference on Software Engineering (ICSE 2019) - Software Engineering in Practice track},
  title            = {Software Engineering for Machine Learning: A Case Study},
  url              = {https://www.microsoft.com/en-us/research/publication/software-engineering-for-machine-learning-a-case-study/},
  comment          = {Survey of AI teams at Microsoft to identify common approaches and issues. ``identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components - models may be “entangled” in complex ways and experience non-monotonic error behavior.''
''We have made a first attempt to create a process maturity metric to help teams identify how far they have come on their journeys to building AI applications''
AI teams are sometimes staffed by ``polymath data scientists, who <<do it all>> [others] domain experts who deeply understand the business problems,
modelers who develop predictive models, and platform builders who create the cloud-based infrastructure''
''machine learning workflows are highly non-linear and contain several feedback loops''
''frameworks and environments to support the ML workflow and its experimental nature'' but ``engineers still struggle to operationalize and standardize working processes''
''variation in the inherent uncertainty (and error) of data-driven learning algorithms and complex component entanglement caused by hidden feedback loops could impose substantial changes (even in specific stages) which were previously well understood in software engineering (e.g., specification, testing, debugging, to name a few).''
''It is important to develop a “rock solid, data pipeline, capable of continuously loading and massaging data, enabling engineers to try out many permutations of AI algorithms with different hyper-parameters without hassle.”''
''rigorous and agile techniques to evaluate ... experiments ... multiple metrics ... automating tests ... human remains in the loop ... not only to automate the training and deployment pipeline, but also to integrate model building with the rest of the software''
''Data Availability, Collection, Cleaning, and Management, is ranked as the top challenge by many respondents''
''maturity model with six dimensions evaluating whether each workflow stage: 
(1) has defined goals, 
(2) is consistently implemented, 
(3) documented, 
(4) automated, 
(5) measured and tracked, and 
(6) continuously improved.''
''While there are very well-designed technologies to version code, the same is not true for data''
''Conway's Law, ... makes the observation that the teams that build each component of the software organize themselves similarly to its architecture''},
  creationdate     = {2020-02-19},
  keywords         = {Artificial Intelligence, Sustainable Software, MLStrat Programme, metrics, trust},
  modificationdate = {2023-12-11T08:03:36},
  owner            = {ISargent},
  year             = {2019},
}

@Article{SuttonATM2010,
  author       = {Paul C. Sutton and Sharolyn J. Anderson and Benjamin T. Tuttle and Lauren Morse},
  title        = {The real wealth of nations: Mapping and monetizing the human ecological footprint},
  pages        = {11--22},
  volume       = {16},
  comment      = {Presents a method for the monetization of natural capital and human impact on supporting ecosystems. It is a step towards a much needed `ecological accounting' at the global scale.

The goal of this research is to present a new method for measuring anthropogenic environmental impact, which we monetize as an environmental cost. This is achieved using existing global maps of Net Primary Production (NPP) and Impervious Surface Area (ISA).

Net Primary Production (NPP) is “the net amount of solar energy con-verted to plant organic matter through photosynthesis—it can be measured in units of elemental carbon and represents the primary food energy source for the world's ecosystems” (Imhoff et al., 2004).

This ecological accounting data set was found not to correlate highly with poverty. However, this may be (in whole or in part) due to the fact that both areas of high NPP and areas of low NPP correspond with areas of relatively high poverty.

Contains table of ecological Balance statistics for 200 nations of the world in the year 2000.},
  creationdate = {2020-03-26},
  journal      = {Ecological Indicators},
  keywords     = {Environment},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2012},
}

@TechReport{CCC2019ProgressReport,
  author      = {John Gummer and Julia King and Keith Bell and Nick Chater and Piers Foster and Rebecca Heaton and Paul Johnson and Le Qu\`{e}r\`{e}, Corinne},
  institution = {Committee on Climate Change},
  title       = {Reducing UK Emissions: 2019 Progress Report to Parliament},
  comment     = {Great review of UK's progress and lack of it on reducing emissions in sectors:
Surface transport
Aviation and shipping
Industry
CCS
Hydrogen
Buildings
Power
Agriculture and land use
Waste 
F-gases
Public engagement
Some excellent statistics and charts. Summary is that whilst it's good that Net0 by 2050 is enshrined in law we are not currently on track to meet this, except, maybe in the energy sector.},
  keywords    = {Environment, Climate},
  owner       = {ISargent},
  priority    = {prio1},
  creationdate   = {2020-03-26},
  year        = {2019},
}

@Article{LevyVBT2018,
  author       = {Peter Levy and van Oije, Marcel and Gwen Buys and Sam Tomlinson},
  title        = {Estimation of gross land-use change and its uncertainty using a Bayesian data assimilation approach},
  pages        = {1497--1513},
  url          = {https://www.biogeosciences.net/15/1497/2018/bg-15-1497-2018.pdf},
  volume       = {15.5},
  comment      = {use data from multiple sources for Scotland including Countryside Survey, CEH Land Cover Map, Corine and several others to produce a cube of land use over space and time (U) as well as an array of areas of each land use at each time (A) step and an array of changes between land uses at each time step (B). Describe how they bring the different data sources together to create these land use data structures. Use MCMC to determine the posterior probabilities of change between land uses (I think, a bit confusing). ``An attractive feature of the Bayesian data assimilation approach is that additional data sources can be added to the process as they become available, without any major changes to software or step-changes in results. Several other data sources exist in the UK which could be incorporated. These include spatial data on the granting of woodland felling licenses, which would further constrain the likely location of deforestation, and national mapping agency data on urban expansion. As new satellite instruments come on-stream (e.g. from Sentinel and synthetic aperture radar), further remotely sensed data products will become available which could be added into the estimation of A, B, and U.''},
  creationdate = {2020-04-06},
  journal      = {Biogeosciences},
  owner        = {ISargent},
  year         = {2018},
}

@Report{EUSDG2019,
  author       = {{Sustainable Development Solutions Network / Institute for European Environmental Policy}},
  institution  = {European Union},
  title        = {2019 Europe Sustainable Development Report: Towards a strategy for achieving the Sustainable Development Goals in the European Union. Includes the SDG Index and Dashboards for the European Union and member states.},
  type         = {techreport},
  comment      = {Part 1. The EU's performance against the SDGs 1
1.1 The SDG Index and Dashboards 2
1.2 Leave no one behind 7
1.3 Convergence across EU member states 10
1.4 International spillovers 12
Part 2. Six SDG Transformations 15
2.1 An operational framework for achieving the SDGs 15
2.2 Applying the SDG Transformations for the EU 17
2.3 Long term pathways and stakeholder engagement for SDG
Transformations 23
Part 3. Implementing the SDGs in the EU 27
3.1 Internal priorities for the EU and member states 28
3.2 EU Diplomacy and Development Cooperation for the SDGs 32
3.3 Tackling international SDG spillovers 35
3.4 Getting it done: Strategy, budgets, monitoring, and
member state engagement

Some excellent diagrams for comparing current status of countries in various aspects to goals and to each other.},
  creationdate = {2020-04-14},
  keywords     = {Environment},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2019},
}

@InProceedings{RobinsonHMSCDJ2019,
  author       = {Caleb Robinson and Le Hou and Kolya Malkin and Rachel Soobitsky and Jacob Czawlytko and Bistra Dilkina and Nebojsa Jojic},
  booktitle    = {CVPR},
  title        = {Large Scale High-Resolution Land Cover Mapping with Multi-Resolution Data},
  url          = {http://openaccess.thecvf.com/content_CVPR_2019/papers/Robinson_Large_Scale_High-Resolution_Land_Cover_Mapping_With_Multi-Resolution_Data_CVPR_2019_paper.pdf},
  comment      = {Microsoft paper on land cover mapping across very large regions. Full of useful ideas. 
Scale and cost of existing data: Useful list of existing EO datasets for landcover and commentary on each.
Map generalisation: High-resolution land cover labels at 1m resolution only exist at concentrated locations develop methods for generalizing models to new regions, achieving high-quality results in the entire US. Specifically, we augment high-resolution imagery with low-resolution (30m) satellite images 
Evaluation: We run our best model, a U-Net variant, over the entire contiguous US to produce a countrywide high-resolution land cover map. This computation took one week on a cluster of 40 K80 GPUs, at a cost of about $5000, representing massive time and cost savings over the existing methods used to produce land cover maps. We provide a web tool through which users may interact with the pre-computed results - see http://aka.ms/cvprlandcover -
exposing over 25TB of land cover data to collaborators.
Issue is that test data in land cover mapping problems is not drawn from the same sample distribution as that the training data conditions on the ground, atmosphere, sensor, etc are different. This paper proposes the following techniques to overcome this:
Low-Resolution Input Augmentation: We find that augmenting high-resolution imagery with low-resolution imagery that has been averaged over large time horizons improves model performance.
Label Overloading: Because labels are often a snapshot in time, and the true land cover of a location is not likely to change over short time scales, we augment our training dataset
by pairing high-resolution training labels with high-resolution image inputs from different points in time.
Input Color Augmentation: Color is important for land cover classification but can vary greatly across datasets. To address this, given a training image, we randomly adjust the brightness and contrast per channel by up to 5%.
Super-Resolution Loss: We augment the training set with additional low-resolution labels from outside of the spatial extent in which we have high-resolution training data to better inform the model.

To improve the generalization ability of supervised models in an unsupervised fashion, domain adaptation methods [8, 18, 29, 32, 27] learn to map inputs from different domains into a unified space, such that the classification/segmentation network is able to generalize better across domains. We use an existing domain-adversarial training method [8] for the land cover mapping task (Adapt). In particular, we attach a 3-layer domain classification sub-network to our proposed U-Net architecture. This subnetwork takes the output of the final up-sampling layer in our U-Net model and classifies the source state (New York, Maryland, etc.) of the input image as its “domain”.

Compare FC-DenseNet, UNet, and U-Net Large to a baseline of random forest. 
The U-Net Large model only performs slightly better than the U-Net model and the FC-DenseNet does similarly well. Generalisation to new regions is very good.},
  creationdate = {2020-04-24},
  keywords     = {Land cover, remote sensing, MLStrat DLRS},
  owner        = {ISargent},
  year         = {2019},
}

@Article{Pauly1995,
  date         = {1995/10/1},
  title        = {Anecdotes and the shifting baseline syndrome of fisheries},
  issue        = {10},
  volume       = {10},
  abstract     = {Fisheries science has responded as well as it could to the challenge this poses by developing methods for estimating targets for management-earlier the fabled Maximum Sustainable Yield (MSY) l, now annual total allowable catch (TAC) or individual transferable quotas (ITQ). If these methods are to remain effective, fisheries scientists need to follow closely the behavior of fishers and fleets, but this has tended increasingly to separate us from the biologists studying marine or freshwater organisms and/or communities, and to factor out ecological and evolutionary considerations from our models. There are obviously exceptions to this, but 1 believe the rule generally applies, and it can be illustrated by our lack of an explicit model accounting for what may be called the `shifting baseline syndrome'. Essentially, this syndrome has arisen because each generation of fisheries scientists accepts as a baseline the stock size …},
  authors      = {Daniel Pauly},
  comment      = {The source of the concept of shifting baseline syndrome - the problem that we only observe the descline in species/ecology in our own lifetimes and are unaware of the decline that has happened during the lifetimes of previous generations and therefore don't have a sense of how big the decline is.},
  creationdate = {2020-04-24},
  journal      = {Trends in Ecology \& Evolution},
  keywords     = {Ecology, Psychology, Environment},
  owner        = {ISargent},
  year         = {1995},
}

@TechReport{SharpY2019,
  author    = {Elspeth Sharp and Mattie Yeta},
  title     = {Helping businesses create a greener, more sustainable future through ICT: An industry guide by Defra in collaboration with our ICT (information and communication technology) industry suppliers \& partners},
  url       = {https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/840765/defra-industry-guide-ict-sustainability.pdf|},
  comment   = {Guide to making ICT within organisations more sustainable. Describes and recommends action on the following topics:
Circular Economy
Sustainable ICT procurement
Ecological footprint
Meeting and managing ISO standards
Learning and development
Seems to be lacking an inroduction and conclusion for those not involved in the creation of the doc - but a useful resource for making IT more sustainable. I believe it's due for an update.},
  keywords  = {Carbon, Greening Business, Environment},
  month     = {10},
  owner     = {ISargent},
  priority  = {prio1},
  creationdate = {2020-04-29},
  year      = {2019},
}

@Article{GosselinS2003,
  author       = {Frédéric Gosselin and Philippe G. Schyns},
  title        = {Superstitious Perceptions Reveal Properties of Internal Representations},
  url          = {https://journals.sagepub.com/doi/abs/10.1111/1467-9280.03452},
  comment      = {''Wiener (1958) showed that noise could be used to analyze the behavior of a black box, even suggesting that the brain could be studied this way''
''We started from an unstructured external stimulus (white noise), and we led observers to believe that the stimulus comprised a signal. As white noise does not represent coherent structures in the image plane, the superstitious perception of a signal had to arise from the observers' share. To characterize these internal representations, we reverse correlated the observers' detection and rejection responses with the corresponding white-noise stimuli.''

First experiment: Told 3 paid naive observers that the letter S exists in about 50\% of 20,000 trials. Observers had to say when they see the 'S' (there is not signal in any of the images).

''We computed a “yes image” (vs. “no image”) by adding together all the stimuli leading to detections (vs. rejections). We then subtracted the no image from the yes image to produce a classification image''

Second experiment: Told 2 observers that there is a smile in images - each of which had the top half of a face in the top half but just noise in the bottom

''we randomly sampled 27.5\% of the black pixels of the contours of a face without a mouth ... and filled the remainder of the image with bit noise with the same density of black pixels. No signal was therefore presented in the mouth area.''

In both cases, the resulting 'signal' is not very strong vidually but they are able to correlate it with one of a range of examples (different fonts or different smile images). 

Previous work has looked at the specific receptive fields of neurons but then are low level representations - not at the object level. Others, similar to this, have looked at higher level representations but they include some element of signal and so are biased towards the signal. ``we went back to Wiener's (1958) original idea and used only white noise to depict the observer's share.''},
  creationdate = {2020-05-05},
  journal      = {Psychological Science},
  keywords     = {Vision, Cognitive},
  owner        = {ISargent},
  year         = {2003},
}

@TechReport{ESTNetZero2020,
  author           = {Stuart McKinnon and Scott Milne and Adam Thirkill and Paul Guest and Danial Sturge},
  date             = {2020-03-10},
  institution      = {Energy Systems Catapult},
  title            = {Innovating to Net Zero},
  comment          = {Follow up to ESC's ``Living Carbon Free'' (some good stats here too: https://es.catapult.org.uk/reports/net-zero-a-consumer-perspective/). The Committee on Climate Change have set out 3 levels of action that need taking: Core actions, Further Ambition and Speculative. A central message of CCC is that we need to double electricity generation and increase Carbon Capture and Storage and hydrogen-powered tech.

''in how we make, move, store and use energy ... this will require innovation - in technology of course, but equally in consumer propositions, market design, business models and system definition and design. ... deliver clean economic growth.''

Uses their ESME - Energy System Modelling Environment - model to determine what combination of technology innovation and societal behaviour changes are needed to get to Net Zero. Follows territorial emissions, but recognises in the introduction that this ``ignores a significant proportion of the emissions that result from what we consume in the UK''. They model 4 scenarios:
- FA96 which does not require speculative measures (see below) and acheives 96 percent carbon reduction
- TECH100 which is technology-based to achieve net zero - this is the foundation for their ``clockwork'' scenario
- SOC100 which is the society-based approach to achieving net zero - this is the foundation for their ``patchwork'' scenario
- BOB100 uses the best of TECH100 and SOC100
MAX where aviation demand and livestock emissions fall to zero by 2050, other parts of the system are decarbonised at highly accerated timescale - ``pushes against the bounds of plausability''

In the Clockwork, government drives long-term investment in energy infrastructure (e.g. nuclear) and their is a steady deployment of greenhouse gas infrastructure meaning that people don't notice a difference in their living standards (and can heat their houses to 21 degrees). They give a lot of detail about what this scenario looks like.

In the Patchwork scenario, central government does not take such a leading role but factors such as offshore wind, woodland planting and public engagement allow us to meet the goal. My impression is that this is a lucky scenario. 

Largely dismiss the possiblity of non-linear reductions that are tested in the MAX scenario.

''Meeting the UK's Net Zero target will require unprecedented innovation across the economy. Innovation not just in new technologies, but in new ways of deploying existing technologies, new business models, new consumer offerings, and, crucially, new policy, regulation and market design.'' 

Recognises that ``serious societal engagement is essential'' but assumes that aviation will continue to increase (by 20 percent in their speculative model)  and there will be a small to medium reduction in lifestock farming (down by 50 percent in their speculative model). This is based on EST's ``early public engagement'' which looked at willinghess of the public to significatly adapt their lifestyle. It found, for isntance, that and customers will demand technologies that are as good as or better than e.g. their current heating systems. Thus that CCS and bioenergy are essential AND finds that 99 percent carbon capture rates may be necessary.

The ESC portfolio includes deep dives into the role of floating offshore wind, advanced nuclear technologies, storage and flexibility solutions, hydrogen in buildings and digitalisation of energy systems to help accelerate progress and decrease the cost. Use a whole systems approach which has 3 broad areas:
- Energy to the customer: generation, transmission, distribution, buildings, consumer
- Breaking down silos: Electricity, heat, transport
- Joining all aspects: Physical system, Digital system, Market System, Policy

To achieve net zero, the final 4 percent of carbon can only be removed with 
- only 20\% growth in aviation
- 50\% decline in meat/dairy consumption
- 20 kha./yr increase in forestry
- 25TWh/yr increase in UK biocrops
- 99\% CCS capture rate
- 25MT/yr Direct air carbon capture and storage (DACCS)

3 requirements: switch to low carbon tech, tackle demand for hard to treat activities (aviation and livestock), ensure sufficient carbon sequestration to offset residual emissions.

Innovation is needed in electricity generation, hydrogen and district heat networks. Support sustainable lifestyles such as real-time data on consumption and activity planning. Also innovation in potential for public campaigns and undestand what messages works in such campaigns. 

The entire UK building stock needs to move to low carbon heating by the late 2040s - and of 25 million existing homes that will still be in use by 2050, 18 million will need to undergo whole house retrofitting. Natural gas cannot be part of this mix. 1 million installations per year.

''For avation there are no likely alternatives to kerosene-fuelled aeroplanes by 2050''. ``Vehicles must be all electric (or hydrogen) from the mid-2030s onwards''. ``effectively-managed intergration of [electric cars] can improve electricity network efficiency and system resilience, whicle limiting the need for new infrastructure to meet growing electricity demand''

''We have not investigated the potential impact of [flight-shaming]''. Other analysis and commentry by other groups achieve additional ambition through deep demand reduction. Tested this is the MAX scenario.

''Global estimates of damages from failure to reduce emission are many times greater than the cost of mitigation'' and ``there are many co-benefits of decarbonisation ...  health and biodiversity''. ``Importance of stable and credible policies that successfully reduce the cost of capital demanded by private sector investors for a broad range of clean capital intensive technologies'' ``...technolog, policy, regulation and finance mechanisms, and supply chains and workforce skills built up'' ``economy-wide carbon policy framework for Net Zero''. 

This is the paper with the briliant graph showing the effective carbon prices and emission in the UK - with aviation being the cheapest way of emitting carbon and rail being the most expensive. Also show the size of the emission per sector. Recommnds that carbon policy includes emissions trading, sectoral decarbonisation, carbon credits. Specific policy foci for electricity (''large-scale developments'' groan), transport, buildings, industry and digitalisation.

This model seems to be a pretty powerful approach to understanding the options for mix of emissions reduction and draw-down but is used with a bias towards technology (this is a catapult, after all). I am surprised to find nothing, or very little, on energy efficiency and also no move away from road (assumption that electric vehicles are a substitute). Also seem to have no idea that peat is better than forest for carbon draw down. Also does not consider the society impact of certain policies such as increasing inequality and poverty. Also doesn't consider the impact of developments, especially large-scale developments, on local regions and emissions themselves. In particular, they account for lifecycle emissions associated with land-use change for biomass but do not mention this for other developments such as electric vehicles, nuclear, etc).},
  creationdate     = {2020-05-05},
  keywords         = {Climate Change, Net Zero, Carbon, Mitigation, Environment},
  modificationdate = {2022-10-16T15:04:14},
  owner            = {ISargent},
  priority         = {prio1},
  year             = {2020},
}

@TechReport{AllwoodEtAl2019,
  author       = {Allwood, J.M. and Dunant, C.F. and Lupton, R.C. and Cleaver, C.J. and Serrenho, A.C.H. and Azevedo, J.M.C. and Horton, P.M. and Clare, C. and Low, H. and Horrocks, I. and Murray, J. and Lin, J. and Cullen, J.M. and Ward, M. and Salamati, M. and Felin, T. and Ibell, T. and Zhou, W. and Hawkins, W},
  date         = {29/11/2019},
  institution  = {University of Cambridge},
  title        = {Absolute Zero: Delivering the UK's climate change commitment with incremental changes to today's technologies},
  doi          = {10.17863/CAM.46075},
  comment      = {Report taking as a starting point reaching absolute zero carbon emissions by 2050 and considering what needs to be done to achieve this in UK using today's technology. Thus, similar to CAT's Zero Carbon Britain, although the emphasis is on absolute, not net, zero because there are no scalable carbon capture technologies. Much of the emphasis of transformation is on the manufacturing and construction industries. In summary, by 2050, we will have more renewable energy but not enough for all current processes so will need to make savings. Divides technology into ``today's'', ``incremental'' (e.g. smaller cars) and ``breakthrough'' (e.g. hydrogen cars). [Are these like the CCC's Core actions, Further Amition and Speculative?] There is no current technology to continue current aviation, shipping and cement production and these will have to cease by 2050 - with a view to growing aviation and shipping as new clean technology comes on stream. Also, sheep and cattle products must be phased out by 2050. Some excellent resources such as a page of key messages for industrial sectors and one for individuals, as well as some useful schematics showing change required over next decades. I was disappointed that there is not more emphasis on policy and legislation - and felt the ``Transitions'' section - which gave a detailed overview of the prisoners dilemma aspect of the problem - assumed that far more was possible by voluntary change than I think evidence suggests (given the timescales we need to meet). A ``roadmap'' is the main solution given in the Transitions in Business section. I like that the report calls for this to generate public discussion of changes required and lots of collaboration ``perhaps only seen during times of war''.

Government policy, lobbying \& this report
P4 P14 two ``escape'' words - ``net'' and ``territory''. ``Net zero'' allows the assumption that we have scalable carbon capture and storage, but this should be considered a breakthrough technology and therefore not available until after 2050. ``our territory'' excludes international flights and shipping and encourages UK to export our manufacturing, which would not reduce emissions (and would probably increase them). This report has a target of zero emissions including from imported goods, international flights and shipping.
P9 ``every national and international government plan for responding to climate change has chosen to prioritise technology innovation, yet global emissions are still rising''
P31 ``To deliver the rapid pace of improvement needed we propose that stretching and imaginative embodied emissions standards are phased in for almost all manufactured product and imposed equally on UK manufactured and imported goods''
P36 ``existing strategies to motivate individuals to use less energy are not generating the scale of impact required''
P36 ``message framed about fear and climate crisis have been found to be ineffective at motivation change''
P36 ``this change will be driven by individuals acting in their professional capacity as managers, designers, engineers, cost consultants and so on''
P38 ``requires a level of cooperation which has perhaps only been seen during times of war''
P43 ``Without question, some incumbent businesses such as the fossil fuel industries, will decline and inevitably they current spend the most money on lobbying the government to claim that they are par og the solution. This is unlikely''.
P45 ``is it right to be funding research using public funds which includes technlogy developms which we know are not aligned with the 17 UN SDGs?''
P46 ``parallels between hosting the 2012 Olympic games and delivering absolute zero''
P47 Propose Government Absolute Zero Executive

Beyond 2050
P46 ``We also have to consider life beyond 2050''
P40 ``Those who are starting secondary school now in 2019/2020 will be 43 in 2050. Thinking about what education is appropriate for a very different set of industries is a key question''
P45 ``When the painful period of mitigation nears an end, we have an educated population ready to take advantage of the zero-carbon era''

Energy supply and demand
P8 P12 estimate that we could grow the UK renewable energy output to 50\% more than currently. All processes that rely on fossil fuel combustion for energy will have to move to clean electric. This is not a direct conversion, joule for joule, because heating and motors are much more efficient and so today's non-electric energy would require less energy when moved to electric. However, the increase in demand for electric that this will produce will be greater than the increase in supply and so we need to find savings of around 40\% of the energy that we currently use.
P13 ``fully electrified by 2050 and used all the final services as today, our demand will be 960 TWh…however, project of the rate … we have expanded … in the past 10 years, estimate that we will have just 580TWh available''
P4 much of our reduction in emissions is due to switch from coal to gas and the loss of heavy industry (often to suppliers overseas). 
P11 Most of our increase in clean energy supply needs to come from wind-generation - something like 4.5GW of capacity added every year for next 2 decades. Currently, only 2GW/year seems to be supported by UK government.
P12 ``If every south-facing roof in the UK were entirely conversed in high-grade solar cells, this would contribute around 80TWh/year''

Carbon capture and storage/usage
P9 ``The UK has no current plans for even a first installation [of CCS] and … it is not yet operating at a meaningful scale'' ``[CCS's] total contribution to reducing global emission is too small to be seen''.
P33 ``the only power plant operating with CCS - the Boundary Dam project at Saskatchewan … very small [and no clear evidence of how effective it is]
P33 Bio-energy CCS is ``entirely implausible due to the shortage of biomass''
P33 Carbon capture and usage requires significant additional electrical input
P33 increasing afforestation ``planting new trees is the most important technology on this page''

Statistics
P11 ``three quarters of global emissions (slightly more in the UK because we import 50\\% of our food) arise from the combustion of fossil-fuels''
P11 ``Over the past twenty years the UK's population has grown by 16\%''
P16 two thirds [of UK emissions] are produced by our use of vehicles and buildings.
P16 12\\% of UK emissions come from domestic food production, waste disposal and land use change
P40 ``Those who are starting secondary school now in 2019/2020 will be 43 in 2050. Thinking about what education is appropriate for a very different set of industries is a key question''

Lifestyle
P10 ``the only solutions available in the time remaining require some change of lifestyle''

Food
P15 ``our commitment to zero emissions in 2050 requires that we refrain from eating beef or lamb''
P20 UK imports about half of its food

Transport
P18 car weights about 12 times more than the passengers so almost all the fuel is moving the vehicle
P18 slower vehicles less drag less efficient. Also, reduce front area of vehicle - e.g. trains
P19 full electric train moves people using 40 times less energy per passenger than a single user car
P19 50 million batteries required if we convert car fleet to electric (I don't get this - DVLA site says there are just under 32M cars)
P32 beyond 2050 hydrogen can be produced by electrolysis in water … excess supply of electricity
P35 cycling in the Netherlands is a product of careful long term investment in cycling infrastructure and legislation both of which were self-reinforcing

Aviation
P33 P14 ``difficult to scale up solar-powered aeroplanes due to the slow rates of improvement in solar cell output … battery-powered flight in inhibited by the high weight of batteries, bio-fuel substitutes for Kerosene face … competition for land with food''
P18 flying in economy class equates to 180kgCO2 per person per hour…30 hours flying is about a typical UK citizen's annual emissions

Shipping
P33 P14-15 ``no electric merchant ships … isn't space to have enough solar cells on a ship to general enough energy to propel it … no attempt to build a better powered container ship. Nuclear powered nave ships operate, but without any experience of their use for freight''
P32 ammonia production for shipping may be available after 2050 by means that doesn't release CO2 in production and doesn't emit NOx

Buildings
P16 heat pumps could save approximately 80\\% of current energy demand for heating…ducted heating…saves more energy…heat pumps would almost double the demand for electricity in buildings
P27 most energy use in construction is in the materials production
P27 building codes could enforce an upper limit on amount of material

Materials
P16 ``we are already very efficient in our use of energy when making materials, but wasteful in the way we use the materials''

Cement
P15 ``cement production the chemical reaction as limestone is calcined to become clinker. There are no alternative processes … a zero-emissions economy in 2050 will have no cement-based mortar or concrete''
P21 60\\% of emissions from cement product arise from … calcination… the remaining emissions … combustion of fossil fuels
P23 may be able to use concrete demolition waste
P23 alternatives rammed earth, straw-bale, hemp-lime, engineered bamboo and timer

Steel
P15 ``Blast furnaces making steel from iron ore and coke release carbon dioxide … old steel can be recycled efficiently in electric arc furnaces … a zero-emissions economy in 2050 will have … no new steel''
P21 amount of steel available for recycling in 2050 will be roughly equivalent to 2010 steel production
P24 very high quality steep from recycling in Rotherham used in aerospace but issues removing copper from alloys

Plastic
P25 about a tonne of CO2 is released per tonne of plastic produced. Burning plastic releases CO2 - in effect a fossil fuel
P26 mechanical recycling leads to lower grade plastics. Chemical recycling may be available in the future leading to better quality recyclates.
P26 plastics are stable when landfilled and do not generate methane

Other materials
P24 there will not be enough aluminium for recycling so will need to continue primary production (bauxite???)
P25 recycling critical metals may require even more energy than primary production
P25 ceramics, mining, glass, 
P26 chemicals, paper, textiles, engineering composites
P15 ``may be possible to continue some … applications … if the [F-]gases are contained during use and at the end of product life''
P25 CO2 from ammonia production is currently captured and used in urea production  - urea decomposes to release CO2
P29 current commercial lubricants are derived from fossil fuels and emit GHG by oxidation
P29 ``solvents that emit volatile organic compounds cannot be used''
P41 UK Government's Resources and Waste Strategy proposes a national Materials Datahub to provide  comprehensive data on the available of raw and secondary materials.

Technology
P37 new product launches are main driver of change in production technologies},
  creationdate = {2020-05-05},
  keywords     = {Climate Change, Net Zero, Carbon, Mitigation, Environment},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2019},
}

@Article{vandenoordLV2019,
  author           = {van den Oord, Aaron and Yazhe Li and Oriol Vinyals},
  title            = {Representation Learning with Contrastive Predictive Coding},
  url              = {https://arxiv.org/pdf/1807.03748.pdf},
  comment          = {The InfoNCE paper. ``The main intuition behind our model is to learn the representations that encode the underlying shared information between different parts of the (high-dimensional) signal''

The longer-term trend, lower dimensional signal, is more interesting/relevant. 

For images there are a number of steps:
Each image is subdivided into overlapping patches to make a 7 by 7 grid of patches, each patch having 50\% overlap with verticle and horizontal neighbours.
Each patch is then encoded independantly - I am not sure if this uses convolutions and if these are learned and, if so, if different weights are learned for each position in the 7 by 7 grid. however, this encoding is finished with an average pooling so that the patch produces a vector of length v, and this there is a 7 x 7 x v for the image. The prediction then involves using patch-vectors in one part of the image, e.g. the top,  - aggregated into a single, context, vector - to predict the patch-vectors from the other part (the bottom). this means than the Trains with images by dividing images into overlapping patches and target is to predict which of the possible outputs is the actual next image. It's a bit ambiguous but Henaff2019 describes this better.

''CPC is a new method that combines predicting future observations (predictive coding) with a probabilistic contrastive loss .... This allows us to extract slow features, which maximize the mutual information of observations over long time horizons. Contrastive losses and predictive coding have individually been used in different ways before''. In the case of imagery - 'slow' would be features that exist over longer space (rather than longer time).

From https://github.com/davidtellez/contrastive-predictive-coding:

    Contrastive: it is trained using a contrastive approach, that is, the main model has to discern between right and wrong data sequences.
    Predictive: the model has to predict future patterns given the current context.
    Coding: the model performs this prediction in a latent space, transforming code vectors into other code vectors (in contrast with predicting high-dimensional data directly).

From https://machinethoughts.wordpress.com/2018/08/15/predictive-coding-and-mutual-information/:
''empirical results which include what appears to be the best ImageNet pre-training results to date by a large margin'' ``From a data-compression viewpoint we want to store only the important information.  Density estimation by direct cross-entropy minimization models noise.'' ``The difference between the entropy before the observation and the entropy after the observation is one way of defining mutual information. ``

From https://deepai.org/publication/data-efficient-image-recognition-with-contrastive-predictive-coding:

''CPC encourages representations that are stable over space by attempting to predict the representation of one part of an image from those of other parts of the image. Although CPC training is completely unsupervised, its learned features tend to linearly separate image classes, as evidenced by state-of-the-art accuracy for ImageNet classification with a simple linear network [49]''

DeepMind paper on contrastive predictive coding. Reads very well, but almost as if they are the first team to use CPC. Describe CPC probabilistically. Predictive coding ``is one of the oldest techniques in signal processing for data compresssion'' (refs Elias 1955 and Bishnu \& Schroeder 1970). There is also neuroscientific theories on predictive coding in the brain (Rao and Ballard, 1999 and Friston, 2005).

CPC ``combines predicing future observations (predictive coding) with a probabilistic contrastive loss''

''The main intuition behind our model is to learn the representations that encode the underlying shared information between different parts of the (high dimensional) signal ... When predicting further in the future, the amount of shared information becomes much lower, and the model needs to infer more global structure. These 'slow features' ... are more interesting''

images ``contain thousands of bits of information while the high-level latent variable such as the class label contain much less informatoin (10 bits for 1,1024 categories)''

''we do not predict future observations directly with a generative model. Instead we model a density ratio which preserves the mutual information between [the future (distant) state] and [the current state]''.

''by using a density ratio and inferring [the future state] with an encoder, we releive the model from modelling the high dimensional distribution [of conditions over time (space)]''.

''any type of encoder and autroregressive model can be used in the proposed framework''. 

Use noise contrastive estimation (InfoNCE) loss (the categoritcal cross-entrop of classifying the positive sample correctly) - minimising this loss maximises a lower bound on mutual information.

Experiment with audio, vision, natural language and reinforcement learning problems. The vision approach seems to be to predict subcrops from a patch given a crop from the patch centre - but it's described very obscurely!},
  creationdate     = {2020-05-06},
  journal          = {arXiv:1807.03748v2},
  keywords         = {Unsupervised, Deep Learning, unsupervised, MLStrat Training},
  modificationdate = {2022-05-03T06:56:29},
  owner            = {ISargent},
  year             = {2019},
}

@Article{Henaff2019,
  author       = {Olivier J. H{\'{e}}naff and Aravind Srinivas and Jeffrey De Fauw and Ali Razavi and Carl Doersch and S. M. Ali Eslami and A{\''{a}}ron van den Oord},
  title        = {Data-Efficient Image Recognition with Contrastive Predictive Coding},
  url          = {http://arxiv.org/abs/1905.09272},
  volume       = {abs/1905.09272},
  comment      = {Uses contrastive predictive coding as unsupervised pre-training and then a small amount of supervised fine-tuning to get awesome results in ImageNet.},
  creationdate = {2020-05-06},
  journal      = {CoRR},
  keywords     = {Unsupervised, Deep Learning, pretraining, object detection, MLStrat Training},
  owner        = {ISargent},
  year         = {2019},
}

@TechReport{WWF2020,
  author       = {European Policy Office, WWF},
  date         = {2020-04-07},
  institution  = {WWF},
  title        = {Building resilience: WWF recommendations for a just \& sustainable recovery after Covid-19. Using the European Green Deal to drive Europe's recovery and transition to a fair, resource-efficient and resilient society},
  comment      = {1. ENSURING JUST AND SUSTAINABLE RECOVERY PLANS
Direct at least 50\% of recovery plans into environmentally sustainable activities
Deliver social benefits through a “just transition” for all
Uphold and strengthen existing environmental standards and policies
Communicate benefits of improving the overall environmental health of societies
Ensure that EU support to third countries adheres to the same principles

2. CHANGING THE RULES OF THE GAME: RETHINK REGULATION TO STRENGTHEN RESILIENCE
Strengthen and continue implementing the European Green Deal
End fossil fuel and environmentally harmful subsidies; scale up environmental fiscal reform
Reform EU fiscal rules to facilitate public investment in decarbonising the economy
Accelerate EU sustainable finance policies to shift the trillions
Sustainable production and supply chains within and to the EU
Put people's wellbeing at the heart of the crisis response

3. RESOURCE SIDE: FINANCIAL TOOLS THAT COULD BE USED FOR RECOVERY PLANS AT EU LEVEL},
  creationdate = {2020-05-11},
  keywords     = {Environment, Covid-19, Climate Change},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2020},
}

@InProceedings{BelghaziBROBCH2018,
  author    = {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeshwar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, Devon},
  title     = {Mutual Information Neural Estimation},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  year      = {2018},
  editor    = {Dy, Jennifer and Krause, Andreas},
  volume    = {80},
  series    = {Proceedings of Machine Learning Research},
  publisher = {PMLR},
  month     = {7},
  pages     = {531--540},
  url       = {http://proceedings.mlr.press/v80/belghazi18a.html},
  address   = {Stockholmsmässan, Stockholm Sweden},
  file      = {belghazi18a.pdf:http\://proceedings.mlr.press/v80/belghazi18a/belghazi18a.pdf:PDF},
  owner     = {ISargent},
  creationdate = {2020-05-12},
}

@InProceedings{HjelmFLGBTB2019,
  author       = {Devon Hjelm and Alex Fedorov and Samuel Lavoie-Marchildon and Karan Grewal and Philip Bachman and Adam Trischler and Yoshua Bengio},
  booktitle    = {ICLR | 2019 : Seventh International Conference on Learning Representations},
  title        = {Learning deep representations by mutual information estimation and maximization},
  url          = {https://www.microsoft.com/en-us/research/publication/learning-deep-representations-by-mutual-information-estimation-and-maximization/},
  comment      = {The Deep InfoMax (DIM) paper. A development of MINE (BelghaziBROBCH2018): `` Deep InfoMax (DIM) follows MINE in this regard, though we ﬁnd that the generator is unnecessary. `` (following from Deep InfoMax is Augmented Multiscale Deep InfoMax representation learning https://github.com/Philip-Bachman/amdim-public).

Came out around the same time as Contrastic Predictive Coding (CPC; vandenoordLV2019) but `` In contrast [to CPC], the basic version of DIM uses a single summary feature that is a function of all local features, and this “global” feature predicts all local features simultaneously in a single step using a single estimator''.

In general, these methods appear to train by learning to select the correct 'other part' of the input image from a set of possible 'other parts'. In this podcast: https://www.microsoft.com/en-us/research/blog/going-meta-learning-algorithms-and-the-self-supervised-machine-with-dr-philip-bachman/, this is described nicely as ``taking something that looks like unsupervised learning, but instead, here, what we're doing, is treating it more like a supervised learning problem''.

Once representations are learned (the paper seems to indicate that this doesn't matter so much) the goodness of the representations can be assessed using a number of metrics. This is very relevant to the Toponet work and A4I because these are a range of metrics that can be used to assess how good is a trained model:

''we use the following metrics for evaluating representations. For each of these, the encoder is held ﬁxed unless noted otherwise: 
• Linear classiﬁcation using a support vector machine (SVM). This is simultaneously a proxy for MI of the representation with linear separability. 
• Non-linear classiﬁcation using a single hidden layer neural network (200 units) with dropout. This is a proxy on MI of the representation with the labels separate from linear separability as measured with the SVM above. 
• Semi-supervised learning (STL-10 here), that is, ﬁne-tuning the complete encoder by adding a small neural network on top of the last convolutional layer (matching architectures with a standard fully-supervised classiﬁer). 
• MS-SSIM(Wang et al., 2003), using a decoder trained on the L2 reconstruction loss. This is a proxy for the total MI between the input and the representation and can indicate the amount of encoded pixel-level information. 
• Mutual information neural estimate (MINE) to maximize the DV estimator of the KL-divergence. 
• Neural dependency measure (NDM) using a second discriminator that measures the KL between Ew(x) and a batch-wise shufﬂed version of Ew(x). ``

Also sort of described in this podcast: https://blubrry.com/microsoftresearch/60482376/115-diving-into-deep-infomax-with-dr-devon-hjelm: ``Training a classifier between images that go together and images that don't go together ... takes a full image, presents it through a deep NN -  if you look at different layers - different locations of the CNN have been processed by different patches of the image - you can think of features at different loations being part of the input. Deep InfoMax says all those features go together so I'm going to group them all togehter and present them to a classifier and say tell me that these go together. Then take combinations of those patch representations with images that came from somewhere else and say that these don't go together. MINE - things that go together are samples from the same joint distribution - things that don't go together are samples from the product of marginals. When you train a classifier to distinguish between these two you train the model in a similar way to interpret the dependencies between things that go together - that make them go together, ike why do they go together? - encode it into the idea about the joint distribution - this is like estimating Mutual Information (MI). In Deep InfoMax we don't really care about estimating MI, we just want a model that estimates whether there is more or less MI so that we can use that as a learning signal to train the encoder...because I'm effectively doing a comparison between patches of the same image, it needs to undestand why those patches are related....doesn't need to understand the texture because maybe that's not understandable [un]like the shape or the colour. The model will focus on those things that are easy to pick up  - how we design breaking up the data matters more than the objective that we use...architectures like ResNet quickly expand the receptive field so it is larger than the input. If you want to really leverage the locations from the natural architecture you have to be more careful how you apply the architectures. DIM tried to leverage the internal structure of the model over messing with the data and designing losses on top of that.''

This blog describes the problem nicely and shows how contrastive approaches differ from generative approaches.  https://ankeshanand.com/blog/2020/01/26/contrative-self-supervised-learning.html which has a great ideagram showing the difference between Generative / Predictive models and contrastive models.

Another blog:: https://www.microsoft.com/en-us/research/blog/deep-infomax-learning-good-representations-through-mutual-information-maximization/},
  creationdate = {2020-05-13},
  keywords     = {deep learning, unsupervised, mutual information, MLStrat Training},
  owner        = {ISargent},
  year         = {2019},
}

@Article{ZahaskyL2010,
  author       = {Christopher Zahasky and Samuel Krevor},
  date         = {2020-05-21},
  title        = {Global geologic carbon storage requirements of Q2 climate change mitigation scenarios},
  doi          = {10.1039/d0ee00674b},
  comment      = {from https://www.sciencemediacentre.org/expert-reaction-to-study-looking-at-carbon-capture-and-storage-and-climate-targets/
''“The study quantifies the potential storage capacity available to CCS and the authors conclude that this could meet the sink needed to meet IPCC warming limits.  It also describes some promising progress with CCS technology and its application''},
  creationdate = {2020-05-23},
  journal      = {Energy \& Environmental Science},
  keywords     = {Environment, Climate},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2020},
}

@Article{LeQuereEtAl2020,
  author       = {Le Quéré, Corinne and Jackson, Robert B. and Jones, Matthew W. and Smith, Adam J. P. and Abernethy, Sam and Andrew, Robbie M. and De-Gol, Anthony J. and Willis, David R. and Shan, Yuli and Canadell, Josep G. and Friedlingstein, Pierre and Creutzig, Felix and Peters, Glen P.},
  date         = {2020/05/19},
  title        = {Temporary reduction in daily global CO2 emissions during the COVID-19 forced confinement},
  doi          = {10.1038/s41558-020-0797-x},
  pages        = {1758-6798},
  url          = {https://doi.org/10.1038/s41558-020-0797-x},
  comment      = {Article using proxy data and a Confinement Index to determine global carbon emissions reductions due to Covid-19 lockdown

''Despite the critical importance of CO2 emissions for understanding global climate change, systems are not in place to monitor global emissions in real time. CO2 emissions are reported as annual values[1], often released months or even years after the end of the calendar year. ... Observations of CO2 concentration in the atmosphere are available in near-real time13,14, but the influence of the natural variability of the carbon cycle and meteorology is large and masks the variability in anthropogenic signal over a short period15,16. Satellite measurements for the column CO2 inventory17 have large uncertainties and also reflect the variability of the natural CO2 fluxes18, and thus cannot yet be used in near-real time to determine anthropogenic emissions.''

See also https://www.huffingtonpost.co.uk/entry/uk-carbon-emissions-lockdown-coronavirus_uk_5ec7cd95c5b68c7e2a862f53},
  creationdate = {2020-05-23},
  journal      = {Nature Climate Change},
  keywords     = {climate change, COVID19, Environment},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2020},
}

@Article{SinghJ2018,
  author        = {Ashudeep Singh and Thorsten Joachims},
  title         = {Fairness of Exposure in Rankings},
  journal       = {CoRR},
  year          = {2018},
  volume        = {abs/1802.07281},
  eprint        = {1802.07281},
  url           = {http://arxiv.org/abs/1802.07281},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1802-07281.bib},
  comment       = {One of several referenced in https://www.fairness-measures.org/.
Considers ways of returning results that rank according to relevance and other aspects such as exposure. This is because slight bias in the relevance/utilty can result in even larger bias in the returned results. Fairness is expressed in such a way that it can be tailored to any application - since bias/fairness is application specific.},
  owner         = {ISargent},
  creationdate     = {2018-08-13},
}

@Article{SingletonA2019,
  author    = {Singleton, Alex and Arribas-Bel, Daniel},
  title     = {Geographic Data Science},
  journal   = {Geographical Analysis: an international journal of theoretical geography},
  year      = {2019},
  issue     = {Specia.},
  pages     = {1 - 15},
  url       = {https://livrepository.liverpool.ac.uk/3046829},
  comment   = {Looks well worth reading properly. Argues for bringing Data Science and geography much closer together.},
  owner     = {ISargent},
  creationdate = {2020-06-05},
}

@Article{LiuSHDL2020,
  author       = {Jian Liu and Yifan Sun and Chuchu Han and Zhaopeng Dou and Wen-hui Li},
  title        = {Deep Representation Learning on Long-tailed Data: A Learnable Embedding Augmentation Perspective},
  volume       = {abs/2002.10826},
  comment      = {Method of altering feature space so that classes are not only better balanced but equally distributed in feature space. 'Tail' classes - those that are poorly represented - can have a low intraclass vairability and be less easily distinguished from other classes. Reducing the number of samples in a 'head' class can have a similar effect.

Usually, balancing classes is achieved by re-sampling [1], reweighting[21],anddataaugmentation[3] - paper include a clear, short overview of methods.

Also reviews other work that have introduced normalisation to the loss function to, for example, increase the separation between classes or reduce the distance from the sample and the class centre..

In this paper, two options are presented. In the first, a threshold is used to define head and tail classes. Features are extracted from samples by passing them through a network The class centres in this feature space are determined and then the angle between the classes feature values and their centres are calculated. A method (that I don't understand) is then used to change the angle between the tail classes and their centres to be the same as the head classes (an average of the head classes angles). My intuition for this is that the spread of these lesser populated tail classes is increased so that it matches the spread of the more populated head classes. Finally, the loss is calculated based on baseline methods CosFace [35] and ArcFace [5] which apply penalties based on the angle between the weight vector and the feature vectors (I don't have an intuition for this).. 

In the second option, ``We have observed that the intra-class diversity is positively correlated with the number of samples, in general. Therefore, we calculate the overall variance by weighting the angular variance of each class. `` and thus contribution to the calculated variation is determined by the size of the class sample (larger classes contribute more).. ``The advantage of the full version is that the calculation of feature cloud entirely depends on the distribution of the dataset. `` and no threshold needs defining.

This is relavant because we often struggle with unbalanced datasets and fits well with the idea of working with data in a way that maximises the variance from every class.},
  creationdate = {2020-06-11},
  journal      = {ArXiv},
  keywords     = {deep learning, training, datasets},
  owner        = {ISargent},
  year         = {2020},
}

@TechReport{ASCCCC2016,
  author       = {{ASC}},
  date         = {2016-07-12},
  institution  = {Adaptation Sub-Committee of the Committee on Climate Change},
  title        = {UK Climate Change Risk Assessment 2017: Synthesis report: priorities for the next five years},
  comment      = {Clear overview of the risks of climate change for the UK
Breaks down the risks into 5 areas:
Natural environment and natural assets
Infrastructure
People and the built environment
Business and industry
International dimensions
as well as Cross-cutting issues. Each area has all the specific risks identified and current status of adaptation in terms of (increasing urgency):
Watching brief, Sustain current action, Reseach prioroty and More action needed 
Very few are labelled ``watching brief''

''Key climate change risks for the United Kingdom
Following a systematic review of the available evidence by leading academics and other experts, the Adaptation Sub-Committee has identified six key areas of climate change risk that need to be managed as a priority.
There is an urgent case for stronger policies to tackle five of these risk areas, and a further area where there is a pressing need for more research in order to inform future policy approaches.
More action needed:
• Flooding and coastal change risks to communities, businesses and infrastructure.
• Risks to health, wellbeing and productivity from high temperatures.
• Risk of shortages in the public water supply, and for agriculture, energy generation and industry, with impacts on freshwater ecology.
• Risks to natural capital including terrestrial, coastal, marine and freshwater ecosystems, soils and
biodiversity.
• Risks to domestic and international food production and trade.
Research priority:
• New and emerging pests and diseases, and invasive non-native species, affecting people, plants and animals.''},
  creationdate = {2020-06-16},
  keywords     = {Environment, Climate},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2016},
}

@Book{CCC2019b,
  author       = {{Committee on Climate Change}},
  date         = {2019-07-10},
  title        = {Progress in preparing for climate change 2019 Report to Parliament},
  volume       = {II},
  comment      = {''tougher targets do not themselves reduce emissions. New plans must be drawn up to deliver them. And even if net zero is achieved globally, our climate will continue to warm in the short-term, and sea level will continue to rise for centuries. We must plan for this reality. Climate change adaptation is a defining challenge for every government, yet there is only limited evidence of the present UK Government taking it sufficiently seriously ... We find a substantial gap between current plans and future requirements and an even greater shortfall in action ... 56 risks ... 21 have no formal actions ... unable to give high scores for manaing risks to any of the sectors ... current global plans give only a 50\% change of meeting 3 degrees C ... prudent to plan adaptation strategies for a scenario of 4°C, but there is little evidence of adaptation planning for even 2°C. Government cannot hide from these risks ... policy ambition and implementation now fall well short of what is required ... Government continues to be off track for the fourth and fifth carbon budgets - on their own appraisal - and the policy gap has widened further this year as an increase in the projection of future emissions has outweighed the impact of new policies.''

''...The Committee has expressed particular concern about the lack of long-term planning related to coastal change, land use and housing policy, and has published reports on these issues over the last year ... The exceptions to this are planning for future public water supply, flood risk planning for new defences and infrastructure, and planning for roads and rail by Highways England and Network Rail. The Environment Agency is also currently undertaking an exercise to consider how its operations could be affected by a 4°C rise in global temperature.''

''There are still substantial gaps in our understanding, including: how businesses are impacted by extreme weather and the actions they are taking to prepare for climate change; trends in vulnerability and exposure to surface water flooding and coastal erosion; the resilience of infrastructure services including ports and airports, telecoms, digital and ICT; and infrastructure interdependencies ... The Government's Industrial Strategy makes no mention of climate change as a risk to meeting its goals, nor as an opportunity for UK skills, services, and technologies to support adaptation efforts.''

''Leaving adaptation responses to local communities and individual organisations without a strategic plan is not a strategy to manage the risks from climate change''

The executive executive summary for this could be:
''Given the piecemeal nature of the [National Adaption Plan], the gaps within it, the decline in resources and local support, and the lack of progress in managing risks, the Committee's view is that the Government's approach of mainstreaming adaptation has, so far, not succeeded in putting in place a coherent and coordinated plan, nor the resources to enable the required actions to be carried out.''

''The latest climate science and observations

Globally, average surface temperature over the 2006-2015 decade was 0.87°C (+/- 0.12°C) warmer than the 1850-1900 (pre-industrial) period.For the UK, annual average temperature for 2018 was 9.5 °C, which is 0.6 °C above the 1981-2010 long term average

Nine of the ten warmest years for the UK have occurred since 2002 and all the top ten warmest years have occurred since 1990. In the recent past (1981-2000) the chance of seeing a summer as hot as 2018 was low (<10%). The chance has already increased as a result of climate change and is now between approximately10-20%, and will increase further.

For England, the longest running instrumental record of temperature in the world, the Central England Temperature dataset, shows that the most recent decade (2008-2017) was around 1 °C warmer than the pre-industrial period (1850-1900).19.

An increase in annual average rainfall is particularly marked over Scotland for which the most recent decade (2008 - 2017) has been on average 4\% wetter than 1981 - 2010. 20 Trends in heavy rainfall over the UK vary according to the metric used; some of the Met Office's metrics for heavy rain show an increase since 1960, whilst others do not.
At the seasonal scale, UK summers for the most recent decade (2008 - 2017) have been on average 17\% wetter than 1981 - 2010 and 20\% wetter than 1961 - 1990, with only summer 2013 drier than average. The summer of 2018, however, was the driest since 2003.22 ...

The number of wildfire incidents and the area affected have been notably smaller in the last five years (2012 - 2013 to 2016 - 2017) than in the three years prior to this (2009 - 2010 to 2011 - 2012)  The Forestry Commission has not yet updated its statistics to include 2018 and 2019. Several serious moorland fires have occurred during this time including on Saddleworth Moor and Winter Hill in summer 2018. Spring fires have also been recorded in early 2019 (for example on Marsden Moor, started by a barbeque but then taking hold due to unusually dry conditions).

Observations show that the sea level around the UK has already risen by about 16 cm since 1900 and the mean rate of sea level rise around the UK was approximately 1.4 mm/yr in the 20th century.''

''Future projections

The latest UK climate projections (UKCP18) suggest that the UK climate will continue to warm over the rest of this century, and on average, rainfall is expected to increase in
winter and decrease in summer, though individual years may not conform to this pattern. UKCP18 gives projections of UK annual-average temperature increasing to between 0.5 and 5.7°C above a 1981 - 2000 baseline by the end of the century, depending on the future emissions trajectory and taking into account model uncertainty ... at least to 2050, temperatures in the UK are expected to almost certainly increase regardless of how strongly emissions are reduced globally. The change by 2050 ... between 0.5 and 2.7°C above the 1981 - 2000 baseline, depending on the pathway of global emissions. By 2050, the chance of experiencing a hot summer like 2018 is expected to be around 50\% regardless of emissions trajectory....

Winter precipitation is expected to increase significantly, and summer precipitation decrease significantly, on average. UKCP18 gives a range of projections of UK average rainfall change of up to 35\% more rainfall in winter and 47\% less rainfall in summer by 2070...

Some of the projections though show a chance of drier winters and wetter summers, so there remains more variability in projections of rainfall compared to that for temperature...

Sea levels around the UK have increased and will increase significantly more according to the latest climate change projections. UKCP18 indicates that, by 2100, sea level on the coast near London, for example, is expected to rise by between 29 - 70 cm under a low emissions scenario (approximately 1.6 degrees of mean global warming above pre-industrial levels) and by between 53 - 115 cm under a high emissions scenario (approximately 4 degrees of warming above pre-industrial levels).31 Over longer timescales, global sea level is expected to rise for centuries from now regardless of the world's climate change mitigation efforts, due to the long response time of sea level to past emissions of greenhouse gases.''

Observes serious decline in staffing of climate team in Defra (page 24) :-(

''The UK may still be considered a leader in climate change adaptation internationally, but there is a sense that progress has slowed and that there is a risk of complacency.''},
  creationdate = {2020-06-16},
  institution  = {Committee on Climate Change},
  keywords     = {Environment, Climate},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2019},
}

@InProceedings{TaubenbockWDGRSBD2009,
  author       = {H. {Taubenbock} and M. {Wurm} and N. {Setiadi} and N. {Gebert} and A. {Roth} and G. {Strunz} and J. {Birkmann} and S. {Dech}},
  booktitle    = {2009 Joint Urban Remote Sensing Event},
  title        = {Integrating remote sensing and social science},
  doi          = {10.1109/URS.2009.5137506},
  pages        = {1-7},
  abstract     = {The alignment, small-scale transitions and characteristics of buildings, streets and open spaces constitute a heterogeneous urban morphology. The urban morphology is the
physical reflection of a society that created it, influenced by historical, social, cultural, economic, political, demographic and natural conditions as well as their developments. Within the complex urban environment homogeneous physical patterns and sectors of similar building types, structural alignments or similar built-up densities can be localized and classified. Accordingly, it is assumed that urban societies also feature a distinctive socioeconomic urban morphology that is strongly correlated with the characteristics of a city's physical morphology: Social groups settle spatially with one's peer more or less segregated from other social groups according to, amongst other things, their economic status. This study focuses on the analysis, whether the static physical urban morphology correlates with socioeconomic parameters of its inhabitants - here with the example indicators
income and value of property. Therefore, the study explores on the capabilities of high resolution optical satellite data (Ikonos) to classify patterns of urban morphology based on physical parameters. In addition a household questionnaire was developed to investigate on the cities socioeconomic morphology.},
  comment      = {''The physical urban appearance is a reflection of the society that created it.''
''this study focuses on the question how remote sensing might inform social surveys and how social surveys might inform remote sensing''
''This study shows that physical parameters of the urban landscape like building size, building height, built-up density, vegetation fraction or location clearly correlate with socioeconomic parameters, which were in our case `income per month' and `value of the property'''
Ask Mel Green (Liverpool PhD) for more on this topic and on how the urban environment is shaped by society etc.},
  creationdate = {2020-06-22},
  keywords     = {social science, remote sensing},
  owner        = {ISargent},
  year         = {2009},
}

@Book{CCC2019a,
  author       = {{Committee on Climate Change}},
  date         = {2019-07-10},
  title        = {Progress in reducing UK emissions},
  comment      = {''• Policy implementation in the last year. Last year, the Committee set out 25 headline policy actions for the year ahead. Twelve months later, only one has been delivered in full. Ten of the required actions have not shown even partial progress.
• Underlying progress. The Committee also monitor indicators of underlying progress such as improvements to insulation of buildings and the market share of electric vehicles. Only seven out of 24 of these were on track in 2018. Outside the power and industry sectors, only two indicators were on track. This is a continuation of recent experience - over the course of the second carbon budget (2013-2017), only six of 21 indicators were on track.
• Projected progress. The Gover nment's own projections demonstrate that its policies and plans are insufficient to meet the fourth or fifth carbon budgets (covering 2023-2027 and 2028-2032). This policy gap has widened in the last year as an increase in the projection of future emissions outweighed the impact of new policies.''


''priorities for the Government in stepping up their delivery approach 
• Embed net-zero policy across all levels and departments of government, with strong leadership and coordination at the centre. This is likely to require changes to the
Government's overall approach to driving down emissions. For example, the Prime Minister could chair regular meetings of a Climate Cabinet that includes the Chancellor and relevant Secretaries of State, with transparent public reporting of progress and plans.
• Make policy business-friendly. It will be businesses that primarily deliver the net-zero target and provide the vast majority of the required investment. UK business groups have strongly welcomed the setting of the net-zero target and are already acting to reduce emissions. Policy should provide a clear and stable direction and a simple investable set of rules and incentives that leave room for businesses to innovate and find the most effective means of switching to low-carbon solutions.
• Put people at the heart of policy design. Over half of the emissions cuts to reach net-zero emissions require people to do things differently. The public must be engaged in the challenge and both policy and low-carbon products should be designed to reflect this. We welcome the programme of Citizens' Assemblies being convened by a group of
Parliamentary Select Committees to discuss the pathways to net-zero emissions and the involvement of the Youth Steering Group announced alongside the net-zero target.
• Support international increases in ambition and celebrate the UK ambition. Global carbon-intensity of energy has improved every year since 2011 but total emissions still grew in 2018 to record levels, over 55 GtCO2e. Many countries are currently considering revised pledges of effort ahead of the UN climate summit in late-2020 (COP26), which the UK expects to co-host with Italy. The UK should use its new net-zero target and potential position as host of COP26 to help encourage increased effort elsewhere, including adoption of similar targets by other developed countries in the EU and beyond.''


Lists Sector Priorities for the coming year Longer-term milestones for 
Surface Transport (115 MtCO2e)
Aviation \& Shipping (50 MtCO2e)
Industry (104 MtCO2)
CCS (carbon capture and storage)
Hydrogen (H2)
Buildings (88 MtCO2e)
Power (65 MtCO2e)
Agriculture \& land use (46, -10 MtCO2e)*
Waste (20 MtCO2e)*
F-gases (15 MtCO2e)*
Public Engagement
*2017 emissions figures

There are three
primary sources of uncertainty in the UK inventory:
• Uncertainty in the current GHG inventory. This comprises the statistical uncertainty in emission factors and activity data used in estimating emissions. It is internal to the inventory, is well quantified and it is possible to formally assess the probability of errors through methods set out in IPCC guidelines. For the 2014 inventory, the uncertainty was estimated as 
±3\% with 95\% confidence. This uncertainty was concentrated in sectors involving complex biological processes or diffuse sources such as waste, agriculture and land use, land-use change and forestry (LULUCF).3
• Uncertainty in Global Warming Potentials (GWPs) assigned to GHGs. GWPs are used to convert emissions from different gases into a single comparable metric (tonnes of CO2- equivalent, or tCO2e). There have been multiple changes to the GWP estimates used for methane, N₂O and F-gases since the inception of the inventory. Future changes to GWPs will significantly affect emissions as measured in MtCO₂e.
• Uncertainty from other activities. Some sources of emissions and activities (e.g. peatlands) are not currently included in the inventory but will be included in the future, thus adding to overall GHG estimates.

''Excluding the power sector, economy-wide progress was much less positive, with emissions falling by 1.0\% on average (2.0\% when temperature-adjusted). Reaching net-zero emissions in 2050 will require an average annual emissions reduction of around 15 MtCO2e (equivalent to 3\% of 2018 emissions) across the economy''

''UK greenhouse gas (GHG) emissions fell by 2.3\% in 2018 to 491 MtCO₂e and have fallen 40\% since 1990.Over the same period, the economy has grown by 75\% (Figure 1.2). Adjusting for differences in temperatures between 2017 and 2018, the underlying change in total emissions in this latest year was slightly higher, a reduction of 3.1%. These figures include emissions from international aviation and shipping (IAS), consistent with the net-zero 2050 target. UK emissions are often stated excluding IAS.2 This figure shows a higher rate of reduction (44%) since 1990, but overstates the overall progress in reducing total UK emissions as IAS emissions have increased by over 80\% since 1990.''
''The UK's consumption emissions were estimated at 784 MtCO₂e in 2016, around 56\% higher than territorial emissions (including international aviation and shipping) of 503 MtCO₂e (Figure 1.6).11 The difference is primarily due to international trade: the production overseas of goods that are imported into the UK releases more emissions (355 MtCO₂e) than the production of goods within the UK that are exported (121 MtCO₂e).''},
  creationdate = {2020-06-24},
  keywords     = {Environment, Climate},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2019},
}

@TechReport{DwyerW2020,
  author           = {Ciara Dwyer and Jonathan Wentworth},
  date             = {2020-06-19},
  institution      = {Parliamentary Office of Science and Technology},
  title            = {Managing land uses for environmental benefits},
  url              = {http://researchbriefings.files.parliament.uk/documents/POST-PN-0627/POST-PN-0627.pdf},
  comment          = {Understanding the social, economic and local factors in land use and management are necessary for successful landscape approaches. 

Natural boundaries rarely coincide with administrative boundaries, which can result can in a mismatch. 

If the management of land use remains fragmented, the UK will fail to meet 2050 net-zero emissions targets and most of the Convention on Biological Diversity global 2020 targets. 
To safeguard delivery of environmental benefits an integrated approach is required, rather than managing land uses in isolation. 
This includes working with widespread communities, sufficient mapping and modelling of landscapes, and continual monitoring of environmental outcomes while management is underway. There has previously been a lack of systematic long-term monitoring of habitat condition, which is a key indicator of the health of the environment. Benefits of integrated land use on a large scale include increased resilience to extreme weather events and enhanced sustainability of agriculture systems. Barriers to the implementation include different approaches to governance and administrative boundaries. 

From <https://post.parliament.uk/research-briefings/post-pn-0627/> 

* Fragmented land management approaches have failed to protect the biodiversity that underpins the provision of multiple benefits essential for human health and well-being.
* There have been initiatives to integrate management choices across landscapes to provide environmental benefits.
* A key challenge is encouraging partnerships between organisations, communities and landowners, to deliver multiple desired benefits from the same areas of land.
* The Environment and Agriculture Bills contain measures that may provide opportunities to support benefit provision at the landscape scale, such as the Nature Recovery Network and the Environmental Land Management scheme.},
  creationdate     = {2020-06-26},
  keywords         = {Environment, Climate},
  modificationdate = {2022-12-03T15:34:35},
  owner            = {ISargent},
  priority         = {prio1},
  year             = {2020},
}

@TechReport{WinnBF2015,
  author       = {Winn, J.P. and, Bellamy, C. C. and Fisher, T},
  institution  = {The Wildlife Trusts.},
  title        = {EcoServ-GIS Version 3.3 (Great Britain): A toolkit for mapping ecosystem services. User Guide.},
  comment      = {EcoServ-GIS is a Geographic Information System (GIS) toolkit for mapping ecosystem services at a county or regional scale. It uses input GIS/map data to generate fine-scale maps that illustrate human need or demand for ecosystem services as well as the capacity of the natural environment to provide them.
https://ecosystemsknowledge.net/ecoserv-gis

Other tools exist 

SWEEP [Online]. Natural Environment Valuation Online tool. https://sweep.ac.uk/portfolios/natural-environment-valuation-online-tool-nevo/
MAGIC [Online]. MAGIC. https://magic.defra.gov.uk/
SolVES [Online]. Social Values for Ecosystem Services (SolVES). https://www.usgs.gov/centers/gecsc/science/social-values-ecosystem-services-solves?qt-science_center_objects=0#qt-science_center_objects
Ecosystem Knowledge Network [Online]. InVEST (Integrated Valuation of Ecosystem Services and Tradeoffs). https://ecosystemsknowledge.net/invest
Ecosystems Knowledge Network [Online]. Eco-metric. https://ecosystemsknowledge.net/ecometric
CEH [Online]. Ecosystem Land Use Modelling \& Soil Carbon Flux Trial https://www.ceh.ac.uk/our-science/projects/ecosystem-land-use-modelling-soil-c-flux-trial-elum},
  creationdate = {2020-06-26},
  owner        = {ISargent},
  year         = {2015},
}

@Article{Li1954,
  author    = {C. C. Li and Louis Sacks},
  title     = {The Derivation of Joint Distribution and Correlation between Relatives by the Use of Stochastic Matrices},
  journal   = {Biometrics},
  year      = {1954},
  volume    = {10},
  number    = {3},
  pages     = {347--360},
  url       = {https://www.jstor.org/stable/3001590},
  comment   = {Original paper about joint distribution modelling?},
  owner     = {ISargent},
  creationdate = {2020-06-29},
}

@Article{MazzilliS2013,
  author       = {Giacomo Mazzilli and Andrew J Schofield},
  date         = {2013-01-01},
  title        = {A Cue-Free Method to Probe Human Lighting Biases},
  doi          = {https://doi.org/10.1068/p7517},
  comment      = {Use noise-only images (no cue) to investigate the strength of the light-from-above prior whereby in the absence of other information the brain assumes that shading is below an object.  Finds variability in the subjects and concludes from this that the light-from-above prior is ``less strong than might be expected from the literature ... contribute to the growing evidence that the prior is weak and dependent on stimulus features''.

Really clearly written with a great review of other litereature on this topic.},
  creationdate = {2020-07-16},
  journal      = {https://doi.org/10.1068/p7517},
  keywords     = {vision; visual perception;classification images},
  owner        = {ISargent},
  year         = {2013},
}

@Article{InfectioNet2020,
  author    = {Ben Attwood and Arjan Dhaliwal and Kyriaki Dionysopoulou and Freja Hunt and Josh Pooley and Jacob Rainbow and Jonathan Hughes and Kirk Harland and Martyn Fyles and David Martin},
  title     = {Ordnance Survey COVID-19 Response: Predicting the Geospatial Spread of Disease using Spatial Interaction Modelling with Gridded Data},
  url       = {https://github.com/OrdnanceSurvey/InfectioNet_Paper/blob/master/InfectioNet_06-07.pdf},
  comment   = {The paper about IngectioNet. Uses Susceptible-Exposed-Infected-Recovered-Dead model for each Person object within spatial context, defined by a grid of nodes and edges (although other forms of geospatail data could be used since this is a graphical model).},
  keywords  = {COVID19},
  owner     = {ISargent},
  creationdate = {2020-07-17},
  year      = {2020},
}

@Article{BrownEtAl2020,
  author           = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  title            = {Language Models are Few-Shot Learners},
  eprint           = {2005.14165},
  url              = {https://arxiv.org/abs/2005.14165},
  archiveprefix    = {arXiv},
  comment          = {Great paper related to the GPT-3 phenomenon in natural language processing.  The approach is to pre-train a model with a huge corpus of text (the internet - far more than anyhuman will read in their lifetime) and then apply this trained model to a series of language-related problems. 

One approach is to fine-tune pre-trained networks however ``the architecture is task-agnostic'' but ``there is still a need for task-specific datasets and task-specific fine-tuning''. ``the need for a large dataset of labelled examples for every new task limits the applicability of the language models''. Further there's some evidence that fine-tuning approaches to not generalise well outside of their original training distributions. Also, humans don't usually need a lot of examples for their own 'finetuning'. Describe the task-specific examples as ``conditioning''.

This paper explores meta-learning - a more general term for ``zero-shot transfer'' - even though previous papers haven't shown as much promise as fine-tuning approaches. Other papers in this area found increasing improvements with more complex models - more trainable paramenters- and so this paper investigates this by ``training a 175 billion parameter autoregressive language model.

The paper doesn't describe the training approach (but from poking through the GTP-2 paper RadfordWCLAS2019 and then the GPT paper I see that the unsupervised pre-training uses a multi-layer Transformer decoder) but considers 4 settings that apply the general model to a specific problem: fine-tuning (does not focus on this becaue the focus of the paper is on it's task-agnostic performance), few-shot (giving an explanation context - what the machine much do - and a few demonstration examples), one-shot (an explanation context and giving one demonstration example ) and zero-shot (only giving the explanation context) before proving the test samples. The last three examples require decreasing (to zero) numbers of task-speicifc example data. Clearly, the fewer the task-specific examples presented, the more challenging the setting.

Mention a number of text corpuses including Common Crawl which sounds like the ImageNet of text. Interestingly they say that ``we have found that unfiltered or lightly filtered version of Common Crawl tend to have lower quality than more curated datasets''

Experimented with with different numbers of parameters.
''As found in JKMH+20 and MKAT18, larger models can typically use a larger batch size but require a smaller learning rate''.
Comparison with validation loss, number of parameters and compute demonstrates a clear power-law that the greater number of parameters results in considerable reduction in loss (fewer parameters seem to have a lower limit to the loss) and obviously the compute is fairly predictable. Useful example of a way of visualising what loss is achievable given the compute availability. 

Tested the pre-trained GPT-3 against a number of tasks including different question answer, completion, translation, comprehension, reasoning, word puzzles article generation. 

The limitations section is interesting as later the limits are possibly applicable to image understanding. 

''A more fundamental limitation of the general approach described in the paper ...  is that it may eventually run into (or could already be running into) the limits of the pretraining objective'' - whatever the pretraining objective is, this will define the scope outside of which the model may not function well, even with the very best training data. 

''Another limitation ... is poor sample efficiency during training'' - it sees more text than any human ever will. ``Improving pre-training sample efficiency is an importantn direction for future work''.

''A limitation or at least uncertainity associated with few-shot learning in GPT-3 is the ambguity about whether few-shot learning actually learns new tasks ``from scratch'' at inference time or if it simply recognizes and identifies tasks that it has learned during training...posibilities exist on a spectrum''.

Really interesting section on broader impacts including a look at fairness, bias and representation that say much more about the data set than the model. For example, some experiments on associations between gender and occupation found that male identifiers tended to be associated with occupations demonstrating higher levels of education or those require physical labour. When looking at race, explored sentiment of co-occurring words with race prompts and found that ``'Asian' had a consistently [positive] sentiment ... 'Black' had a consistently [negative] sentiment ... these differences narrowed marginally with larger model sizes''.

Even look at energy usage and concluded that pre-training results in more energy efficient approaches when the result is subsequently used for a variety of purposes.},
  creationdate     = {2020-07-27},
  keywords         = {deep learning, NLP, training, MLStrat Training},
  modificationdate = {2022-12-09T13:16:48},
  owner            = {ISargent},
  primaryclass     = {cs.CL},
  year             = {2020},
}

@Article{RadfordWCLAS2019,
  author    = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  title     = {Language models are unsupervised multitask learners},
  number    = {8},
  pages     = {9},
  volume    = {1},
  comment   = {The GPT-2 paper (fewer parameters than GPT-3 

Trained with WebText which is web-scraped with extra curation based on document quality.},
  journal   = {OpenAI Blog},
  keywords  = {unsupervised, NLP, MLStrat Milestones},
  owner     = {ISargent},
  creationdate = {2020-07-27},
  year      = {2019},
}

@Article{MachalabaLDK2015,
  author       = {Machalaba, Catherine C. and Loh, Elizabeth H. and Daszak, Peter and Karesh, William B},
  title        = {Emerging Diseases from Animals},
  doi          = {10.5822/978-1-61091-611-0_8},
  pages        = {105--116},
  url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7124125/},
  comment      = {''The emergence of such “zoonoses,” responsible for a growing number of disease outbreaks that have sickened or killed millions, is facilitated by the human disruption of natural ecological conditions, which has allowed for increased human-animal contact.''
''diseases of animal origin account for about two-thirds of human infectious diseases, causing about a billion cases of human illness and millions of deaths each year, and racking up hundreds of billions of dollars in economic damage over the past two decades''
''activities give zoonoses tremendous range: with more than 1 billion international travelers every year''
''Air travel helped [Ebola] leap from West Africa to other continents, including North America and Europe''},
  comments     = {Executive vice president for health and policy, all at EcoHealth Alliance. This chapter is adapted from William B. Karesh, Andy Dobson, James O. Lloyd-Smith, Juan Lubroth, Matthew A. Dixon, Malcolm Bennett, Stephen Aldrich, Todd Harrington, Pierre Formenty, Elizabeth H. Loh, Catherine C. Machalaba, Mathew Jason Thomas, and David L. Heymann, “Ecology of Zoonoses: Natural and Unnatural Histories,” The Lancet 380, vol. 9857 (December 1, 2012): 1936-45.''},
  creationdate = {2020-08-06},
  journal      = {Nature Public Health Emergency Collection},
  keywords     = {COVID19, Environment},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2015},
}

@Article{Murray2011,
  author    = {Richard F. Murray},
  title     = {Classification images: A review},
  journal   = {Journal of Vision},
  year      = {2011},
  date      = {May 2011},
  volume    = {11},
  issue     = {5},
  url       = {https://jov.arvojournals.org/article.aspx?articleid=2191849},
  comment   = {Seems to provide all the background you need on classification images and written really clearly.

There are different ways of calculating classification images - to sum all the possitive responses and subtract from this the summed negative responses, or to find a correlation between the noise field and the responses. For the latter, it is demonstrated how this can actually be simplified to the summing approach. The stimulus images could contain actual signal, or just noise (and the participant is told that there is signal). Describe Volterra and Wiener kernels and progress to how these can be used to model neural systems and this can be applied to biological sysems by injecting white noise curent into a neural cell.. 

By modelling the observer as linear, a generalised model can be built in which the covariance is the the stimullus, the regression coefficent is the observer's template and the dependent variable is the observer's responses. To perform the regression, some researchers have asked observers to provide a multiple (e.g. 4) point response (e.g. a measure of confidence) but responses may not themselves be linear. Instead of this general linear model, a generalised linear model (GLM) is suggested that allowed for more non-linearity in the system. Even with this. it is known that human observers will depart from the GLM in important ways (e.g. perceptual learning, spatial uncertainty, response nonlinearities). Discusses more about enabling multiple responses from observer and having non-linear models later in the paper.

Classification images are produced from several thousand examples to obtain an adequate signal-to-noise ratio. However, this can result in a vey high dimensional space. Workers have tried various means to approach this - using images with fewer pixels, combining neighbouring pixels (e.g. using radial averages), ensuring noise is only applied along constrained dimensions, (e.g. position or orientation of stimulus images). [This discussion has me thinking about the inceptonism approach to understanding deep networks that can produce multiple templates throughout the generated images]. Later in the paper they reference Tjan and Nandy who proposed that an uncertain observer uses multiple templates, not just a single one.

Eye tracking. A section on PCA. Some methodological developments in other fields...

Observers templates are different in different types of noise - in low pass noise, observers shifted to using higher spatial frequencies. But in high-pass noise, they were unable to shift to lower spatial frequencies.

Method also appllied to finding illusory and occluded contours and these are found to play a key role in the preception of shape..

Important not to overstate the claims on the value of classification images. Different decision rules can produce the same classification images and the same rule can produce different classification images. They have provided an additional source of conversing evidence and sometimes have allowed researchers to estimate the properties of visual processing more precisely than was feasible with other methods.

The accuracy with which a classification image predicts and observer's trial-by-trial responses is generally less important than qualitative features such as inhibitory surrounds, that may have little impact on predicting performance and yet may be significant for our understanding of visual processing.

[not sure if this is mentioned in this or other papers but I wonder if stimulus images that produce a positive result could be shifted and scaled to obtain the best correlation to overcome the issue that observers may be finding a match in different parts of the image and at different scales?]},
  keywords  = {Visual, Psychology, Vision},
  owner     = {ISargent},
  creationdate = {2020-08-11},
}

@Article{RenYS2020,
  author       = {Zhongzheng Ren and Raymond A. Yeh and Alexander G. Schwing},
  title        = {Not All Unlabeled Data are Equal: Learning to Weight Data in Semi-supervised Learning},
  url          = {https://arxiv.org/pdf/2007.01293v1.pdf},
  comment      = {Useful for the Incisive Tagging work -= find pivotal examples by their impact on the resulting model.
From The Batch:
''Semi-supervised learning — a set of training techniques that use a small number of labeled examples and a large number of unlabeled examples — typically treats all unlabeled examples the same way. But some examples are more useful for learning than others. A new approach lets models distinguish between them.
What's new: Researchers Zhongzheng Ren, Raymond A. Yeh, and Alexander G. Schwing from the University of Illinois at Urbana-Champaign developed an algorithm that weighs the most significant examples more heavily.
Key insight: In its most common form, semi-supervised learning tries to minimize a weighted combination of supervised and unsupervised losses. Most previous approaches effectively weight each unlabeled example as equally important. The authors, instead of assigning one weight to all unlabeled examples, calculate weights for every example automatically by evaluating how it changes the model's output during training.
How it works: The algorithm works with any semi-supervised model. It trains by alternating between optimizing the model and the per-example weights.
•	First, the authors trained the model on the training set while keeping the per-example weights fixed.
•	Then they trained the per-example weights on the validation set while keeping the model parameters fixed.
•	The authors derived an influence function to calculate the gradient of the validation loss. This function measures how changing the weight assigned to an unlabeled training example affects the model parameters.
Results: Using synthetic data, the authors demonstrated that less useful examples were assigned lower weights. In image classification using the Cifar-10 and SVHN datasets, their approach marginally outperformed previous state of the art semi-supervised learning work including FixMatch and UDA. Specifically, using a Wide ResNet-28-2 and Cifar-10 with 250 labeled examples, the authors' method combined with FixMatch achieved a classification error of 5.05 percent compared to FixMatch's 5.07 percent. Combined with UDA, the authors' method on Cifar-10 achieved a classification error of 5.53 percent compared to UDA's 8.76 percent.
Why it matters: Unlabeled data points are available in far greater profusion than labeled data points. This work explores a path toward unlocking their value.
We're thinking: Sometimes another 1,000 cat pictures don't provide a model with any more useful information. But keep sending them anyway. The Batch team appreciates it!
''},
  creationdate = {2020-08-24},
  journal      = {arXiv:2007.01293v1},
  keywords     = {Incisive Tagging; Pivotal examples; data labelling; machine learning, MLStrat Data},
  owner        = {ISargent},
  year         = {2020},
}

@TechReport{STIDigitalWorkforce2020,
  date         = {July 2020},
  institution  = {{OECD} Science},
  title        = {Building digital workforce capacity and skills for data intensive science},
  number       = {90},
  type         = {STI POLICY PAPER},
  abstract     = {This report looks at the human resource requirements for data intensive science. The main focus is on research conducted in the public sector and the related challenges and training needs. Digitalisation is, to some extent, being driven by science and at the same time it is affecting all aspects of scientific practice. Open Science, including access to data, is being widely promoted and there is increasing investment in cyber-infrastructures and digital platforms but the skills that are required by researchers and research support professionals to fully exploit these tools are not being given adequate attention. The COVID-19 pandemic, which struck as this report was being finalised, has served to emphasise the critical importance of data intensive science and the need to take a strategic approach to strengthen the digital capacity and skills of the scientific enterprise as whole. This report includes policy recommendations for various actors and good practice examples to support these recommendations.},
  comment      = {''includes an assessment of how the digital workforce requirements for science differ from other sectors of society and the economy, concluding that there are unique conditions in science that are reflected in specific skills requirements...There is a need for both digitally skilled researchers, who have a common set of foundational digital skills coupled with domain-specific specialised skills, and a variety of professional research support staff, including data stewards and research software engineers''.
''There is also an important role for private sector actors to play both in the provision of training and in working together with public sector partners to define and address digital research capacity needs''
''There are different professional roles emerging, including multiple types of ``data scientist'', some of whom are supporting resarch and others who are actively involved in conducting research''.
Focus is on academia. 
Considers skillsets:
- data collection and curation
- advanced programming
- project management
- knowledge of legal aspects
This excludes the requirement for infrastructure support which is noted by several replies to https://twitter.com/sjh5000/status/1297819219950735361. Quotes from a study by (Barone, Williams and Micklos, 2017), ``universities and other institutions [in the USA] have done a fantastic job at providing physical computational resouces but haven't provided some of the nnecessary catalyssts for their effective use''
Related to the aims of #TechSolent the Pan Canadian AI strategy aims to addrss both ack of training and the effects of international mobility on reducing the talent pool.
A quote from the Carpentries found that ``more than 60\% of researchers surveyed sais that their greatest need was additional training, compared to a measgre 5\% who need access to additinoal compute power''
''Whilst requirements for open science may be specific to public sector research, the requirements for both reproducibility and ethocal practise are shared by researchers in both the public and private sector''
''DigComp... constsist of five areas that categorise the key components of digital competence'' See page 17 for these and a list of ``potential missing competencies, or competencies needing extension''
Much volunteer work goes unacknowldged. Scroggins and Pasqueto 2020 note that ``behind the data-intensive science's technological facade lies a bewildering array of huma labour, some performed in the spotlight by star scientists, but most performed behind the scenes by the precariously employed in conjuntion with computational machines''
''Buliding and maintaining the digitally skilled workformce that is needed for data intensive science requires attention to careers and reward structures....There is a need for digitally skilled researchers as well as a new cadre of professional support staff most probably data stewards and RSEs...there is considerable competition between the academic and other sectors for digitally skilled personnel, particularly in 'hot areas' such as artificial intelligence''.
The Declaration on Research Assessment (DORA, 2012) recommends that research assessment consider the valua and impact of all research outputs ... the value of digital outpus - data, software, algorithms and code - remains limited''.
''A major enabler is an education system that incorporates digital skills training at all levels, creating a pipeline of appropriately skilled students going into research careers''
Recommends organisations ``evaluate and improve the maturity of their digital workforce capacity strategy. Maturity models are commonly used to help organisations in a given area and to support understanding of what is needed to imporve performance'' (gives an example from COx et al2017)..},
  creationdate = {2020-09-04},
  keywords     = {AI, data science, skills, MLStrat Programme},
  owner        = {ISargent},
  series       = {TECHNOLOGY AND INNOVATION POLICY PAPERS},
  year         = {2020},
}

@Article{MartinPM2020,
  author       = {Charles H. Martin and Tongsu (Serena) Peng and Michael W. Mahoney},
  date         = {2020-02-17},
  title        = {Predicting trends in the quality of state-of-the-art neural networks without access to training or testing data},
  url          = {https://arxiv.org/abs/2002.06716},
  comment      = {''The quality metrics we consider are based on the spectral properties of the layer weight matrices…Note that, while we use traditional norm-based and [power law]-based matrics, our goals are not the traditional goals. Unlike more common ML approaches, we do not seek a bound on the generalisation (e.g. by evaluating training/test error during training), we do not seek a new regulariser, and we do not aim to evaluate a single model. Instead, we want to examine different models across common architecture series, and we want to compare models between different architectures themselves and in both cases we ask:
Can we predict trends in the quality of pretrained DNN models without access to training or testing data?''

https://www.aiupnow.com/2020/02/weightwatcher-empirical-quality-metrics.html

The Power Law (PL) exponents (\alpha) measures the amount of correlation, or information, that a model contains-without peeking at the training or test data. 

The empirical Norm metrics depend strongly on the scale of the weight matrix W. As such, they are highly sensitive to problems like Scale Collapse-and examining these metrics can tell us when something is potentially very wrong with our models, e.g.:
- Correlation Flow : comparing different architectures for how well information flow through the architecture
- Alpha Spikes : Identifying overparameterized models
- Scale Collapse : potential problems when distilling (finetuning) models apparent when 1 or more layers have unusually small Spectral and/or Frobenius Norms
Pretrained models used for transfer learning may appear overparameterized (from the blog): ``we suspect that many models, like BERT and GPT-xl, are over-parameterized, and that to fully use them in production, they need to be fine-tuned. Indeed, that is the whole point of these models; NLP transfer learning''.

Also https://calculatedcontent.com/2020/02/16/weightwatcher-empirical-quality-metrics-for-deep-neural-networks/},
  creationdate = {2020-09-09},
  journal      = {arXiv:2002.06716},
  keywords     = {Metrics, Deep Learning, Neural Networks, transfer learning, TopoNet, MLStrat Discovery},
  owner        = {ISargent},
  year         = {2020},
}

@Article{DmitrievGKV2017,
  author       = {Pavel Dmitriev and Somit Gupta and Dong Woo Kim and Garnet Vaz},
  title        = {A Dirty Dozen: Twelve Common Metric Interpretation Pitfalls in Online Controlled Experiments},
  year         = {2017},
  comment      = {Microsoft share the mistakes theyve made using metrics so that we can avoid the pain. Focus is online tools, websites and stuff.

Metrics are divided into 4 types:
- Data Quality Metrics
- Overall Evaluation Criteria (OEC) Metrics (you want to achieve some good stuff)
- Guardrail Metrics (you want to avoid some bad stuff too)
- Local Feature and Diagnostic Metrics (these can improve but sometimes with detriment to overall metrics)

The 12 pitfalls are
- Metric sample ratio mismatch
- Misinterpretation of ratio metrics
- Telemetry loss bias
- Assuming underpowered metrics had no change
- Claiming success with a borderline p-value
- Continuous monitoring and early stopping
- Assuming the metric movement is homogeneous
- Segment (mis)interpretation
- Impact of outliers
- Novelty and primacy effects
- Incomplete funnel metrics

See also https://blog.acolyer.org/2017/09/25/a-dirty-dozen-twelve-common-metric-interpretation-pitfalls-in-online-controlled-experiments/},
  keywords     = {Metrics},
  organisation = {Analysis and Experimentation Team, Microsoft Corporation},
  owner        = {ISargent},
  creationdate    = {2020-09-09},
}

@InProceedings{AbbeyE2000,
  author    = {Craig K. Abbey and Miguel P. Eckstein},
  date      = {14 April 2000},
  title     = {Estimates of human-observer templates for a simple detection task in correlated noise},
  url       = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/3981/1/Estimates-of-human-observer-templates-for-a-simple-detection-task/10.1117/12.383092.short?SSO=1},
  comment   = {Two-alternative  forced-choice (2AFC) experiment with a simple bump signal and different types of noise. Two observers, one naive and one is one of the authors. Conclude that human observers are sensitive to the correltation structure of the noise but that templates do not appear to fully adapt to noise correlations as would be the case for the ideal observer.},
  keywords  = {Visual, Psychology, Vision},
  owner     = {ISargent},
  creationdate = {2020-09-09},
  year      = {2020},
}

@Article{GoldMBS2000,
  author    = {Gold, J. M. and Murray, R. F. and Bennett, P. J. and Sekuler, A. B.},
  title     = {Deriving behavioural receptive fields for visually completed contours},
  journal   = {Current biology},
  year      = {2000},
  volume    = {10},
  number    = {11},
  pages     = {663--666},
  url       = {https://doi.org/10.1016/s0960-9822(00)00523-6},
  comment   = {First/early paper investigating the stimulus for observers judging whether a shape overlies others in the image. Asked observers to decide between 'fat' and 'thin' rectangles over inducers (dark circles) where the difference between fat and thin is actually a rotation o fht e inducer. Different conditions were considered - where the contours of the overlying shape are included, where they are not, where the contour of the inducer is included, where hashing covers all but the region of the inducer and where the inducer is 'fragmented' and so the appearance of the shape isn't really there at all (my interpretation). Found that the classification image is a useful way of understanding the stimulus, also if that is then reduced to only the significant negative and positive values. 2 of the 3 observers relied much more on the left hand edge, all relied on the vertical rather than horizontal. The top left was most relied on which may relate to the bias found in native English readers (refence).},
  owner     = {ISargent},
  creationdate = {2020-09-09},
}

@TechReport{Hannan2020,
  author           = {{Hannan et al.}},
  institution      = {British Conservation Alliance},
  title            = {Green Market Revolution},
  comment          = {The manifesto (https://www.greenmarketrevolution.eco) is a series of essays on the application of markets to protecting and restoring the environment. I've read a few such manifestos centre_alt_tech's ZeroCarbonBritain, UKFIRES  AbosoluteZero and EnergySysCat's Innovating to Netzero are 3 that are worth balancing for ideas on meeting carbon budgets - every one contains some excellent ideas, as well as some points that I personally support less. What does the BCA manifesto bring to the mix?

The first few essays seem to be more about criticising existing environmental groups than stating the position of BCA. It's as if the BCA is positioned deliberately to oppose other groups. Whilst providing an alternative for the neoliberal thinker, who still believes that climate change and ecosystem damage is real and a bad thing, this scene-setting makes it extremely difficult to read the following essays with balance and without reactively criticising the ideas being set out.

So, glossing over the lazy ignorance of GretaThunberg, Fridays4future, XRebellionUK demands (I will return to this), that global warming (the mitigation of which ``need[s] to be high on the agenda—if not at the top'') does not fit the theoretical Kuznets curve (draw the Kuznets curve for this https://twitter.com/weatherdak/status/1303442984214683648) , that ``we never quite seem to run out'' cannot be said of the hundreds of species that have gone extinct due to our actions, that inequality is increasing in the richest countries (or is that nothing to do with markets?) … glossing over all that there are some valuable points in the book.

For example, ideas such as ``removing some of the unfair tax burden from the nonpolluters'' make sense. The book, in parts, recognises that markets are irresponsive to externalities such as future damage and so I'm glad that the report notes ``a free market can only operate fairly if negative externalities such as carbon emissions are accounted for'' and ``ending market distortions such as fossil fuel subsidies''. Yet there is something bigger missing from this manifesto.

What I found missing is that there is no indication of precisely what it is trying to achieve. This would look like 'limit global warming to 1.5 degrees' or 'make sure the Cotswolds stay pretty' and would require that we ``pick metrics'' (as the books says in reference to individual technologies but not in terms of an overall aim). Most examples given of how market-based approaches work are local environmental issues - local pollution and resource depletion. Much of what we face now is global and requires large scale action. Yet, there is no sense in this book of the urgency of many of the issues we face (collapse of ecosystems, loss of species, risks resulting from climate heating of 10ths of a degree). One writer even seems to suggest that the consequences of continuing climate warming may be fixed by spending the money save by not mitigating now. Imagine if the war effort was simply left up to ``the market''. 

An foundational aspect of this manifesto is that “Markets and private property rights are a viable alternative to the misguided top-down government approach that has prevailed in environmental debates for so long''. Yet, an essential part of the approach to dealing with environmental damage is given in the Localism essay which mainly advocates nuisance laws. What did I miss, are such laws and regulations not ``top-down''? 

In this essay in particular, I was disturbed by the implicit notion that environmental rights are only for the landed and access to infrastructure (bc road tolls etc) prioritise the wealthy. But, as Twitter likes to tell me, I'm a socialist or something (my personal jury is still out on this). Further, there is a complete disregard for the fact that people are already being made homeless due to climate change and it's hard not to experience much of this manifesto as a 'sod the colonies, so long as I'm alright' excuse not to bother until the problems worsen on our own shores.

Ultimately, I didn't find an answer to my original question: ``how?''. How will get these ideas enacted? How will the changes required be precipitated through our country, our economy and our society? This book sets out ideas, not a roadmap. This is what concerns me more than those individual aspects that I disagree with. As I said, I've read the prepared solutions if many groups and have a pick and mix of personal favourites but quite honestly, anything of any political hue would be better than what we currently have because currently almost nothing is being done and no-one is acting at the scale and urgency that the science demands.},
  creationdate     = {2020-09-14},
  keywords         = {Climate Change, Net Zero, Carbon, Mitigation, Environment},
  modificationdate = {2022-07-18T14:23:27},
  owner            = {ISargent},
  priority         = {prio1},
  year             = {2020},
}

@InProceedings{BasuGMDKN2015,
  author       = {Basu, Saikat and Ganguly, Sangram and Mukhopadhyay, Supratik and DiBiano, Robert and Karki, Manohar and Nemani, Ramakrishna},
  title        = {DeepSat: A Learning Framework for Satellite Imagery},
  doi          = {doi:10.1145/2820783.2820816},
  isbn         = {9781450339674},
  publisher    = {Association for Computing Machinery},
  url          = {https://doi.org/10.1145/2820783.2820816},
  address      = {New York, NY, USA},
  comment      = {Satellite data classification. Uses unsupervised pre-training on normalised hard coded features to produced learned features that are applied to supervised network for classification.

There are 150 hard coded features:: mean, standard deviation, variance, 2nd moment, direct cosine transforms, correlation, co-variance, autocorrelation, energy, entropy, homogeneity, contrast, maximum probability and sum of variance of the hue, saturation, intensity, and NIR channels as well as those of the color co-occurrence matrices.

The unsupervised training is a deep belief network,  (DBN)which is then trained using Contrastive divergence

Once trained the DBN is used to initialize the weights of a feedforward backpropagation neural network.

''On the SAT-4 dataset, our best network produces a classification accuracy of 97.95\% and outperforms three state-of-the-art object recognition algorithms, namely - Deep Belief Networks, Convolutional Neural Networks and Stacked Denoising Autoencoders by ∼11%. On SAT-6, it produces a classification accuracy of 93.9\% and outperforms the other algorithms by ∼15%. Comparative studies with a Random Forest classifier show the advantage of an unsupervised learning approach over traditional supervised learning techniques''

''We argue that handwritten digit datasets like MNIST and object recognition datasets like CIFAR-10 lie on a much lower dimensional manifold than the airborne SAT-6 dataset. Hence, even if Deep Neural Networks can effectively classify the raw feature space of object recognition datasets but the dimensionality of the airborne image datasets is such that Deep Neural Networks cannot classify them. In order to estimate the dimensionality of the datasets, we use the concept of intrinsic dimension[8].'' Us DANCo algorthm CerutiBRLCC2012 to determine the dimensionality of data: 

Datase \& tIntrinsic Dimension
MNIST \& 16
CIFAR-10 \& 17
SAT-6 \& 115
Haralick Features extracted from SAT-6 \& 4.2

(I don't know why the Haralick features are introduced here)
(I find it hard to believe that CIFAR-10 only has 1 degree more than MNIST)},
  creationdate = {2020-09-17},
  keywords     = {Deep Learning, Earth Observation, Contrastive divergence, unspuervised, TopoNet, MLStrat DLRS},
  owner        = {ISargent},
  year         = {2015},
}

@Article{CerutiBRLCC2012,
  author       = {Claudio Ceruti and Simone Bassis and Alessandro Rozza and Gabriele Lombardi and Elena Casiraghi and Paola Campadelli},
  date         = {2012-06-18},
  title        = {DANCo: Dimensionality from Angle and Norm Concentration},
  comment      = {Describes the DANCo algorithm. ``we propose a novel robust intrinsic dimensionality estimator that exploits the twofold complementary information conveyed both by the normalized nearest neighbor distances and by the angles computed on couples of neighboring points, providing also closed-forms for the Kullback-Leibler divergences of the respective distributions. Experiments performed on both synthetic and real datasets highlight the robustness and the effectiveness of the proposed algorithm when compared to state of the art methodologies.''

Be really interesting to try this on different data sets.},
  creationdate = {2020-09-17},
  journal      = {arXiv:1206.3881},
  keywords     = {Data, Dimensionality},
  owner        = {ISargent},
  year         = {2012},
}

@TechReport{NFU2019,
  title        = {Achieving Net Zero, Farming's 2040 goal},
  comment      = {From CCC land use report:
''The NFU set out their assessment of how to reach net-zero agriculture emissions in England and Wales
in their publication 'Achieving Net Zero Farming's 2040 goal' published in September 2019. They
outline three pillars to achieving this:
• Pillar 1 (11.5 MtCO2e by 2050) focuses on improving farming productive efficiency through
measures aimed at improved soil quality, livestock health, diets and breeding, on-farm anaerobic
digestion and energy efficiency of vehicles and buildings.
• Pillar 2 (9 MtCO2e by 2050) is around increasing carbon storage in soils through measures such as
hedgerows, woodland on farms, soil carbon practices, and peatland and wetland restoration.
• Pillar 3 (26 MtCO2e) uses bioenergy with CCS, using bio-based materials in industry and
application of biochar to soils in the longer-term.
Bioenergy with CCS is an important component of our scenarios for achieving net-zero GHG emissions
in the UK, but we do not account for the GHG removals from this technology in the agriculture and
land-use sector. A joint report by the Royal Academy of Engineering (RAE) and Royal Society (RS)
considered how to achieve net-zero carbon emissions in the UK by 2050 through the deployment of
GHG removal measures. Their estimates for afforestation and peatland restoration are similar to ours:
• Afforestation: The RAE and RS estimate that increasing woodland cover from 13\% currently to
18%, by planting 1.2 million hectares by 2050, could deliver annual savings of 15 MtCO2e.
• Peatland restoration: While the RAE and RA analysis assumes a similar area of peatland is restored
as in our study, they assume net carbon sequestration occurs before 2050.
The RAE \& RS report also considered the sequestration potential of additional measures not covered in
our analysis:
• Soil carbon of agricultural land: The RAE and RS estimate that a range of management practices
deployed on cropland and grassland could lead to a soil carbon sink of 10 MtCO2e per year by
2050.
• Biochar: Biochar is produced from organic matter using the pyrolysis process, making it resistant
to decomposition and therefore a potential long-term store of carbon. The RAE and RS estimate
that biochar could sequester 5 MtCO2e per year by 2050, but this technology has not been
demonstrated at scale.
• Enhanced weathering: Silicate rocks naturally fix carbon out of the air over geological timescales.
This process can be speeded up by grinding up rocks (in order to vastly increase the exposed
surface area) which can be dispersed over cropland. The RAE and RS estimate that enhanced
weathering could sequester 15 MtCO2e per year by 2050. This option is not currently available at
scale in the UK.
There is a lack of sufficient and robust evidence to suggest that mineral soils can continually increase
carbon sequestration through management practices alone.17 We did not include biochar and
enhanced weathering in our scenarios for the net-zero analysis due to the potential for unforeseen
long-term side-effects, but support research to develop these options and gain a better understanding
of the potential environmental consequences of deployment in the UK.''},
  creationdate = {2020-09-17},
  keywords     = {Climate Change, Net Zero, Carbon, Mitigation, Environment},
  organisation = {National Farmers' Union},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2019},
}

@TechReport{CCCLandUse2020,
  institution  = {Committee on Climate Change},
  title        = {Land use: Policies for a Net Zero UK},
  comment      = {The actions we identify entail rapid changes in farming practices and consumer behaviour, such that around one-fifth of agricultural land is released by 2050 for actions that reduce emissions and sequester carbon (Figure 1). 
	• Low-carbon farming practices
	• Afforestation and agro-forestry
	• Peatlands
	• Bioenergy crops
	• Reducing consumption of the most carbon-intensive foods

Key objectives of the new policy framework
	• Strengthening the regulatory baseline to ensure low-regret measures are taken up 
		○ reduce on-farm emissions
		○ ban damaging practices
		○ obligation for water companies to restore peatland
	• Funding for actions above the baseline to support more costly measures
		○ auctioned contracts (e.g. similar to those offered for renewable electricity) or a carbon trading scheme
		○ public funding should be used to encourage non-carbon benefits of afforestation (e.g. alleviating flood risk, recreation)
		○ Public funding should be used to incentivise the take-up of low-carbon farming practices (e.g. precision farming)
		○ peatland restoration should also receive public funding, alongside sustainable management practices on peat that remains in agricultural production
		○ Bioenergy crops should be supported through existing instruments in the short term - in the long term best use may be in construction and with Carbon, Capture and Storage
	• Enabling measures to address non-financial barriers
		○ strengthen skills, training and market commercialisation of innovative low-carbon farming options
		○ scaling-up of capacity of the domestic forestry supply chain
		○ training on the adoption of sustainable management practices on lowland peat
		○ support the UK bioenergy market
		○ Address constraints on farms tenanted or designated as common land
		○ Review tax to ensure there is no disadvantage to farmers from changing their use of land to forestry
		○ Policies are needed to encourage consumers to shift diets and reduce food waste
		○ Development of common metrics and standards and mandatory reporting of emissions

Includes Key recommendations to deliver net-zero on land which start in 2020.

From IPCC special report on climate change and land:
	• The agriculture, forestry and other land use sector (AFOLU) is responsible for around 23\% of global GHGs largely from deforestation, and agricultural emissions from soil, livestock and nutrient management. This sector accounted for around 13\% of CO2, 44\% of methane (CH4), and 82\% of nitrous oxide (N2O) emissions from human activities globally during 2007-2016. 
	• In the UK, emissions from agriculture were 46 MtCO2e in 2017, 9\% of UK GHGs. The land use, land-use change and forestry (LULUCF) sector was a net sink, sequestering nearly 10 MtCO2e in 2017, equivalent to around 5\% of UK GHGs. Emissions from peatlands, most of which are not currently included in the GHG inventory but taken account of in our analysis, were estimated at 23 Mt CO2e in 2017.
	• Emissions from the global food and land-use system contribute 21-37\% of global GHG emissions today, but measures to reduce these exist with many bringing other co-benefits for sustainable development.
	• Land plays a critical role in supporting human society and is already under pressure from climate change. Land provides humans with food, freshwater and ecosystem services. About 70\% of the planet's ice free surface is directly affected by humans in some way. Land surface temperatures have warmed by over 1.5°C from the pre-industrial period. This warming, and the associated changes in weather extremes, is already impacting on food security including crop yields and agricultural pests and diseases.
	• Very large-scale use of land for mitigation (e.g. afforestation or biomass production) could have negative consequences for other functions of land such as biodiversity, food production and climate resilience

Among other evidence, used a land suitability modelling tool that takes account of soil series data and the impacts of future climate projections could be used to support decisions on appropriate land use now and in the future. commissioned Environment Systems Ltd (Envsys) working on the Welsh Government Capability, Suitability and Climate Programme, to consider the capability of land in Wales to support afforestation now and in the future under different climate scenarios, while taking account of other uses (Environment Systems Ltd (2019) 'Tree Suitability Modelling - Planting Opportunities for Sessile Oak and Sitka Spruce in Wales in a Changing Climate'. https://www.theccc.org.uk/wp-content/uploads/2020/01/Environment-Systems-Ltd-2020-Tree-Suitability-Modelling-%E2%80%93-Planting-Opportunities-for-Sessile-Oak-and-Sitka-Spruce-in-Wales-in-a-Changing-Climate.pdf)


• The warming from emitting 1 MtCO2e of CO2 persists in the long-term. Each additional tonne of CO2 emitted creates more warming, meaning warming induced by CO2 increases in proportion to the cumulative total emissions of CO2. p41
• The warming from emitting 1 MtCO2e of methane is more potent than CO2 over the first few decades but has largely disappeared by 100 years. This means that the warming from methane emissions largely depends on the sustained rate of methane emissions as methane does not accumulate in the atmosphere long-term. P41
• As N2O has a lifetime of approximately 120 years, it affects the climate similarly to CO2 over the first century, but has a less long-lived effect in the very long-term p41

Based on recent IPCC assessments, nearly 0.5 °C of warming is currently being masked by aerosol in the best estimate (although there remains substantial scientific uncertainty around this value) p45

47\% of food consumed in the UK imported and 18\% of UK-produced food exported (Defra) p46

Brazil and Indonesia rank particularly high due to land-use change emissions associated with clearing high-carbon land for grazing, whereas UK and European production benefits from having access to land which was already deforested centuries ago, helping to avoid these emissions. UK-produced beef generally has a lower GHG-intensity compared to the global average but similar to much of EU. P47

Our assessment is that when delivered in full by 2050, our scenario represents a strong net social gain to the UK economy; requiring investment with net lifetime costs of £17 billion but delivering at least £96 billion in benefits. P53

present analysis of economic impacts on a private as well as a social cost basis. P55 onwards

Our analysis shows that to deliver the net-zero land use scenario will require a total annual additional expenditure of £1.4 billion to fund measures that are not cost-effective to the land manager without financial support (Table 3.1). As well as reduced GHG emissions and increased carbon sequestration, this investment will generate co-benefits, including recreational benefits, air quality improvements, and public health improvements from increased physical activity. These result in total benefits to society of £4.0 billion per year. This compares with a business as usual scenario of continuing current limited tree planting, peatland restoration and uptake of low-carbon farming practices of £1.0 billion per year. The overall social [Net Present Value] NVP of our scenario is £3.3 billion per year. P55

As set out above, for many of the measures private costs outweigh private benefits resulting in an annual funding requirement of £1.4 billion to 2050. Funding will be needed for (nearly) all mitigation options (Figure 3.2). The measures to deliver the net-zero ambition for land will not be achieved if left to the market itself p57

, the 'Further Ambition' scenario for achieving net-zero by 2050 requires increasing planting rates to around 30,000 hectares of new woodland per year. P61

Flood risk alleviation …  valued using a recent report by Forest Research that looked at the costs involved in holding the amount of water held in all UK woodlands in UK reservoirs (a replacement expenditure approach). This UK wide value is then scaled down to a per hectare basis. P64

Peatland restoration … benefit to water companies due to reduce dissolved organic carbon (DOC) in water. P68

Low carbon farming … can have multiple other benefits including improved air, water and soil quality, reduced pests and diseases and improved soil structure. When considering just the emissions reductions and reduced air pollution benefits, these outweigh the costs of implementing these measures in aggregate. P69
… The largest water quality improvement arose from reducing livestock numbers p70

Social benefit cost ratios - England has lowest and Ireland the highest. Costs include land acquisition, forestry, peatland restoration, benefits include amenity and health benefits of woodland, biodiversity, low carbon farming (cheaper),  p72-73

Policy framework p75 onwards includes support, financial incentives, enabling policies, monitoring, reporting and verification

Now is the time, therefore, to redesign agricultural support systems across the UK to deliver better on climate change mitigation and other objectives, including climate change adaptation p81

Existing strategies: p81
	• The Clean Growth Strategy
	• The 25 Year Environment Plan
	• The Clean Air Strategy
	• The Industrial Strategy

This will require strong and effective leadership at all levels of government, supported by actions from people and businesses. Farmers and landowners will face many challenges over this transition, but the framework set out in this report can help to make it a fair one by creating new opportunities and revenue streams and a range of new enabling measures to overcome nonfinancial barriers. P104

Consumer behaviour from p105 onwards

At a global level, shifting to diets with lower levels of animal products consumption could contribute an important 'wedge' of emissions reduction needed to move the world to a trajectory consistent with achieving the long-term temperature goal of the Paris Agreement. P113

Research finds awareness of the environmental footprints of foods is low and that a welldesigned GHG emissions label has the potential to be an effective intervention (Camallieri et al., (2019) Consumers underestimate the emissions associated with food but are aided by labels. Nature Climate Change). P118},
  creationdate = {2020-09-17},
  keywords     = {Environment, Climate},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2020},
}

@TechReport{Christie2020,
  author       = {Lorna Christie},
  date         = {2020-10-06},
  institution  = {UK Parliament Parliamentary Office on Science and Technology},
  title        = {Interpretable machine learning},
  number       = {633},
  comment      = {Useful summary of requirements for and approaches to interpreting the outputs from ML algorithms.

Explains ML in simple terms. Describes algorithmic bias.

''''Most experts agree that for some applications, retaining a degree of human oversight is important, particularly for applications that may have significant impacts on people''

''Institute have recommended that organisations prioritise using systems that use interpretable ML methods if possible, particularly for applications that have a potentially high impact on a person or are safety critical''

Tools for interpreting black box ML may be proxy models, saliency mapping or visualisation or counterfactual explanation.

Benefits and challenges to interpretability:
Improved performance
Improved user trust
Regulatory compliance
Commercial sensitivity
Risk of gaming
Cost
Mistrust or deception (either can occur with oversimplified explanations) 

''Several national and international bodies have started to produce industry standards to promote the ethical development of ML. In 2016, the British Standards Institution published the first UK standards for ethical design of robots and autonomous systems.133 The Institute of Electrical and Electronics Engineers (a global standards body) is also working AI standards''.},
  creationdate = {2020-10-22},
  keywords     = {explaining ML, MLStrat Experts},
  owner        = {ISargent},
  year         = {2020},
}

@Article{ChenKNH2020,
  author           = {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Hinton},
  title            = {A Simple Framework for Contrastive Learning of Visual Representations},
  eprint           = {2002.05709},
  url              = {https://arxiv.org/abs/2002.05709},
  archiveprefix    = {arXiv},
  comments         = {The SimCLR paper. Investigates impact of different regimes on the quality of representations by contrastive approaches for transfer learning and fine tuning. A simese network approach. Main conclusions: - Data augmentation, particularly combining different augmentations (especially cropping and colour augmentation) improves performance - Introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations - ``Representation learning with contrastive cross entropy loss benefits from normalized embeddings and an appropriately adjusted temperature parameter'' - recommend Normalized Temperature-scaled Cross Entropy (NT-Xent) - Contrastive learning benefits from larger batch sizes and longer training compared to its supervised counterpart - Like supervised learning, contrastive learning benefits from deeper and wider networks The training framework involves performing 1 or more random augmentations on images in a randomly sampled minibatch. For each image 2 different random augmentation combinations are applied, resulting in a twice the number of data points as in the original minibatch. For each positive pair (pairs derived from the same image) all other data points are negative examples. Therefore, the larger the batch size, the more negative examples there are. The contrastive training goal is therefore to recognise the correct paired image from all other images in the batch. Mainly using ResNet-50 as base encoder, except when comparing different numbers of parameters. ``we use linear warmup for the first 10 epochs, and decay the learning rate with the cosine decay schedule without restarts (Loshchilov \& Hutter, 2016)'' Perform loads of experiments and assess the impact on top-1 (and sometimes top-5) accuracy of classification for ImageNet and other datasets - mainly by passing trained network outputs through a head layer which is trained for the classification. For each experiment, most other factors are kept simple, e.g. Linear classifier, augmentations only on one of the images in each pair. There are other tests on the impact of this approach to pre-training on fine-tuning (this approach give good results here too). Consider the representations to be the output from the average pooling layer. Paper includes some excellent figures and tables illustrating the results, to determine impact on transfer classification performance of: - Different types of augmentation when applying 1 or 2 random augmentations on one of the image pair - Different depths and widths of networks - SimCLR ReseNet-50 with 4x width and linear head is comparable standard supervised ReseNet-50 - Different projection heads - nonlinear is better but its the representation before the head that is improved (the head representations are a bit rubbish) - Different loss functions - I'm not great on the intuition with these but it seems that NT-Xent is allows better learning because it allows different degrees of negative examples (or something) - Different normalisations - ``Without `2 normalization, the contrastive task accuracy is higher, but the resulting representation is worse under linear evaluation'' - Different batch sizes and training times - longer is better, bigger is better but less important the longer you train - Different approaches to transfer learning applied to many different data sets - comparing performance of linear head on pre-trained networks (either supervised from scratch or SimCLR) with fine tuning (from either random init, supervised or SimCLR) - linear classification with SimCLR obtain comparable results and fine-tuning SimCLR with a fraction of the data has good results ``Since ImageNet images are of different sizes, we always apply crop and resize images (Krizhevsky et al., 2012; Szegedy et al., 2015), which makes it difficult to study other augmentations in the absence of cropping.'' - but with rectified RS data this may be much easier to study - impact of scale! Would be so interesting to repeat this with our national set of 'labelled' patches. Includes a useful simple overview of related work but with classic Hinton tone...},
  creationdate     = {2020-10-23},
  keywords         = {deep learning, unsupervised, MLStrat Training},
  modificationdate = {2022-05-03T06:50:37},
  owner            = {ISargent},
  primaryclass     = {cs.LG},
  year             = {2020},
}

@Article{SpringerK2020,
  author       = {Jacob M. Springer and Garrett T. Kenyon},
  date         = {2020-09-03},
  title        = {It's Hard For Neural Networks to Learn the Game of Life},
  url          = {https://arxiv.org/pdf/2009.01398.pdf},
  comment      = {Weight initialisation is really important - the best network in the trial were down to a 'lucky' set of initial weights. More nodes in a CNN means more chance of a 'lucky' set of weights and thus the size of the networks required to learn this function are often significantly larger than the minimal network required to implement the function. (and in BauZSLZT2020 there are references to work that finds networks can be compressed and then retrained and recover the overall classification accuracy).},
  creationdate = {2020-11-05},
  journal      = {ArXiv},
  keywords     = {CNN, Deep learning, weight initialisation, MLStrat Training},
  owner        = {ISargent},
  year         = {2020},
}

@Article{RibeiroSG2016,
  author       = {Ribeiro, Marco Tulio and Sameer Singh and Carlos Guestrin},
  title        = {''Why Should I Trust You?'': Explaining the Predictions of Any Classifier},
  url          = {https://arxiv.org/abs/1602.04938},
  comment      = {The LIME paper - local interpretable model-agnostic explanations. Also SP-LIME - sub-modular pick - the method for selecting the best subset of examples to demonstrate the features that the model has learned.

Desired characteristics for an explainer:
interpretable
local-fidelity
model-agnostic
provide a global perspective

''accuracy may often not be a suitable metric to evaluate the model and thus we want to explain the model. Building upon the explanationa for the individual predictions, we select a few explanations to present to the user such that they are representations of the model''.

Achieved by fitting a model that achieves the same output as the model being explained for the local region in input space. This is done by extracting nearby examples to the example being explained. 

The result are images showing highlighted regions to demonstrate the most influential parts of that image on the final output.

Tested on text SVMs and deep networks for images.

The submodular pick identifies examples that demonstrate the most of the most important features (demonding on the budget - the number of examples that the user requests).},
  creationdate = {2020-11-11},
  journal      = {arXiv.org > cs > arXiv:1602.04938},
  keywords     = {Deep learning, visualising representations, explaining ML, visualisation, MLStrat Discovery},
  owner        = {ISargent},
  year         = {2016},
}

@InProceedings{LundbergL2017,
  author           = {Scott Lundberg and Su-In Lee},
  booktitle        = {NIPS'17: Proceedings of the 31st International Conference on Neural Information Processing Systems},
  date             = {2017},
  title            = {A Unified Approach to Interpreting Model Predictions},
  url              = {https://dl.acm.org/doi/10.5555/3295222.3295230},
  comment          = {Demonstrates that LIME, DeepLIFT, layer-wise relevance propagation and three methods of Shapley Value Estimation (Shapley regression values, Shapley sampling values and quantitative input influence) are all additive feature attribution methods - and that they have an ``explanation model that is a linear function of bindary variables''.

Identify 3 properties that explanation models should have:
local accuracy
missingness (missing features have no impact)
consistency (which I haven't really understood in this context)

Propose SHAP (SHapley Additive exPlanation) values as a unified measure of feature importance. 

Observations:Kernal SHAP has imporved sample efficience and values from LIME can different significantly from SHAP values
Much stronger agreement between human explanations and SHAP than with other methods},
  creationdate     = {2020-11-11},
  keywords         = {Deep learning, visualising representations, explaining ML, MLStrat Visualisation, explainability},
  modificationdate = {2023-01-26T10:18:23},
  owner            = {ISargent},
  year             = {2017},
}

@Article{BauZSLZT2020,
  author           = {Bau, David and Zhu, Jun-Yan and Strobelt, Hendrik and Lapedriza, Agata and Zhou, Bolei and Torralba, Antonio},
  title            = {Understanding the role of individual units in a deep neural network},
  doi              = {10.1073/pnas.1907375117},
  eprint           = {https://www.pnas.org/content/early/2020/08/31/1907375117.full.pdf},
  issn             = {0027-8424},
  url              = {https://www.pnas.org/content/early/2020/08/31/1907375117},
  abstract         = {Deep neural networks excel at finding hierarchical representations that solve complex tasks over large datasets. How can we humans understand these learned representations? In this work, we present network dissection, an analytic framework to systematically identify the semantics of individual hidden units within image classification and image generation networks. First, we analyze a convolutional neural network (CNN) trained on scene classification and discover units that match a diverse set of object concepts. We find evidence that the network has learned many object classes that play crucial roles in classifying scene classes. Second, we use a similar analytic method to analyze a generative adversarial network (GAN) model trained to generate scenes. By analyzing changes made when small sets of units are activated or deactivated, we find that objects can be added and removed from the output scenes while adapting to the context. Finally, we apply our analytic framework to understanding adversarial attacks and to semantic image editing.The code, trained model weights, and datasets needed to reproduce the results in this paper are public and available to download from GitHub at https://github.com/davidbau/dissect and at the project website at https://dissect.csail.mit.edu/data/.},
  comment          = {Excellent paper.

Summarises nicely why salience maps aren't very helpful (my opinion) because they ``ask where a network looks when it makes a decision''. ``The goal of our current inquirey is [to] ask what a network is looking for, and why''.

Uses network dissection to ``directly interpret the internal compution of the network itself, rather than training an auxillary model''.

This paper finds ``that a trained network contains units that correspond to high-level visual concepts that were not explicity labelled in the training data''.

The visual concepts - colours, materials, parts and objects, are identified using the segmentation model of Xiao and Zhou, 2018 ``that is trained to predict the presence of the visual conept c within image x at position p''. Using this they are able to correlate the parts of images that fired particular network units with different visual concepts and then count the number of matched concepts for each unit. Found that early layer of VGG matched conepts of colour, then colour and material and gradually parts and then whole objects activate units. 

''the last convolutional layer has the largest number of object classes detected units while the number of objects parts peaks two layers earlier''.

Would be great to repeat this approach with TopoNet/GlobeNet and use humans to match the concepts? 

Conclude that the emergent object detection done by units in the last convolutional layer is not spurious. ``The most interpretable units are those that are important to many different output classes. Units that are important to only one class (or none) are less interpretatble, measured by IOU ... important units are predominantly positively correlated with their associated classes and different combinations of units provide support for each class''. 

They conduct similar experiments on GANs as an unsupervised equivalent and find similar results along the path to output of particular visual concepts - although the largest number of emergent concepts appear in the middle of the network. By tweaking the units, it is possible to add and remove concepts, or make them more or less dominant, and fill in regions that different concepts had previously occluded - although they cannot be easily added in parts of an image in which they would not be found in the original data (e.g. a door in the sky). . 

''Studies of network compression have shown that many units can be eliminated from a network while recoering overall classification accuracy by retraining''.  (and we know from SpringerK2020 that starting more units means more chance of a good solution)

http://dissect.csail.mit.edu/
https://github.com/davidbau/dissect},
  creationdate     = {2020-11-11},
  elocation-id     = {201907375},
  journal          = {Proceedings of the National Academy of Sciences},
  keywords         = {Deep learning, visualising representations, explaining ML, MLStrat Discovery, IncisiveTagging},
  modificationdate = {2022-06-27T15:15:36},
  owner            = {ISargent},
  publisher        = {National Academy of Sciences},
  year             = {2020},
}

@Article{BachBMKMS2020,
  author           = {Sebastian Bach and Alexander Binder and Gr\'{e}goire Montavon and Frederick Klauschen and Klaus-Robert Müller and Wojciech Samek},
  date             = {2015-07-10},
  title            = {On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation},
  number           = {7},
  url              = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140},
  volume           = {10},
  :url             = {https://doi.org/10.1371/journal.pone.0130140},
  comment          = {''We would like to find out, separately for each image x, which pixels contribute to what extent to a positive or negative classification result''

Long paper with lots of theory and maths working out how to show the relelvance of pixels to the output classification by producing heatmaps.

''Layer-wise relevance propagation in its general form assumes that the classifier can be decomposed into several layers of computation''.

Messages distribute relelvance of a neuron k onto its input nerons at layer l

''In our proposed definition, the total relevance is constrained to be preserved from one layer to another, and the total node relevance must be the equal to the sum of all relevance messages incoming to this node and also equal to the sum of all relevance messages that are outgoing to the same node ... different algorithms with different resulting solutions may be admissible under these constraints''.

Most of the paper is about Taylor-type decomposition.

''the local gradient at the prediction point x may not be a good explanation for the contributions of single dimensions to the function value f(x)''.

ZeilerF2014 ``solves optimization problems in order to reconstruct the image input, while our approach attempts to reconstruct the classifier decision''

SimonyanVZ2013 present an approach that lies between partical derivatives at the input point x and a full Taylor-series around a different point $x_0$

Our approach aims at explaining the decision for a given input rather than finding optimal stimuli for a particular neuron.

''we have proposed two different approaches to pixel-wise decomposition: The first one, Taylor-type decomposition, seeks to linearly approximate the class scoring function locally by performing a Taylor decomposition of it near a neutral data point without class membership, where the contribution of each dimension (i.e. pixel) can easily be identified. The second one, coined layer-wise relevance propagation, applies a propagation rule that distributes class relevance found at a given layer onto the previous layer. The layer-wise propagation rule was applied iteratively from the output back to the input''

''Notably, these two methods were not defined as a particular solution to the heatmapping problem, but instead as a set of constraints that the heatmapping procedure must fulfill in order to be admissible''

''in the case of the ImageNet convolutional network, we have shown that the heatmapping procedure finds class-relevant features that can be large areas of a particular color, localized features, image gradients, or more structured visual features such as edges, corners, contours, or object parts''.
Tests approach on Baf of Words and Deep Learning approaches.},
  creationdate     = {2020-11-11},
  journal          = {PLoS ONE},
  keywords         = {Deep learning, visualising representations, explaining ML, visualisation, MLStrat Discovery, explainability},
  modificationdate = {2023-01-25T10:55:59},
  owner            = {ISargent},
  year             = {2015},
}

@Book{Klein2007,
  author    = {Naomi Klein},
  title     = {The Shock Doctrine: The Rise of Disaster Capitalism},
  year      = {2007},
  comment   = {Notes in causes folder},
  owner     = {ISargent},
  creationdate = {2020-11-19},
}

@TechReport{TyresL2020,
  author           = {Roger Tyers and Pauline Leonard},
  date             = {2020-10-01},
  institution      = {Web Science Institute, University of Southampton},
  title            = {Making Smart Fair},
  eprint           = {https://southampton.ac.uk/~assets/doc/wsi/WSI%20white%20paper%203.1%20smart%20cities-1.pdf},
  number           = {WSI White Paper #3},
  subtitle         = {Building Inclusive, Fair and Sustainable Transport for Cities of the Future},
  comment          = {Review of Smart City research and commentry

''Smart cities are intended to harness and harmonise technological innovations - especially Big Data and the Internet of Things (IoT) - to improve infrastructures and outcomes in terms of efficiency, sustainability and citizen engagement (Debnath et al., 2014; Gil-Garcia et al., 2015)''

''Some worry that the smart city might just be a `neoliberal' city, where private consultants, engineering corporations and tech start-ups erode democraticallyelected and often cash-strapped public authorities with self-serving technological `solutionism' (Grossi \& Pianezzi, 2017). Echoing wider and largely unresolved concerns over the use of Big Data, others fear that ethical and privacy concerns may easily be over-ridden in the collection of personal data from citizens as they travel about their daily lives (Kitchin, 2016).

''This paper will critically review these views,  but also seeks to add to them through four key areas:
1. Definitions and barriers: cost, privacy and security
2. Inclusion and fairness: gendered inequalities in urban transport
3. Sustainability in smart city transport
4. The post-COVID city''},
  creationdate     = {2020-11-23},
  keywords         = {trust, transport, smart, environment},
  modificationdate = {2023-12-11T08:04:12},
  owner            = {ISargent},
  year             = {2020},
}

@Article{SrivastavaHKSS2014,
  author    = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title     = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  number    = {56},
  pages     = {1929--1958},
  url       = {https://jmlr.org/papers/v15/srivastava14a.html},
  volume    = {15},
  comment   = {This is THE dropout paper},
  journal   = {Journal of Machine Learning Research},
  keywords  = {deep learning, training, MLStrat Milestones},
  owner     = {ISargent},
  creationdate = {2020-11-27},
  year      = {2014},
}

@Article{SergeevD2018,
  author    = {Alexander Sergeev and Del Balso, Mike},
  date      = {2019-02-21},
  title     = {Horovod: fast and easy distributed deep learning in TensorFlow},
  url       = {https://arxiv.org/abs/1802.05799},
  comment   = {The Horovod paper

Horovod is a Russian dnace
ring-allreduce algorithm (from Baidu) to calculate the gradiants during training. Decentralise the averaging by passing gradients between workers without the need for a parameter servier - much less bandwidth required.},
  journal   = {arXiv:1802.05799},
  keywords  = {training, MLStrat Training},
  owner     = {ISargent},
  creationdate = {2020-11-26},
  year      = {2018},
}

@Article{GoyalFacebook2018,
  author       = {Priya Goyal and Piotr Dollár and Ross Girshick and Pieter Noordhuis and Lukasz Wesolowski and Aapo Kyrola and Andrew Tulloch and Yangqing Jia and Kaiming He},
  date         = {2018-04-30},
  title        = {Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour},
  url          = {https://arxiv.org/abs/1706.02677},
  comment      = {Facebook paper finding that its good to ``warm up'' the learning rate by stating with lower learning rates at the start of training.
''In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyperparameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training.''},
  creationdate = {2020-11-27},
  journal      = {arXiv:1706.02677},
  keywords     = {deep learning, training, MLStrat Training},
  owner        = {ISargent},
  year         = {2017},
}

@Article{VermaLBNMCLB2019,
  author        = {Vikas Verma and Alex Lamb and Christopher Beckham and Amir Najafi and Ioannis Mitliagkas and Aaron Courville and David Lopez-Paz and Yoshua Bengio},
  date          = {2020-05-11},
  title         = {Manifold Mixup: Better Representations by Interpolating Hidden States},
  eprint        = {1806.05236},
  url           = {https://arxiv.org/abs/1806.05236},
  archiveprefix = {arXiv},
  comment       = {Improve the generalisation of neural networks by mixing up inputs and their representations. 
1) select a random layer k from a set of eligible layers S in the neural network. This set may include the input layer g0(x)
2) process two random data minibatches (x, y) and (x0, y0) as usual, until reaching layer k. This provides us with two intermediate minibatches (gk(x), y) and (gk(x0), y0)
3) perform Input Mixup (Zhang et al., 2018) on these intermediate minibatches. This produces the mixed minibatch
4) continue the forward pass in the network from layer k until the output using the mixed minibatch (˜gk, y˜)
5) output is used to compute the loss value and gradients that update all the parameters of the neural network

See also https://mila.quebec/en/article/learning-better-representations-by-interpolating-hidden-states/ although it doesn't give much away},
  journal       = {arXiv:1806.05236},
  keywords      = {Neural Networks, training, supervised, generalisation},
  owner         = {ISargent},
  primaryclass  = {stat.ML},
  creationdate     = {2020-12-01},
  year          = {2019},
}

@Article{DuWZBSS2019,
  author        = {Simon S. Du and Yining Wang and Xiyu Zhai and Sivaraman Balakrishnan and Ruslan Salakhutdinov and Aarti Singh},
  title         = {How Many Samples are Needed to Estimate a Convolutional or Recurrent Neural Network?},
  eprint        = {1805.07883},
  archiveprefix = {arXiv},
  comment       = {Addresses the ``folklore'' that convolutional approaches to learning deep models are more efficient than fully-connected neural networks (FNNs) using a lot of maths that I lost the plot with.
''We show that the sample-complexity to learn CNNs and RNNs scales linearly with their intrinsic dimension and this sample-compexity is much smaller than for their FNN counterparts''},
  creationdate  = {2020-12-10},
  keywords      = {deep learning, dataset, MLStrat Intro},
  owner         = {ISargent},
  primaryclass  = {stat.ML},
  year          = {2019},
}

@Article{Garciaetal2020,
  author        = {Dolores Garcia and Gonzalo Mateo-Garcia and Hannes Bernhardt and Ron Hagensieker and Ignacio G. Lopez Francos and Jonathan Stock and Guy Schumann and Kevin Dobbs and Freddie Kalaitzis},
  title         = {Pix2Streams: Dynamic Hydrology Maps from Satellite-LiDAR Fusion},
  year          = {2020},
  eprint        = {2011.07584},
  url           = {https://arxiv.org/abs/2011.07584},
  archiveprefix = {arXiv},
  comment       = {Multi-sensor deep-learning model for segmenting water networks. Lidar, Worldview (Maxar), Planet time series. See slides from presentation on 14/12/20 in OneNote doc. Uses U-net for segmentation of water features.

Maxar gets highest accuracy - it is highest resolution. However, for daily update, Planet + lidar is a good option.},
  keywords      = {Deep learning; Segmentation; Hydrology},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2020-12-14},
}

@Article{DoerschZ2017,
  author        = {Carl Doersch and Andrew Zisserman},
  title         = {Multi-task Self-Supervised Visual Learning},
  eprint        = {1708.07860},
  archiveprefix = {arXiv},
  comment       = {Look at different options for proxy tasks in self-supervised learning, including combining tasks. 

Self-supervised tasks usually involve removing part of a complex signal and asking the network to fill in the missing information. Examples include colourising greyscale images, predicting the relative position of two points in a image, filling in image holes, solving jigsaw puzzles or predicting movement in videos. Networks pre-trained this way tend to not perform as well as those trained in an evaluation task directly. 

Also give examples of multi-task learning - simulteneous supervised learning of multiple tasks - in which the idea is that the representations learn are applicable to all problems.

Some discussion of distribution of the training tasks across different workers in a cluster and the different approaches to updating the weights. When learning for multiple tasks, weight update needs to consider both the different workers and different tasks. After some experimentation they used an approach that aggregated gradients across workers for single tasks and updated when the task was ready, without synchronising with other tasks. Found RMSProp optimizer use per tasks was the better option (over SGD) and used separate moving averages for each tasks to scale the updates before applying them.

Better results (on benchmark classification tests) are obtained for relative position and colourisation (colorization) when used as single proxy tasks. However, a combination of relative position, colourisation, exemplar and motion segmentation generally produces even better results. 

They do consider how to harmonize the approach so that inputs look similar for different tasks when training with multiple tasks because networks may be learning to recognise specific inputs (e.g. colour from relative position tasks) but this has little beneficial effect.},
  keywords      = {unsupervised, self-supervised, deep learning, MLStrat Training},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2020-12-14},
  year          = {2017},
}

@Article{SankararamanDXHG2020,
  author        = {Karthik A. Sankararaman and Soham De and Zheng Xu and W. Ronny Huang and Tom Goldstein},
  title         = {The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent},
  eprint        = {1904.06963},
  url           = {https://arxiv.org/abs/1904.06963},
  archiveprefix = {arXiv},
  comment       = {Considers how the architecture of a network affects how easily it trains. Come up with 'gradient confusion' (~ does the gradient change direction between updates ) and find that increasing the width of layers decreases gradient confusion but increasing the number of layers increases gradient confusion.  Suggest that widening the network with depth is part of the solution to this.

Also show that the combination of batch normalization and skip connections lower gradient confusion and help train very deep models and that a promising direction for improving the trainability of very deep networks is to initialise weight matrices with orthogonal matrices.},
  keywords      = {deep learning, weights initilisation, gradient descent, training, MLStrat Training},
  owner         = {ISargent},
  primaryclass  = {cs.LG},
  creationdate     = {2020-12-15},
  year          = {2020},
}

@Article{MaennelATBBGK2020,
  author        = {Hartmut Maennel and Ibrahim Alabdulmohsin and Ilya Tolstikhin and Robert J. N. Baldock and Olivier Bousquet and Sylvain Gelly and Daniel Keysers},
  title         = {What Do Neural Networks Learn When Trained With Random Labels?},
  eprint        = {2006.10455},
  url           = {https://arxiv.org/abs/2006.10455},
  archiveprefix = {arXiv},
  comment       = {Not got any labels? What about training with random labels? Whilst this has been used to study if networks can memorize data, this paper shows that pre-training with random labels can be an efficient approach to producing better results when fine-tuning to a real task than scratch training on a task.},
  keywords      = {deep learning, unsupervised learning, pre-training, MLStrat Training},
  owner         = {ISargent},
  primaryclass  = {stat.ML},
  creationdate     = {2020-12-15},
  year          = {2020},
}

@Article{AhmadvA2020,
  author        = {Nasir Ahmad and Marcel A. J. van Gerven and Luca Ambrogioni},
  title         = {GAIT-prop: A biologically plausible learning rule derived from backpropagation of error},
  eprint        = {2006.06438},
  url           = {https://arxiv.org/abs/2006.06438},
  archiveprefix = {arXiv},
  comment       = {Back propogation for weight update works extremely well for neural networks and is standard in deep learning. However, it is considered biologically implausible and so means of updating that are more plausible for real neural networks have been proposed. One approach is target propogation, in which each layer has a target (probably the output values, unless they are derived using another heuristic) by which the error is calculated at each layer. However, target propogation proposals so far do not perform as well as back propagation. This paper proposes a modified form of target propagation (GAIT-prop) where the target is a small perturbation of the forward pass and demonstrates a equlivalence to backpropagation.},
  keywords      = {deep networks, learning algorithms, back propagation, MLStrat Training},
  owner         = {ISargent},
  primaryclass  = {cs.LG},
  creationdate     = {2020-12-15},
  year          = {2020},
}

@Article{BaiKK2020,
  author           = {Shaojie Bai and Vladlen Koltun and J. Zico Kolter},
  title            = {Multiscale Deep Equilibrium Models},
  eprint           = {2006.08656},
  url              = {https://arxiv.org/abs/2006.08656},
  archiveprefix    = {arXiv},
  comment          = {The MDEQ

A differentiable modelling approach that produces implicit representations rather than the explicit (hierarchical) representations in a deep network. The difference I obtain from this paper is that explicit approaches have layers with different functions whereas implicit models share the weights and doesn't therefore have explicit layers. DEQ (BaiKK2019) approaches are good for sequence data but not necessarily for multiscale data such as in imagery.

Deep networks model higher resolutions flowing into lower resolutions. In this implicit approach, the higher and lower resolutions are maintained alongside each other.

This mutliscale advancement can be used as a backbone for both classification and segmentation (object localization) tasks.

Development of BaiKK2019.},
  creationdate     = {2020-12-15},
  keywords         = {machine learning, implicit models, MLStrat Training, continious depth},
  modificationdate = {2022-05-08T11:27:27},
  owner            = {ISargent},
  primaryclass     = {cs.LG},
  year             = {2020},
}

@Article{ChenRBD2019,
  author           = {Ricky T. Q. Chen and Yulia Rubanova and Jesse Bettencourt and David Duvenaud},
  title            = {Neural Ordinary Differential Equations},
  eprint           = {1806.07366},
  url              = {https://arxiv.org/abs/1806.07366},
  archiveprefix    = {arXiv},
  comment          = {Fantastic explanation here: https://www.youtube.com/watch?v=jltgNGt8Lpg but also worth watching this https://www.youtube.com/watch?v=YZ-_E7A3V2w in which David Duvenaud gives an honest account of the content of the paper.

Residual networks model the change from one state (layer) to the next but are confined to descrete steps by the layers and so the state between steps is not modelled. Instead, this approach models the whole field of states. Therefore, where the field is complicated, the steps can be small, but in other areas they can be larger. 

This approach leads to a number of very interesting looking alternatives to learning models},
  creationdate     = {2020-12-16},
  keywords         = {machine learning, implicit models, MLStrat Training, continious depth},
  modificationdate = {2022-05-04T19:17:53},
  owner            = {ISargent},
  primaryclass     = {cs.LG},
  year             = {2019},
}

@Article{BaiKK2019,
  author           = {Shaojie Bai and J. Zico Kolter and Vladlen Koltun},
  title            = {Deep Equilibrium Models},
  eprint           = {1909.01377},
  url              = {https://arxiv.org/abs/1909.01377},
  archiveprefix    = {arXiv},
  comment          = {Sort of a one layer network - but sort of recurrent. The input is injected into the layer, as is the output from the layer. Optimisation assumes that an equilibrium point is available and computes diferentials according to this.

Practical implementation on WikiText-103 benchmark of ChenRBD2019.},
  creationdate     = {2020-12-18},
  keywords         = {machine learning, implicit models, MLStrat Training, continious depth},
  modificationdate = {2022-05-04T19:18:57},
  primaryclass     = {cs.LG},
  year             = {2019},
}

@Article{Kkirkwood2020,
  author        = {Charlie Kirkwood},
  title         = {Deep covariate-learning: optimising information extraction from terrain texture for geostatistical modelling applications},
  year          = {2020},
  eprint        = {2005.11194},
  url           = {https://arxiv.org/abs/2005.11194},
  archiveprefix = {arXiv},
  comment       = {Convolutions with probilistic output over terrain model sampled over point measurements of water chemistry. At the fully connected head combine with spatial location (x, y, z) . Then, predictions are probilistic and over whole DTM region. Looks really powerful.},
  keywords      = {deep learning; geostatistics; bayesian approaches},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2020-12-18},
}

@Article{PalmO2006,
  author           = {Elin Palm and Ove Hansson, Sven},
  journaltitle     = {Technological Forecasting and Social Change},
  title            = {The case for ethical technology assessment (eTA)},
  doi              = {https://doi.org/10.1016/j.techfore.2005.06.002},
  url              = {https://www.sciencedirect.com/science/article/pii/S004016250500082X},
  comment          = {Discusses technology assessments, which have a long history but tend to be rather focussed on the perspective of the 'West'.
''social and legal response is constantly one step behind technological development''.
''We see it as the principal task of eTA to find and characterise the ethical aspects of an emerging technology.''
''one approach would be the consequentialist where potential benefits for individuals and society are weighed against potential harms for individuals, community, and environment''.
The following is our preliminary version of the ethical check-list:
1. Dissemination and use of information
2. Control, influence and power
3. Impact on social contact patterns
4. Privacy
5. Sustainability
6. Human reproduction
7. Gender, minorities and justice
8. International relations
9. Impact on human values.},
  creationdate     = {2021-01-04},
  keywords         = {ethics, MLStrat, trust},
  modificationdate = {2023-12-11T07:57:48},
  owner            = {ISargent},
  priority         = {prio1},
  year             = {2006},
}

@Article{SelvarajuCDVPB2019,
  author           = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  date             = {2019},
  title            = {Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization},
  doi              = {10.1007/s11263-019-01228-7},
  issn             = {1573-1405},
  number           = {2},
  pages            = {336-359},
  url              = {http://dx.doi.org/10.1007/s11263-019-01228-7},
  volume           = {128},
  comment          = {''Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept...We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions.''},
  creationdate     = {2021-01-07},
  journal          = {International Journal of Computer Vision},
  keywords         = {deep learning, visualisation, MLStrat Discovery},
  modificationdate = {2023-01-25T11:00:28},
  month            = {10},
  owner            = {ISargent},
  publisher        = {Springer Science and Business Media LLC},
  year             = {2019},
}

@Article{NairH2012,
  author    = {Vinod Nair and Hinton, Geoffrey E.},
  title     = {Rectified Linear Units Improve Restricted Boltzmann Machines},
  url       = {https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf},
  comments  = {The ReLU reference},
  keywords  = {MLStrat Training},
  owner     = {ISargent},
  creationdate = {2021-01-07},
  year      = {2012},
}

@Article{IoffeS2015,
  author        = {Sergey Ioffe and Christian Szegedy},
  title         = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  eprint        = {1502.03167},
  archiveprefix = {arXiv},
  comment       = {The Batch Norm paper},
  keywords      = {deep learning, training, MLStrat Milestones},
  owner         = {ISargent},
  primaryclass  = {cs.LG},
  creationdate     = {2021-01-07},
  year          = {2015},
}

@Online{Hutson2018,
  author       = {Matthew Hutson},
  date         = {2018-05-03},
  title        = {AI researchers allege that machine learning is alchemy},
  url          = {https://www.sciencemag.org/news/2018/05/ai-researchers-allege-machine-learning-alchemy},
  comment      = {''Ali Rahimi, a researcher in artificial intelligence (AI) at Google in San Francisco, California, took a swipe at his field last December—and received a 40-second ovation for it''

''There's an anguish in the field,'' Rahimi says. ``Many of us feel like we're operating on an alien technology.''

The issue is distinct from AI's reproducibility problem

It also differs from the ``black box'' or ``interpretability'' problem

As Rahimi puts it, ``I'm trying to draw a distinction between a machine learning system that's a black box and an entire field that's become a black box.''

Without deep understanding of the basic tools needed to build and train new algorithms, he says, researchers creating AIs resort to hearsay, like medieval alchemists.},
  creationdate = {2021-01-07},
  data         = {May. 3, 2018},
  keywords     = {machine learning, MLStrat Milestones},
  owner        = {ISargent},
  year         = {2018},
}

@InProceedings{SculleySWR2018,
  author           = {D. Sculley and Jasper Snoek and Alex Wiltschko and Ali Rahimi},
  booktitle        = {International Conference on Learning Representations},
  date             = {2018-02-12},
  title            = {Winner's Curse? On Pace, Progress, and Empirical Rigor},
  url              = {https://openreview.net/forum?id=rJWF0Fywf&noteId=rJWF0Fywf},
  comment          = {Great paper outlining current problems in machine learning arising from for example, the tendancy to develop using competitions and less about developing insights. E.g. latest algorithm always wins the competition (because it is tuned until it does). Suggests solutions.},
  creationdate     = {2021-01-07},
  keywords         = {deep learning, training, domain expertise, MLStrat Data},
  modificationdate = {2022-12-11T18:29:52},
  owner            = {ISargent},
  year             = {2018},
}

@InProceedings{ChenRCWJLS2020,
  author    = {Mark Chen and Alec Radford and Rewon Child and Jeffrey K Wu and Heewoo Jun and David Luan and Ilya Sutskever},
  booktitle = {Thirty-seventh International Conference on Machine Learning},
  title     = {Generative Pretraining From Pixels},
  url       = {https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf},
  comment   = {The Image GPT paper},
  keywords  = {deep learning, MLStrat Milestones},
  owner     = {ISargent},
  creationdate = {2021-01-07},
  year      = {2020},
}

@Article{EveringhamEVWZ2015,
  author    = {Everingham, M. and Eslami, S. M. A. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.},
  title     = {The Pascal Visual Object Classes Challenge: A Retrospective},
  number    = {1},
  pages     = {98--136},
  url       = {http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham15.pdf},
  volume    = {111},
  journal   = {International Journal of Computer Vision},
  keywords  = {MLStrat Data},
  month     = {1},
  owner     = {ISargent},
  creationdate = {2021-01-08},
  year      = {2015},
}

@InProceedings{OquabBLS2014,
  author    = {M. Oquab and L. Bottou and I. Laptev and J. Sivic},
  booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {Learning and Transferring Mid-level Image Representations Using Convolutional Neural Networks},
  doi       = {10.1109/CVPR.2014.222},
  pages     = {1717-1724},
  url       = {https://ieeexplore.ieee.org/document/6909618},
  comments  = {Take AlexNet pretrained on ImageNet and apply it to the Pascal VOC object detection and action classification challenges. Localsation of objects is achieved by classifying patches from the Pascal VOC dataset.},
  keywords  = {deep learning, pretraining, transfer learning, MLStrat Training},
  owner     = {ISargent},
  creationdate = {2021-01-08},
  year      = {2014},
}

@Article{LinMBBGHPRZD2015,
  author        = {Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Dollár},
  title         = {Microsoft COCO: Common Objects in Context},
  eprint        = {1405.0312},
  archiveprefix = {arXiv},
  comments      = {This dataset compares well to other image classification and object detection datasets in that it has more categories and more examples than Pascal VOC and a better balance of number of examples of object per image than ImageNet and Pascal VOC and also has more examples of small objects than ImageNet and Pascal VOC.},
  keywords      = {datasets, MLStrat Data},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-01-08},
  year          = {2015},
}

@Article{OzturkSZ2020,
  author    = {Ozan Öztürk and Batuhan Sariturk and Zafer Seker, Dursun},
  title     = {Comparison of Fully Convolutional Networks (FCN) and U-Net for Road Segmentation from High Resolution Imageries},
  doi       = {10.30897/ijegeo.737993},
  issue     = {3},
  pages     = {272--279},
  volume    = {7},
  comment   = {took U-Net and FCN architectures and trained them on their own data for segmenting roads. Results a roughly equivalent between the two approaches and seem more dependant on the input image size.},
  journal   = {International Journal of Environment and Geoinformatics},
  owner     = {ISargent},
  temp      = {September 2020 7(3): Follow journal DOI:},
  creationdate = {2021-01-08},
  year      = {2020},
}

@Article{HeGDG2018,
  author        = {Kaiming He and Georgia Gkioxari and Piotr Dollár and Ross Girshick},
  title         = {Mask R-CNN},
  eprint        = {1703.06870},
  url           = {https://arxiv.org/abs/1703.06870},
  archiveprefix = {arXiv},
  comments      = {Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. For each region proposal, a segmentation mask is predicted for each class using a FCN LongSD2015.},
  keywords      = {deep learning, segmentation, MLStrat Segmentation},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-01-08},
  year          = {2018},
}

@Article{LinGGHD2018,
  author        = {Tsung-Yi Lin and Priya Goyal and Ross Girshick and Kaiming He and Piotr Dollár},
  title         = {Focal Loss for Dense Object Detection},
  eprint        = {1708.02002},
  url           = {https://arxiv.org/abs/1708.02002},
  archiveprefix = {arXiv},
  comments      = {The RetinaNet paper. Combines ResNet with FPN LinDGHHB2017 for segmentation.},
  keywords      = {deep learning, segementation, MLStrat Segementation},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-01-08},
  year          = {2018},
}

@Article{LinDGHHB2017,
  author        = {Tsung-Yi Lin and Piotr Dollár and Ross Girshick and Kaiming He and Bharath Hariharan and Serge Belongie},
  title         = {Feature Pyramid Networks for Object Detection},
  eprint        = {1612.03144},
  archiveprefix = {arXiv},
  comments      = {Feature Pyramid Network FPN paper. This is used in RetinaNet LinGGHD2018. ``we rely on an architecture that combines low-resolution, semantically strong features with high-resolution, semantically weak features via a top-down pathway and lateral connections''. ``The bottom-up pathway is the feedforward computation of the backbone ConvNet'' they use a ResNet. ``The topdown pathway hallucinates higher resolution features by upsampling spatially coarser, but semantically stronger, feature maps from higher pyramid levels.''},
  creationdate  = {2021-01-08},
  keywords      = {deep learning, object detection, MLStrat Object},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  year          = {2017},
}

@Article{LiuAESRFB2016,
  author       = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  title        = {SSD: Single Shot MultiBox Detector},
  doi          = {10.1007/978-3-319-46448-0_2},
  issn         = {1611-3349},
  pages        = {21-37},
  url          = {http://dx.doi.org/10.1007/978-3-319-46448-0_2},
  comments     = {Uses a CNN's pyramidal feature hierarchy as if it were a featurized image pyramid ``We add convolutional feature layers to the end of the truncated base network. These layers decrease in size progressively and allow predictions of detections at multiple scales. The convolutional model for predicting detections is different for each feature layer (cf Overfeat[4] and YOLO[5] that operate on a single scale feature map).''},
  creationdate = {2021-01-08},
  isbn         = {9783319464480},
  journal      = {Lecture Notes in Computer Science},
  keywords     = {deep learning, scale, Object Detection, MLStrat Object},
  owner        = {ISargent},
  publisher    = {Springer International Publishing},
  year         = {2016},
}

@Article{LiuRB2015,
  author        = {Wei Liu and Andrew Rabinovich and Alexander C. Berg},
  title         = {ParseNet: Looking Wider to See Better},
  eprint        = {1506.04579},
  url           = {https://arxiv.org/abs/1506.04579},
  archiveprefix = {arXiv},
  comment       = {built on, and improve on FCN, by incorporating global average pooling to provide global context to aid segmentation.},
  keywords      = {deep learning, segmentation, MLStrat Segmentation},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-01-08},
  year          = {2015},
}

@Article{BadrinarayananKC2016,
  author        = {Vijay Badrinarayanan and Alex Kendall and Roberto Cipolla},
  title         = {SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation},
  eprint        = {1511.00561},
  archiveprefix = {arXiv},
  comment       = {Important paper from Cambridge. Much more efficient that similar approaches (U-Net, DeconveNet). Backbone of VGG Whereas U-Net brings whole feature map over skip connections, SegNet just brings the pooling indices (which encode the position of the max value before pooling) and doesn't have the FC layers that DeconvNet has meaning it has fewer parameters to train.},
  keywords      = {deep learning, segmentation, MLStrat Segementation},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-01-08},
  year          = {2016},
}

@InProceedings{NohHH2015,
  author    = {Hyeonwoo Noh and Seunghoon Hong and Bohyung Han},
  booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
  title     = {Learning Deconvolution Network for Semantic Segmentation},
  doi       = {10.1109/ICCV.2015.178},
  pages     = {1520-1528},
  comments  = {The DeconvNet paper. Built on VGG. Convolutions down to 2 fully connected layers and deconvolution network mirrors in shape. Uses Zeiler and Fergus approach to deconvolution layers of retaining the location of the max value at pooling -},
  keywords  = {deep learning, segmentation, MLStrat Segementation},
  owner     = {ISargent},
  creationdate = {2021-01-08},
  year      = {2015},
}

@Article{YuanSG2021,
  author    = {Xiaohui Yuan and Jianfang Shi Lichuan Gu},
  date      = {1 May 2021},
  title     = {A review of deep learning methods for semantic segmentation of remote sensing imagery},
  url       = {https://www.sciencedirect.com/science/article/pii/S0957417420310836},
  volume    = {169},
  comments  = {Comprehensive review of image segmentation techniques applied to RS. Useful for a list of deep learning for RS papers and summaries of their different approaches. Seems to consider pixel level accucary more important than I would. Other papers have built their approaches on RCN, SegNet, U-Net DeepLab, DenseNet, ShuffleNet, DeconvNet},
  journal   = {Expert Systems with Applications},
  keywords  = {deep learning, segmentation, remote sensing, MLStrat Segmentation},
  owner     = {ISargent},
  creationdate = {2021-01-08},
}

@Article{SchmidtLMBSCB2019,
  author        = {Victor Schmidt and Alexandra Luccioni and S. Karthik Mukkavilli and Narmada Balasooriya and Kris Sankaran and Jennifer Chayes and Yoshua Bengio},
  title         = {Visualizing the Consequences of Climate Change Using Cycle-Consistent Adversarial Networks},
  eprint        = {1905.03709},
  url           = {https://arxiv.org/abs/1905.03709},
  archiveprefix = {arXiv},
  comments      = {Bengio's paper on using GANs to show what climate change looks like - e.g. your house flooded},
  keywords      = {deep learning, climate, MLStrat Unsupervised, Environment},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  priority      = {prio1},
  creationdate     = {2021-01-08},
  year          = {2019},
}

@Article{KarrasLAHLA2020,
  author        = {Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},
  title         = {Analyzing and Improving the Image Quality of StyleGAN},
  eprint        = {1912.04958},
  archiveprefix = {arXiv},
  comments      = {The StyleGAN paper that resulted in https://thispersondoesnotexist.com/},
  keywords      = {deep learning, MLStrat Unsupervised},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-01-08},
  year          = {2020},
}

@Online{SemanticSegmentationDatasets,
  author    = {{Papers With Code Website}},
  title     = {Semantic Segmentation},
  url       = {https://paperswithcode.com/task/semantic-segmentation},
  urldate   = {2021-01-09},
  comments  = {This is the page with the summaries of all papers with code that are applied to semantic segmentation. THE place to go to find a list of approaches to semantic segmentation},
  keywords  = {segmentation, MLStrat Segmentation},
  owner     = {ISargent},
  creationdate = {2021-01-08},
}

@Article{ZhangSPLGHA2018,
  author    = {Zhang, Ce and Sargent, Isabel and Pan, Xin and Li, Huapeng and Gardiner, Andy and Hare, Jonathon and Atkinson, Peter M},
  title     = {An object-based convolutional neural network (OCNN) for urban land use classification},
  pages     = {57--70},
  volume    = {216},
  journal   = {Remote sensing of environment},
  keywords  = {izzypub, MLStrat},
  owner     = {ISargent},
  publisher = {Elsevier},
  creationdate = {2021-01-15},
  year      = {2018},
}

@InProceedings{SwopeRS2020,
  author    = {Aidan M. Swope and Xander H. Rudelis and Kyle T. Story},
  booktitle = {ICLR 2020},
  title     = {Representation Learning for Remote Sensing: An Unsupervised Sensor Fusion Approach},
  url       = {https://openreview.net/forum?id=SJlVn6NKPB},
  comment   = {Use InfoNCE (vandenoordLV2019), DeepInfoMax, related to Contrastive pRedictive Coding to pretraining networks.},
  keywords  = {deep learning, remote sensing, unsupervised, pretraining, MLStrat DLRS},
  owner     = {ISargent},
  creationdate = {2021-01-09},
  year      = {2020},
}

@Article{LongXLYYZZL2020,
  author        = {Yang Long and Gui-Song Xia and Shengyang Li and Wen Yang and Michael Ying Yang and Xiao Xiang Zhu and Liangpei Zhang and Deren Li},
  title         = {DiRS: On Creating Benchmark Datasets for Remote Sensing Image Interpretation},
  eprint        = {2006.12485},
  url           = {https://arxiv.org/abs/2006.12485},
  abstract      = {The past decade has witnessed great progress on remote sensing (RS) image interpretation and its wide applications. With RS images becoming more accessible than ever before, there is an increasing demand for the automatic interpretation of these images, where benchmark datasets are essential prerequisites for developing and testing intelligent interpretation algorithms. After reviewing existing benchmark datasets in the research community of RS image interpretation, this article discusses the problem of how to efficiently prepare a suitable benchmark dataset for RS image analysis. Specifically, we first analyze the current challenges of developing intelligent algorithms for RS image interpretation with bibliometric investigations. We then present some principles, i.e., diversity, richness, and scalability (called DiRS), on constructing benchmark datasets in efficient manners. Following the DiRS principles, we also provide an example on building datasets for RS image classification, i.e., Million-AID, a new large-scale benchmark dataset containing million instances for RS scene classification. Several challenges and perspectives in RS image annotation are finally discussed to facilitate the research in benchmark dataset construction. We do hope this paper will provide RS community an overall perspective on constructing large-scale and practical image datasets for further research, especially data-driven ones.},
  archiveprefix = {arXiv},
  comment       = {assess remote sensing benchmark data and note that:

The ever-growing volume of RS images is acquired while very few of them are annotated with valuable information

Representative and large-scale RS image datasets with accurate annotations are demanded to narrow the gap between algorithm development and real applications.},
  keywords      = {deep learning, datasets, remote sensing, MLStrat DLRS},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-01-09},
  year          = {2020},
}

@Online{ImageClassificationDatasets,
  author    = {{Papers With Code Website}},
  title     = {Image Classification},
  url       = {https://paperswithcode.com/task/image-classification},
  urldate   = {2021-01-09},
  comments  = {This is the page with the summaries of all papers with code that are applied to image classification. THE place to go to find a list of approaches to image classification},
  keywords  = {image classification, MLStrat DLRS},
  owner     = {ISargent},
  creationdate = {2021-01-08},
}

@Online{ObjectDetectionDatasets,
  author    = {{Papers With Code Website}},
  title     = {Object Detection},
  url       = {https://paperswithcode.com/task/object-detection},
  urldate   = {2021-01-09},
  comments  = {This is the page with the summaries of all papers with code that are applied to Object Detection. THE place to go to find a list of approaches to Object Detection},
  keywords  = {Object Detection, MLStrat DLRS},
  owner     = {ISargent},
  creationdate = {2021-01-09},
}

@InProceedings{MurraySHGDCHA2020,
  author       = {Murray, Jon and Sargent, Isabel and Holland, David and Gardiner, A. and Dionysopoulou, Kyriaki and Coupland, S. and Hare, Jonathon and Atkinson, P. M.},
  booktitle    = {XXIV ISPRS Congress},
  title        = {Opportunities for machine learning and artificial intelligence in a national mapping agency: a perspective on enhancing ordnance survey workflow},
  pages        = {185--189},
  volume       = {XXIV},
  keywords     = {MLStrat, izzypub},
  organisation = {International Society for Photogrammetry and Remote Sensing},
  owner        = {ISargent},
  creationdate    = {2021-01-09},
  year         = {2020},
}

@Article{JeanWSALE2018,
  author        = {Neal Jean and Sherrie Wang and Anshul Samar and George Azzari and David Lobell and Stefano Ermon},
  title         = {Tile2Vec: Unsupervised representation learning for spatially distributed data},
  eprint        = {1805.02855},
  url           = {https://arxiv.org/abs/1805.02855},
  archiveprefix = {arXiv},
  comments      = {The Tile2Vec paper. Use triplet loss (HofferA2015) approach but have nearby images being positive examples of the anchor and distant examples being negative examples (as HofferA2015 suggested).},
  keywords      = {deep learning, remote sensing, unsupervised, MLStrat DLRS},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-01-11},
  year          = {2018},
}

@Article{ZhangPLGSHA2018,
  author    = {Zhang, Ce and Pan, Xin and Li, Huapeng and Gardiner, Andy and Sargent, Isabel and Hare, Jonathon and Atkinson, Peter M},
  title     = {A hybrid MLP-CNN classifier for very fine resolution remotely sensed image classification},
  pages     = {133--144},
  volume    = {140},
  journal   = {ISPRS Journal of Photogrammetry and Remote Sensing},
  keywords  = {izzypub, MLStrat},
  owner     = {ISargent},
  publisher = {Elsevier},
  creationdate = {2021-01-15},
  year      = {2018},
}

@InProceedings{SargentHYWDHA2017,
  author       = {Sargent, Isabel and Hare, Jonathon and Young, David and Wilson, Olivia and Doidge, Charis and Holland, David and Atkinson, Peter M},
  booktitle    = {International Conference on Innovative Techniques and Applications of Artificial Intelligence},
  title        = {Inference and discovery in remote sensing data with features extracted using deep networks},
  organization = {Springer, Cham},
  pages        = {131--136},
  keywords     = {izzypub, MLStrat},
  owner        = {ISargent},
  creationdate    = {2021-01-15},
  year         = {2017},
}

@InProceedings{KramerHPS2017,
  author           = {Kramer, Iris and Hare, Jonathon and Prugel-Bennett, Adam and Sargent, Isabel and others},
  booktitle        = {New Forest Knowledge Conference 2017: New Forest Historical Research and Archaeology: Who's doing it?},
  date             = {2017},
  title            = {Automated detection of archaeology in the New Forest using deep learning with remote sensor data},
  location         = {Lyndhurst, United Kingdom},
  creationdate     = {2021-01-15},
  keywords         = {izzypub, MLStrat},
  modificationdate = {2023-06-23T16:54:34},
  owner            = {ISargent},
  year             = {2017},
}

@Article{ZhangSPGHA2018,
  author    = {Zhang, Ce and Sargent, Isabel and Pan, Xin and Gardiner, Andy and Hare, Jonathon and Atkinson, Peter M},
  title     = {VPRS-based regional decision fusion of CNN and MRF classifications for very fine resolution remotely sensed images},
  number    = {8},
  pages     = {4507--4521},
  volume    = {56},
  journal   = {IEEE Transactions on Geoscience and Remote Sensing},
  keywords  = {izzypub, MLStrat},
  owner     = {ISargent},
  publisher = {IEEE},
  creationdate = {2021-01-15},
  year      = {2018},
}

@Patent{SargentH2020toponetpatent,
  author    = {Sargent, Isabel and Hare, Jonathon},
  title     = {Topographic data machine learning method and system},
  note      = {US Patent 10,586,103},
  keywords  = {izzypub, MLStrat},
  month     = {3},
  owner     = {ISargent},
  creationdate = {2021-01-15},
  year      = {2020},
}

@Article{ZhangSPLGHA2019,
  author           = {Zhang, Ce and Sargent, Isabel and Pan, Xin and Li, Huapeng and Gardiner, Andy and Hare, Jonathon and Atkinson, Peter M},
  title            = {Joint Deep Learning for land cover and land use classification},
  pages            = {173--187},
  volume           = {221},
  creationdate     = {2021-01-15},
  journal          = {Remote sensing of environment},
  keywords         = {izzypub, MLStrat},
  modificationdate = {2022-05-08T15:48:06},
  owner            = {ISargent},
  publisher        = {Elsevier},
  year             = {2019},
}

@Patent{Sargent2020contextpatent,
  author    = {Sargent, Isabel Melanie Jane},
  title     = {Topographical contextual grouping},
  note      = {US Patent 10,747,790},
  keywords  = {izzypub, MLStrat},
  month     = {8},
  owner     = {ISargent},
  creationdate = {2021-01-15},
  year      = {2020},
}

@Article{zhang2020scale,
  author    = {Zhang, Ce and Harrison, Paula A and Pan, Xin and Li, Huapeng and Sargent, Isabel and Atkinson, Peter M},
  title     = {Scale Sequence Joint Deep Learning (SS-JDL) for land use and land cover classification},
  pages     = {111593},
  volume    = {237},
  journal   = {Remote Sensing of Environment},
  keywords  = {izzypub, MLStrat},
  owner     = {ISargent},
  publisher = {Elsevier},
  creationdate = {2021-01-15},
  year      = {2020},
}

@Patent{sargent2020joint,
  author    = {Sargent, Isabel and Zhang, Ce and Atkinson, Peter M},
  title     = {Joint Deep Learning for Land Cover and Land Use Classification},
  note      = {US Patent App. 16/549,216},
  keywords  = {izzypub, MLStrat},
  month     = {2},
  owner     = {ISargent},
  creationdate = {2021-01-15},
  year      = {2020},
}

@Patent{sargent2020object,
  author    = {Sargent, Isabel and Zhang, Ce and Atkinson, Peter M},
  title     = {Object-based Convolutional Neural Network for Land Use Classification},
  note      = {US Patent App. 16/156,044},
  keywords  = {izzypub, MLStrat},
  month     = {4},
  owner     = {ISargent},
  creationdate = {2021-01-15},
  year      = {2020},
}

@Book{Behrens2020,
  author           = {Paul Behrens},
  title            = {The Best of Times the Worst of Times},
  publisher        = {The Indigo Press},
  subtitle         = {Futures from the frontiers of climate science},
  comment          = {*Problems Each litre of petrol burnt in a new car melts over a tonne of glacial ice p29 In general, high-income nations use around ten calories of fossil energy to produce one calorie of food energy p76 The total number of deaths globally each year from pollution are fifteen times more than all wars and other violent deaths combined. p82-83 A recent World Health Organisation report found that the cost of meeting climate-change ambitions is entirely outweighed by the health benefits of cleaner air p83 The overall impact of burning existing reserves [of fossil fuels] alone would raise sea levels by 58m p86 Keeping existing nuclear online is important, but new plants will struggle. That's not to say that we shouldn't even consider nuclear power- although social resistance and the long-term storage of waste are significant issues - but nuclear simply struggle to compete economically. New reactors in the UK and the US have repeatedly blown their budgets, and it's public money that has kept them going p97 All clean technologies are important, but wind and solar are simply in a league of their own. With existing technologies, wind alone could produce ten times our current fossil use, solar fifteen times p98 A petrol car is a spectacularly wasteful way to move people around - they lose between 68percent and 76percent of the energy in the fuel, compared to just 16percent for an electric vehicle. Not owning a car is better still, since cars need energy and materials to be built, they spend 92percent of their lives parked, and they consume between 20percent and 50percent of urban land p102 Between [400,000] electric buses, the closure of coal power plants near urban centres and the electrification of homes the air quality in China is improving p103 Spanish and German governments have decided to fund transition programmes for regions dependent on coal, phasing them out while finding new work for people who lose their jobs p108 Cattle farming alone was found to be the largest driver of bird biodiversity loss globally from 200 to 2011. 80percent of Amazonian deforestation is to make way for cattle ranching. 80percent of soy grown in the Amazon is for animal feed. Our taste for hamburgers and other cattle products may quite literally result in the death of the Amazon p125 Although we can rear animals in hundreds of different ways, some of the most environmentally friendly beef released six times more greenhouse gases and uses thirty-six times more land than the least friendly plant proteins p133 Each kilogram of carbon dioxide melts around 650 kilograms of glacier. That's around 1,200 tonnes of ice melted for your London-New York return flight, or 1.9 tonnes of ice for every day one person eats a meaty diet over a vegetarian one p158 <Listing of heatwaves, droughts, flooding and extreme precipitation events that have been probabilistically attributed to climate change - useful list for even examples> p160-163 In poorer regions, the individual suffering will continue to be underreported in a the global press, while other stories may never be linked to climate breakdown in the first place...violent attacks, diseases, suicides and other community stressors may not be identifiable as climate-related but...its fingerprints are everywhere. It also acts as a threat multiplier: an additional stress that can push already stressed societies over the brink...civil war in Syria...was exacerbated by a drought...that was made two to three times more likely by climate change p163-164 Microbes once dormant in the soils are reanimating...Recent history is already coming back to haunt communities as anthrax from dead cattle and reindeer graveyards re-enters the ecosystem. A release in 2016, during record high temperatures, resulted in the hospitalization of ninety-six people and the death of a twelve year-old boy. There are more than 13,500 of these graveyards dotted across Russia alone p165 Scientific articles are increasingly describing what this might look like, A late 2018 paper entitled 'Broad Threat to Humanity from Cumulative Climate Hazards Intensified by Greenhouse Gas Emissions' [https://www.nature.com/articles/s41558-018-0315-6] found 467 pathways by which 'human health, water, food, economy, infrastructure and security have been recently impacted by climate hazards'. P169 *Solutions ...five recognizable stages: 1. we make some awful mistake in ignorance, hubris or callousness...2. Research identify the problem and determine how serious it is. 3. A consensus is reached that something must be done. 4. An interregnum sets in, where politically influential vested interested delay action... 5. Given enough damage and public outrange, there's a concerted attempt to clean up the mistake p174 A meat tax of 20\% could save hundreds of thousands of lives globally each year (and save 14percent of total health costs p134 Taken together, reducing meat consumption is probably the single best individual step we can make for a liveable world, a healthier body and a cleaner conscience p134 What does a world without animal products look like? Land use for agriculture is cut by...75percent...rewilding...a world of wilderness, of parks, of walking and biking trails...biodiversity and draws down more carbon...antibiotics last longer...millions of lives are saved from illnesses like colorectal cancer and cardiovascular disease...water health is dramatically improved, fish populations recover, more rivers are swimable...cities...no longer...spend millions to remove the additional nutrients and effluents of intensive animal farms from drinking water p134 The richest 10percent of the world's population are responsible for around 50percent of global carbon emissions, while the poorest half of the world's population are responsible for just 10percent of total emissions. Put differently, if the top 10percent of earners worldwide reduced their consumption to the level of the average European, global emission would be cut by a third p45 Project Drawdown - a project to map out the most beneficial solutions for addressing climate breakdown - estimates that improving women's education and family planning together would range ahead of ninety-nine other climate solutions, above wind energy, plant-based diets and reductions in flying (though these are essential too) p60 ...the economic case for migrations appears to be unassailable...net positive economic outcomes from migrations. Migrants pay more tax, make use of fewer public services and spur innovation. By one estimate, every 1percent increase in migrant population results in a 2percent increase in national wages...even in the short term and even for low-skilled workers, there is no negative economic impact from migration...decline in welfare of low-skilled workers of the past few years is almost entirely down to automation, 'outsourcing' or the offshoring of work, and increasing inequality...in EU...800,000 people need each year just to keep the populations stable p64-65 Migrant remittances to lower-income nations are four times larger than the entire global foreign aid budget...often used to pay for education, healthcare and to alleviate food shortages, especially during droughts. The extra cash can allow girls to remain in education longer, reduces child labour, and improves the change of finding a good job p66 A non-exhaustive list includes: strict building standards...mandatory energy-efficiency building retrofits...frequent flyer levy...bans on road and airport expansion; bans on new development of fossil fuel resources; low-cost effective (electric) public transport; nationwide bike infrastructure including ...(e)bike purchasing programmes; urban renovation including rezoning, pedestrianized areas...priority to reduce private car traffic...national and international coordination of electricity grids...tax on meat and dairy; increased research and development funding for energy and agricultural solutions...removing subsidies for fossil fuels and damaging agricultural practices; a price on carbon for domestically produced goods and the carbon embodies in imports... P183-184 Putting policies in place to protect people from these impacts will be as important as the solutions themselves. If not, the decarbonization agenda could be derailed by public oppositions and sociopolitical upheaval p184 Enacting every national carbon solution currently available to us at their maximum capacity would get us around halfway to net zero by 2030, if emissions remain at 2019 levels p194 ...negative-emission technologies...would be a vast effort...running any system backwards is an affront to the second law of thermodynamics...Under some scenarios, we'd have to build several times the current infrastructure of the global fossil field industry to scrub out the necessary quantity of carbon. P195 BECCS needs huge volumes of water to grow the plants and to operate the power plants ...also suffer from the same air pollution and ash disposal problems of other combustion technologies p196 If we confuse GDP with wealth...then we are blind to much of what makes life worth living...We can't purchase a safe and sustainable future on the open market - it has not price. As Robert Kennedy famously remarked, GDP is 'a measure of everything except that which is worthwhile'. P204 Hubristic as it may be, a framework exists is to assess how much nature is 'worth' to humans in terms of the services it provides, called ecosystem services... One study in 1996 [estimated value]for Earth as a whole...found that the total value was very roughly $60 trillion - around the same as the global GDP at the time...but it undoubtedly befuddles our sense of reality...say we did 'cash in' that $60 trillion, then we'd have no ecosystem left. No life, no clean, water, nor soil, nor air! P207 Genuine Progress Indicator (GPI)...across seventeen high-income countries, economic well-being has ceased to increased with economic growth..well-being as measured by GPI peaked in the latter 1970s p209 ...the Easterlin Paradox...short-term GDP growth appears to correlate with a short-term growth in happiness...long-term economic growth has no correlation with long-term happiness. p210 Three countries (China, South Korea and Chile) grew so quickly that ... 'you'd expect dancing in the streets' - they doubled GDP in under ten, thirteen and eighteen years respectively - yet there was no statistically significant increase in happiness p210 Since BECCS has been on the table, it's been all the rage in government net-zero emission reports. In the UK government's response to the Committee on Climate Change's 2019 Progress Report, it mentions carbon capture and sequestration around five times as much as solar power p222 While [Solar Radiation Management] would cool the planet relative to no SRM, it wouldn't wind back the climate clock to 'safe' pre-industrial conditions p254 The same institutions that have to fix this are the institutions that have failed to managed many social and technological transitions over the years. Institutions that have allowed for a fantastic level of wealth accumulation - the wealthiest 500 people gained $1.2 trillion in 2019, increasing their net worth 25percent to $5.9 trillion (remember that the necessary global climate action would cost in the order of $1 trillion a year) p256-257 There will have to be big changes in personal philosophies to enable sufficient social license for these great transitions to continue as deeply as they need to go. Societies have laboured under misapprehensions of fundamental human behaviour for too long - be it the rational actor assumption...or the idea that humans prefer high levels of discounting used to shrug off the future p264 Surveys show that 85percent of people are worried about warming in the UK, with similar levels of concern in other countries p266},
  creationdate     = {2021-01-22},
  keywords         = {Environment, Climate},
  modificationdate = {2022-01-06T14:26:47},
  owner            = {ISargent},
  priority         = {prio1},
  year             = {2020},
}

@InProceedings{ChenH2021,
  author           = {Xinlei Chen and Kaiming He},
  booktitle        = {ICML},
  title            = {Exploring Simple Siamese Representation Learning},
  eprint           = {2011.10566},
  url              = {https://arxiv.org/abs/2011.10566},
  archiveprefix    = {arXiv},
  comment          = {The SimSiam paper. Comparison with SimCLR ChenKNH2020 and other approaches to siamese network trainining. Finds that stop-gradient (not backpropogating the gradient over one encoder) is essential to prevent the solution collapsing to a constant value output (this is refuted in ZhangZZPYK2022). Useful paper for describing other Siamese approaches.},
  creationdate     = {2021-01-28},
  modificationdate = {2022-05-03T06:14:08},
  owner            = {ISargent},
  primaryclass     = {cs.CV},
  year             = {2021},
}

@Article{KettunenMALDZ2015,
  author     = {Kettunen, Markus and Manzi, Marco and Aittala, Miika and Lehtinen, Jaakko and Durand, Fr\'{e}do and Zwicker, Matthias},
  title      = {Gradient-Domain Path Tracing},
  doi        = {10.1145/2766997},
  issn       = {0730-0301},
  number     = {4},
  url        = {https://doi.org/10.1145/2766997},
  volume     = {34},
  abstract   = {We introduce gradient-domain rendering for Monte Carlo image synthesis. While previous gradient-domain Metropolis Light Transport sought to distribute more samples in areas of high gradients, we show, in contrast, that estimating image gradients is also possible using standard (non-Metropolis) Monte Carlo algorithms, and furthermore, that even without changing the sample distribution, this often leads to significant error reduction. This broadens the applicability of gradient rendering considerably. To gain insight into the conditions under which gradient-domain sampling is beneficial, we present a frequency analysis that compares Monte Carlo sampling of gradients followed by Poisson reconstruction to traditional Monte Carlo sampling. Finally, we describe Gradient-Domain Path Tracing (G-PT), a relatively simple modification of the standard path tracing algorithm that can yield far superior results.},
  address    = {New York, NY, USA},
  articleno  = {123},
  comment    = {Path tracing attempts to model all the possible light paths in a scene to recreate it more realistically. This paper seems to use the gradient domain to help guide the monte carlo sampling to parts of the scene that need more focus (or something). Potentially intersting to Incivie Tagging in that it may be useful to find more salient parts of image (hmm).},
  issue_date = {August 2015},
  journal    = {ACM Trans. Graph.},
  keywords   = {gradient-domain, path tracing, global illumination, light transport},
  month      = {7},
  numpages   = {13},
  owner      = {ISargent},
  publisher  = {Association for Computing Machinery},
  creationdate  = {2021-01-28},
  year       = {2015},
}

@Article{DiakonikolasK2019,
  author        = {Ilias Diakonikolas and Daniel M. Kane},
  title         = {Recent Advances in Algorithmic High-Dimensional Robust Statistics},
  eprint        = {1911.05911},
  url           = {https://arxiv.org/abs/1911.05911},
  archiveprefix = {arXiv},
  comment       = {A review document looking at statiscal approaches to learning in the presence of outliers particularly in high dimensional space - for instance the emprical mean can be very different to the true mean with an outlier. I haven't read this but it could be a useful resouce, not least for describing the problem statistically.},
  keywords      = {learning, data, outliers, theoretical models, sample complexity},
  owner         = {ISargent},
  primaryclass  = {cs.DS},
  creationdate     = {2021-01-28},
  year          = {2019},
}

@Report{AICouncil2021,
  author           = {{The AI Council}},
  date             = {2021-01-06},
  institution      = {Office for Artificial Intelligence, Department for Business, Energy \& Industrial Strategy, and Department for Digital, Culture, Media \& Sport},
  title            = {AI Roadmap},
  eprint           = {https://assets.publishing.service.gov.uk/media/5ff3bc6e8fa8f53b76ccee23/AI_Council_AI_Roadmap.pdf},
  url              = {https://www.gov.uk/government/publications/ai-roadmap},
  abstract         = {The AI Council is an independent expert committee. It provides advice to the UK Government, as well as high-level leadership of the Artificial Intelligence (AI) ecosystem. This independent report draws on the expertise of its members and those in its wider ecosystem to summarise four pillars on which to build the UK's future in AI. It invites action across government to keep the UK at the forefront of safe and responsible AI.},
  comment          = {4 pillars:
1. Research, Development and Innovation
2. Skills and Diversity
3. Data, Infrastructure and Public Trust
4. National, Cross-sector Adoption

Not a ``draft strategy'' or ``instruction manual''.
16 recommendations (these are excellent)

``'confidence will depend on the existence of systems that ensure full accountability, clear ethics and transparency''

''The Council calls for a UK National AI Strategy that creates an even stronger general base of support for AI. Scotland has already embarked on an AI Strategy and the UK strategy should be created in consultation with the devolved administrations and the wider ecosystem.''

``Properly designed and delivered moonshot programmes play to AI's strengths by requiring people to work across boundaries and existing organisational structures, and to build new relationships, networks and common languages in order to develop entirely new solutions to big challenges.''

``One such moonshot ... developing and establishing appropriate methods for safe, ethical, explainable and reproducible AI which will accelerate its use across many sectors.''

``every child leaves school with a basic sense of how AI works ... knowing enough to be a conscious and confident user of AI-related products''

``Over time, AI needs to be built into the curriculum as a specialist subject. As well as being its own subject AI needs to be part of computer science, citizenship studies, and as part of new ways of doing other subjects such as geography or history''

``available to young people of all backgrounds, ensuring that the UK's future AI workforce reflects the diversity of background and thought needed to develop world leading AI capabilities that tackle the issues which matter most''

``need for significant change in the contract between the state, individual and employer and a step change in the scale of continuous professional development and the ability to move between types of work''

``new curriculum resources need to include curated data sets that will enable teachers to create the case studies and exercises that will in turn create the familiarity that leads all young people to increased confidence and data literacy''

``more work is needed to help businesses seeking to use data for AI by creating the conditions for the deployment of suitable privacy enhancing technologies''

``work is needed to build a common language amongst data practitioners''

``Developing and deploying trustworthy AI will be dependent on the UK strengthening its governing environment in a manner that both provides guidance and confidence to businesses to innovate and adopt AI, and reassures the public that the use of AI is safe, secure, fair, ethical and duly overseen by independent entities. This is beginning to be addressed in the National Data Strategy...''

``algorithmic impact assessments''

``fully consider the implications of AI in areas such as labour, environmental, and criminal law. These three tenets: (1) clear transparency about automated decision making, (2) the right to give meaningful public input and (3) the ability to enforce sanctions could be encapsulated in a Public Interest Data Bill''

Some stuff on digital twins

Topics for National, Cross-sector Adoption:
Business as smart adopters
Supporting high growth AI startups
Enabling public sector adoption
Health and social care
Climate change
Defence and security

``Threaded throughout discussions about AI is the need for adaptive and informed regulation and standards as part of an enabling environment of good governance,which includes public engagement,transparency measures and responsible research and innovation on the part of businesses through the lifecycle of AI operations''},
  creationdate     = {2021-02-01},
  keywords         = {AI, Ethics, Policy, Strategy, MLStrat},
  modificationdate = {2023-12-11T11:37:36},
  owner            = {ISargent},
  year             = {2021},
}

@Online{GCDSA2020,
  author       = {{Cabinet Office}},
  date         = {16 December 2020},
  title        = {Access to Geospatial Data - GOV.UK},
  url          = {https://www.gov.uk/government/publications/access-to-geospatial-data/access-to-geospatial-data},
  organization = {Geospatial Commission},
  titleaddon   = {Research and analysis},
  urldate      = {2021-02-04},
  comment      = {Data-sharing assessment tools considers:
	• Personal data
	• 3rd party IP
	• National security
	• Anti-competition
	• Existing licenses
	• Data quality
	• Ethics},
  keywords     = {AI, Ethics, Government, MLStrat},
  owner        = {ISargent},
  creationdate    = {2021-02-04},
}

@Online{DataEthicsFramework2020,
  author    = {{Government Digital Service}},
  date      = {2020-09-30T15:29:53.000+00:00},
  title     = {Data Ethics Framework - GOV.UK},
  url       = {https://www.gov.uk/government/publications/data-ethics-framework/data-ethics-framework-2020},
  urldate   = {2021-02-04},
  comment   = {Score a data set out of 5 according to criteria of: Transparency, Accountability, Fairness. Also, more specifically, score:
	• Public benefit and user need
	• Diverse expertise
	• Compliance with the law
	• Quality and limitations of data
	• Wider policy implications},
  keywords  = {AI, Ethics, Government, MLStrat},
  owner     = {ISargent},
  creationdate = {2021-02-04},
  year      = {2020},
}

@Online{GovAIEthics2019,
  author           = {{CDDO / OAI}},
  date             = {2019-06-10},
  title            = {Understanding artificial intelligence ethics and safety},
  url              = {https://www.gov.uk/guidance/understanding-artificial-intelligence-ethics-and-safety},
  organization     = {Central Digital and Data Office and Office for Artificial Intelligence},
  urldate          = {2021-02-04},
  comment          = {The main ways that AI systems can cause involuntary harm are misuse, questionable design or unintended negative consequences
The rest of this guidance document is a summary of Leslie2019 (Turing AI Ethics Framework).},
  creationdate     = {2021-02-04},
  keywords         = {AI, Ethics, Government, MLStrat},
  modificationdate = {2023-12-09T17:51:41},
  owner            = {ISargent},
}

@Report{IEEEEAD2019,
  author           = {{IEEE Standards Association}},
  institution      = {The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems},
  title            = {Ethically Aligned Design},
  subtitle         = {A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems},
  url              = {https://standards.ieee.org/content/ieee-standards/en/industry-connections/ec/
autonomous-systems.html},
  comment          = {has the most detailed discourse on ethics from many perspectives and provides a considerable set of recommendations, including for businesses. 

Pillars:
	• Universal human values
	• Political self-determination and data agency
	• Technical dependability
Ethically Aligned Design general principles:
	• Human rights
	• Well-being
	• Data agency
	• Effectiveness
	• Transparency
	• Accountability
	• Awareness of misuse
	• Competence},
  creationdate     = {2021-02-04},
  edition          = {First},
  keywords         = {AI, Ethics, MLStrat, EthicsWS},
  modificationdate = {2022-07-05T14:14:09},
  owner            = {ISargent},
  year             = {2019},
}

@Report{IEEEBusiness2020,
  author           = {{IEEE Standards Association}},
  date             = {2020-02-04},
  institution      = {The Institute of Electrical and Electronics Engineers, Incorporated (IEEE)},
  title            = {Ethically Aligned Design for Business},
  subtitle         = {A Call to Action forBusinesses Using AI},
  url              = {https://standards.ieee.org/content/dam/ieee-standards/standards/web/documents/other/ead/ead-for-business.pdf},
  comment          = {AI ETHICS READINESS FRAMEWORK for rating each of the following (lagging, basic, advanced, leading):
Internal training, support, and people resources
Leadership buy-in
Metrics and KPIs
Organizational impact},
  creationdate     = {2021-02-04},
  keywords         = {AI, Ethics, Business, MLStrat, EthicsWS},
  modificationdate = {2022-07-05T14:14:42},
  owner            = {ISargent},
}

@Online{HawesM2020,
  author           = {Ben Hawes and Denise McKenzie},
  date             = {12 October 2020)},
  title            = {Locus Charter},
  url              = {https://ethicalgeo.org/wp-content/uploads/2021/03/Locus_Charter_March21.pdf},
  organization     = {Benchmark Initiative, EthicalGeo},
  subtitle         = {(Draft v2.0)},
  urldate          = {2021-02-04},
  comment          = {Geospatial technologies are developing a''longside Artificial Intelligence, the Internet of Things, and Robotics. Users should be as well informed about risks as users of those technologies are increasingly expected to be ... Location data lends specific powers, which can imply specific responsibilities.

10 principles:
1. Realize opportunities
2. Understand impacts
3. Do no harm
4. Protect the vulnerable
5. Address bias
6. Minimize intrusion
7. minimize data
8. Protect privacy
9. Prevent identification of individuals
10. Provide accountability

7 phases to location data lifecycle:
At each change, you will need to consider the ethics of each decision, trade-off and interation and whether this may result in harm
1. Plan
2. Collect
3. Store / Security / Access
4. Verify
5. Process / Analysis
6. Publish / Visualise
7. After project / phases},
  creationdate     = {2021-02-04},
  keywords         = {location data, Ethics, MLStrat, EthicsWS},
  modificationdate = {2022-07-05T14:14:45},
  owner            = {ISargent},
  year             = {2020},
}

@Online{WilmottD2009,
  author       = {Paul Wilmott and Emanuel Derman},
  date         = {2009-01-07},
  title        = {Financial Modelers' Manifesto},
  url          = {https://corporatefinanceinstitute.com/resources/knowledge/finance/financial-modelers-manifesto/},
  comment      = {The Modelers' Hippocratic Oath reads as follows:

    “I will remember that I didn't make the world, and it doesn't satisfy my equations.”
    “Though I will use models boldly to estimate value, I will not be overly impressed by mathematics.”
    “I will never sacrifice reality for elegance without explaining why I have done so.”
    “Nor will I give the people who use my model false comfort about its accuracy. Instead, I will make explicit its assumptions and oversights.”
    “I understand that my work may have enormous effects on society and the economy, many of them beyond my comprehension.”},
  creationdate = {2021-02-04},
  keywords     = {MLStrat},
  owner        = {ISargent},
}

@Online{DasW2020,
  author    = {Shanti Das and Sophie Wilkinson},
  date      = {2019-12-22},
  title     = {Ordnance Survey axes its Jack the Ripper walks},
  url       = {https://www.thetimes.co.uk/article/ordnance-survey-axes-its-jack-the-ripper-walks-hpdl0xdxt},
  urldate   = {2021-02-04},
  keywords  = {ethics, MLStrat},
  owner     = {ISargent},
  creationdate = {2021-02-04},
}

@Online{GC2020,
  author           = {{Geospatial Commission}},
  date             = {2020-11-24},
  title            = {Enhancing the UK's Geospatial Ecosystem},
  url              = {https://www.gov.uk/government/publications/enhancing-the-uks-geospatial-ecosystem},
  organization     = {Cabinet Office�},
  urldate          = {2021-02-04},
  abstract         = {Outlines the actions needed to nurture the UK's growing geospatial economy, underpinned by an independent study into the current UK location data market.},
  comment          = {Report buildings on the UK's Geospatial Strategy (although I have not been able to develop a very clear, concrete idea of what it wants to achieve).

''the Geospatial Commions's way of working, being evidence-led, iterative, collaborative and open''.

The strategy identified 4 missions:
MIssion 1: Promote and safeguard the use of location data
Mission 2: Improve access to better location data
Mission 3: Enhance capabilities, skills and awareness
MIssion 4: Enable innovation

The geospatial data market study found that that geospatial market is best described as an ecosystem because of the diversity of products and services across multiple industries. It noted that the estimated £6 billion of turnover was underpinned by geospatial data is a conservative figure because it excludes many factors in the wider ecosystem such as revenues from large tech firms.

Foundational data products have mainly been delivered by the UK's six core geospatial public sector agencies. Yet new and emerging types of dynamic geospatial data are being created by private (and some public) sector organisations.

Identified 3 areas of action:
Improving access to location data
Maintaining public trust in how location data is used
Driving location data adoption

Quite a lot about the public good in location data - that good is achieved by giving access and so means are needed to overcome ethical barriers to giving access. 

In terms of trust, much of the comment is about building a conversation with the public and consumers about how data are used and the safeguards that are put in place. Mostly, there doesn't seem to be much recognition of ethical concerns beyond those around personal data.},
  creationdate     = {2021-02-04},
  keywords         = {strategy, geospatial, MLStrat, trust},
  modificationdate = {2023-12-11T08:04:15},
  owner            = {ISargent},
}

@Online{BS86112016,
  author           = {{British Standards Institute}},
  date             = {2016-04-01},
  title            = {BS 8611},
  url              = {https://shop.bsigroup.com/ProductDetail?pid=000000000030320089},
  subtitle         = {Guide to the Ethical Design of Robots and Robotic Systems},
  abstract         = {What is this standard about? 

BS 8611 gives guidelines for the identification of potential ethical harm arising from the growing number of robots and autonomous systems being used in everyday life.

The standard also provides additional guidelines to eliminate or reduce the risks associated with these ethical hazards to an acceptable level. The standard covers safe design, protective measures and information for the design and application of robots.

Who is this standard for? 

Robot and robotics device designers and managers
The general public 
Why should you use this standard?

BS 8611 was written by scientists, academics, ethicists, philosophers and users to provide guidance on specifically ethical hazards associated with robots and robotic systems and how to put protective measures in place. It recognizes that these potential ethical hazards have a broader implication than physical hazards, so it is important that different ethical harms and remedial considerations are considered.

The new standard builds on existing safety requirements for different types of robots, covering industrial, personal care and medical.},
  comment          = {Probably the first ethical framework in AI},
  creationdate     = {2021-02-04},
  keywords         = {ethics, AI, MLStrat, trust},
  modificationdate = {2023-12-11T07:58:52},
  owner            = {ISargent},
  year             = {2016},
}

@Online{ODICanvas2019,
  author           = {{Open Data Institute}},
  date             = {2019-07-03},
  title            = {ODI Data Ethics Canvas},
  url              = {https://theodi.org/article/data-ethics-canvas},
  urldate          = {2021-02-04},
  abstract         = {The Data Ethics Canvas is a tool for anyone who collects, shares or uses data.

It helps identify and manage ethical issues - at the start of a project that uses data, and throughout.

It encourages you to ask important questions about projects that use data, and reflect on the responses. These might be:

What is your primary purpose for using data in this project?
Who could be negatively affected by this project?
The Data Ethics Canvas provides a framework to develop ethical guidance that suits any context, whatever the project's size or scope.},
  comment          = {Its pretty, no idea how useful},
  creationdate     = {2021-02-04},
  keywords         = {ethics, data, MLStrat, EthicsWS},
  modificationdate = {2022-07-05T14:14:55},
  owner            = {ISargent},
  year             = {2019},
}

@Article{KeEBNR2021,
  author    = {Alexander Ke and William Ellsworth and Oishi Banerjee and Andrew Y. Ng and Pranav Rajpurkar},
  title     = {CheXtransfer: Performance and Parameter Efficiency of ImageNet Models for Chest X-Ray Interpretation},
  url       = {https://arxiv.org/abs/2101.06871},
  comment   = {Chest X-ray performance using pretrained networks. Often ImageNet trained networks of various architectures are used and assumption is that the better performance in pretraining the better performance on transfer to x-ray. However, this study finds that that is not the case. Pretraining is useful, but good performance on ImageNet may overfit to that dataset.},
  keywords  = {metrics, deep learning, transfer learning},
  owner     = {ISargent},
  creationdate = {2021-03-12},
}

@Article{DAmourEtAl2021,
  author           = {Alexander D'Amour and Katherine Heller and Dan Moldovan and Ben Adlam and Babak Alipanahi and Alex Beutel and Christina Chen and Jonathan Deaton and Jacob Eisenstein and Matthew D. Hoffman and Farhad Hormozdiari and Neil Houlsby and Shaobo Hou and Ghassen Jerfel and Alan Karthikesalingam and Mario Lucic and Yian Ma and Cory McLean and Diana Mincu and Akinori Mitani and Andrea Montanari and Zachary Nado and Vivek Natarajan and Christopher Nielson and Thomas F. Osborne and Rajiv Raman and Kim Ramasamy and Rory Sayres and Jessica Schrouff and Martin Seneviratne and Shannon Sequeira and Harini Suresh and Victor Veitch and Max Vladymyrov and Xuezhi Wang and Kellie Webster and Steve Yadlowsky and Taedong Yun and Xiaohua Zhai and D. Sculley},
  title            = {Underspecification Presents Challenges for Credibility in Modern Machine Learning},
  eprint           = {2011.03395},
  url              = {https://arxiv.org/abs/2011.03395},
  archiveprefix    = {arXiv},
  comment          = {Just because a model trains well doesn't mean it'll work well when deployed in different domain and the only way to find out which is to the best is to test all the options. 

Really good paper for comparing models for their ability to be used for transfer learning / feature extraction.

''We say that an ML pipeline is underspecified if there are many predictors f that a pipeline could return with similar predictive risk''

''underspecification creates ambiguity in the encoded structure of a predictor, which, in turn, affect the predictor's credibility. In particular, we are interested in behavior that is not tested by iid evaluations, but has observable implications in practically important situations''

apply three types of stress tests, or ``evaluations that probe a predictor by observing its outputs on specifically designed inputs'':
Stratified Performance Evaluations - testing against different subpopulations of the data
Shifted Performance Evaluations - testing against data that are different to the training data in some way
Contrastive Evaluations - does changing the input, change the output in expected ways?





From the Batch

Facing Failure to Generalize
The same models trained on the same data may show the same performance in the lab, and yet respond very differently to data they haven't seen before. New work finds this inconsistency to be pervasive.
What's new: Researchers explored this largely unexamined phenomenon, which they call underspecification. The team, led by Alexander D'Amour, Katherine Heller, and Dan Moldovan, spanned Google, MIT, Stanford, University of California San Diego, U.S. Department of Veterans Affairs, Aravind Eye Hospital, and Shri Bhagwan Mahavir Vitreo-Retinal Services.
Key insight: A well specified model pipeline — a model architecture, hyperparameters, training and test sets, and training procedure — should produce models that behave consistently. In practice, though, the same pipeline can produce many distinct models that achieve near-optimal performance, only some of which generalize to real-world conditions. Building a plethora of models and testing each one is the only way to know which is which.
How it works: The authors built many models per pipeline across a range of machine learning applications. Then they compared their performance on an appropriate test set and alternative data. The tests fell into three categories:
•	The authors probed whether models produced using the same pipeline performed equally well on particular subsets of a test set. For example, with vision models that were trained to recognize an eye disease, they compared performance on images taken by different cameras.
•	They compared performance on an established test set and a similar one with a different distribution. For instance, they compared the performance of ImageNet-trained models on both ImageNet and ObjectNet, which depicts some ImageNet classes from different angles and against different backgrounds.
•	They also compared performance on examples that were modified. For instance, using a model that was trained to evaluate similarity between two sentences, they switched genders, comparing the similarity of “a man is walking” and “a doctor is walking” versus “a woman is walking” and “a doctor is walking.”
Results: The authors found highly variable performance in models produced by identical model pipelines for several practical tasks in language, vision, and healthcare. For instance, they trained 50 ResNet-50 models on ImageNet using the same pipeline except for differing random seeds. On ImageNet's test set, the standard deviation from top-1 accuracy was 0.001. On ImageNet-C, which comprises corrupted ImageNet examples that are still recognizable to humans, the standard deviation was 0.024. A given model's performance on one dataset didn't correlate with its performance on the other.
Why it matters: If our models are to be useful and trustworthy, they must deliver consistent results. Underspecification is a significant barrier to that goal.
We're thinking: This work offers a helpful framework to evaluate the model performance on similar-but-different data. But how can we specify model pipelines to produce consistent models? We eagerly await further studies in this area.},
  creationdate     = {2021-03-12},
  keywords         = {metrics, deep learning, machine learning, transfer learning, trust},
  modificationdate = {2023-12-11T08:04:26},
  owner            = {ISargent},
  primaryclass     = {cs.LG},
  year             = {2020},
}

@Article{ijgi8050232,
  author         = {Anderson, Jennings and Sarkar, Dipto and Palen, Leysia},
  date           = {2019},
  journaltitle   = {ISPRS International Journal of Geo-Information},
  title          = {Corporate Editors in the Evolving Landscape of OpenStreetMap},
  doi            = {10.3390/ijgi8050232},
  issn           = {2220-9964},
  number         = {5},
  url            = {https://www.mdpi.com/2220-9964/8/5/232},
  volume         = {8},
  abstract       = {OpenStreetMap (OSM), the largest Volunteered Geographic Information project in the world, is characterized both by its map as well as the active community of the millions of mappers who produce it. The discourse about participation in the OSM community largely focuses on the motivations for why members contribute map data and the resulting data quality. Recently, large corporations including Apple, Microsoft, and Facebook have been hiring editors to contribute to the OSM database. In this article, we explore the influence these corporate editors are having on the map by first considering the history of corporate involvement in the community and then analyzing historical quarterly-snapshot OSM-QA-Tiles to show where and what these corporate editors are mapping. Cumulatively, millions of corporate edits have a global footprint, but corporations vary in geographic reach, edit types, and quantity. While corporations currently have a major impact on road networks, non-corporate mappers edit more buildings and points-of-interest: representing the majority of all edits, on average. Since corporate editing represents the latest stage in the evolution of corporate involvement, we raise questions about how the OSM community and researchers; might proceed as corporate editing grows and evolves as a mechanism for expanding the map for multiple uses.},
  article-number = {232},
  comment        = {References media articles about corporations who update OSM

''Bing ... has contributed 125 million building footprints in the U.S. to OSM, which they extracted from aerial imagery through deep learning algorithms''

''Facebook's OSM contributions to date have mostly been through supervised automated contributions. They use machine learning to detect road networks from satellite imagery which are then validated and reviewed by their OSM editors who work closely with the local OSM communities''

''Uber also announced through a community posting in an OSM forum that they will involve a team of editors to improve map data specifically for navigation by modifying and adding turn restrictions, directionality, and road geometry''

''Grab has dedicated considerable amount of effort into improving OSM data for Southeast Asia''

''In late 2017, a large part of the Mapbox data-team merged with the Development Seed data team, creating DevSeed Data. Like Facebook, this team is also heavily invested in machine-assisted mapping: using machine learning to help their data team identify features to map''


From JoW: ``this is the paper I mentioned that covers corporate contribution to OSM, with paid editing teams generally QAing automatically extracted features Isabel, Oliver. It talks about Facebook's work on automatic extraction of road networks, and Microsoft's building footprint contributions (US focused)
<https://teams.microsoft.com/l/message/19:00414f27a90247aaa326b7752ab553ef@thread.skype/1614246113577?tenantId=7988742d-c543-4b9a-87a9-10a7b354d289&amp;groupId=aff21ede-42cc-40c1-abe4-093312016ac2&amp;parentMessageId=1614246113577&amp;teamName=Deep Learning&amp;channelName=SeeAlso&amp;createdTime=1614246113577>''},
  creationdate   = {2021-03-12},
  keywords       = {machine learning, deep learning, mapping, remote sensing},
  owner          = {ISargent},
}

@Online{GCArchiveBestPractise2020,
  author    = {GeospatialCommission},
  date      = {2020-12-16},
  title     = {Extracting Data from Archives: Best Practice Guide},
  url       = {https://www.gov.uk/government/publications/extracting-data-from-archives-best-practice-guide/extracting-data-from-archives-best-practice-guide},
  keywords  = {mapping, geospatial data},
  owner     = {ISargent},
  creationdate = {2021-03-12},
}

@Article{ZbontarJMLD2021,
  author        = {Jure Zbontar and Li Jing and Ishan Misra and Yann LeCun and Stéphane Deny},
  title         = {Barlow Twins: Self-Supervised Learning via Redundancy Reduction},
  eprint        = {2103.03230},
  url           = {https://arxiv.org/pdf/2103.03230.pdf},
  archiveprefix = {arXiv},
  comment       = {A simple but apparently powerful approach to siamese network self-supervised learning. Different distortions of same image are passed down each network. The objective function is to get the cross-correlation between the two outputs to be as similar as possible to the identity matrix. As such, this creates an information bottleneck that maximises the information that passes through whilst minimising the information releated to the distortions.

''s redundancy-reduction — a principle first proposed in neuroscience — to self-supervised learning. In his influential article Possible Principles Underlying the
Transformation of Sensory Messages (Barlow, 1961), neuroscientist H. Barlow hypothesized that the goal of sensory processing is to recode highly redundant sensory inputs into a factorial code (a code with statistically independent components). This principle has been fruitful in explaining the organization of the visual system, from the retina to cortical areas (see (Barlow, 2001) for a review''

This is a reallly intuitive approach to unsupervised training. So long as the pair of images _mean_ the same then the cross-correlation should be the identity matrix.},
  creationdate  = {2021-03-22},
  keywords      = {deep learning, unsupervised, self-supervised},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  year          = {2021},
}

@Online{NgMLOps2021,
  title     = {A Chat with Andrew on MLOps: From Model-centric to Data-centric AI},
  url       = {https://www.youtube.com/watch?v=06-AZXmwHjo},
  comment   = {When it comes to MLOps it should be data-centric and less model-centric. Your model (architecture) is probably good enough, but is your data. Really simple ideas in this. See also SculleySWR2018 and Driscoll2014.},
  owner     = {ISargent},
  creationdate = {2021-03-24},
}

@Article{BelloYWAL2020,
  author         = {Bello, Saifullahi Aminu and Yu, Shangshu and Wang, Cheng and Adam, Jibril Muhmmad and Li, Jonathan},
  title          = {Review: Deep Learning on 3D Point Clouds},
  doi            = {10.3390/rs12111729},
  issn           = {2072-4292},
  number         = {11},
  url            = {https://www.mdpi.com/2072-4292/12/11/1729},
  volume         = {12},
  abstract       = {A point cloud is a set of points defined in a 3D metric space. Point clouds have become one of the most significant data formats for 3D representation and are gaining increased popularity as a result of the increased availability of acquisition devices, as well as seeing increased application in areas such as robotics, autonomous driving, and augmented and virtual reality. Deep learning is now the most powerful tool for data processing in computer vision and is becoming the most preferred technique for tasks such as classification, segmentation, and detection. While deep learning techniques are mainly applied to data with a structured grid, the point cloud, on the other hand, is unstructured. The unstructuredness of point clouds makes the use of deep learning for its direct processing very challenging. This paper contains a review of the recent state-of-the-art deep learning techniques, mainly focusing on raw point cloud data. The initial work on deep learning directly with raw point cloud data did not model local regions; therefore, subsequent approaches model local regions through sampling and grouping. More recently, several approaches have been proposed that not only model the local regions but also explore the correlation between points in the local regions. From the survey, we conclude that approaches that model local regions and take into account the correlation between points in the local regions perform better. Contrary to existing reviews, this paper provides a general structure for learning with raw point clouds, and various methods were compared based on the general structure. This work also introduces the popular 3D point cloud benchmark datasets and discusses the application of deep learning in popular 3D vision tasks, including classification, segmentation, and detection.},
  article-number = {1729},
  comment        = {Well-structured overview of DL applied to point clouds - not necessarily remote sensing point clouds.},
  journal        = {Remote Sensing},
  keywords       = {deep learning, 3D, MLStrat},
  owner          = {ISargent},
  creationdate      = {2021-04-01},
  year           = {2020},
}

@Article{LinVCY2020,
  author        = {Yaping Lin and George Vosselman and Yanpeng Cao and Ying Yang, Michael},
  title         = {LGENet: Local and Global Encoder Network for Semantic Segmentation of Airborne Laser Scanning Point Clouds},
  eprint        = {2012.10192},
  url           = {https://arxiv.org/abs/2012.10192},
  archiveprefix = {arXiv},
  comment       = {local and global encoder network. Improvement on KPConv ThomasQDMGG2019. Encoder-decoder structure.},
  keywords      = {deep learning, 3D, remote sensing, MLStrat},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-04-01},
  year          = {2020},
}

@InProceedings{ThomasQDMGG2019,
  author       = {Hugues Thomas and Charles R. Qi and Jean-Emmanuel Deschaud and Beatriz Marcotegui and Fran\c{c}ois Goulette and Leonidas J. Guibas},
  booktitle    = {Proceedings of the IEEE International Conference on Computer Vision},
  title        = {KPConv: Flexible and Deformable Convolution for Point Clouds},
  pages        = {6411--6420},
  url          = {https://openaccess.thecvf.com/content_ICCV_2019/papers/Thomas_KPConv_Flexible_and_Deformable_Convolution_for_Point_Clouds_ICCV_2019_paper.pdf},
  comment      = {Kernel Point Convolution
KPConv is inspired by image-based convolution, but in place of kernel pixels, we use a set of kernel points to define the area where each kernel weight is applied
Applied to online models.
From LinVCY2020: ``KPConv 3D convolutional kernel whose domain is a spherical 3D space. It has a deformable version that adapts to local geometry in order to enhance the representation of features. However, Thomas et al. (2019) suggest that rigid convolutions perform better than deformable ones on scenes that lack of diversity''},
  creationdate = {2021-04-01},
  keywords     = {deep learning, 3D, MLStrat},
  owner        = {ISargent},
  year         = {2019},
}

@InProceedings{QiSMG2017,
  author        = {Charles R. Qi and Hao Su and Kaichun Mo and Leonidas J. Guibas},
  booktitle     = {Conference on Computer Vision and Pattern Recognition (CVPR)},
  title         = {PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation},
  eprint        = {1612.00593},
  url           = {http://stanford.edu/~rqi/pointnet/},
  archiveprefix = {arXiv},
  comment       = {Standford project, also more recently PointNet++.
Works directly on point cloud by transforming and then applying MLPs. Transforms necessary to ensure function is invariant to rotation etc.},
  keywords      = {deep learning, 3D, MLStrat},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-04-01},
  year          = {2017},
}

@Article{AnonXXXX,
  author       = {Anon},
  journaltitle = {Digital Earth},
  title        = {A Scale Sequence Object-based Convolutional Neural Network (SS-OCNN) for crop classification from fine spatial resolution remotely sensed imagery},
  comment      = {My review:

The paper presents a new approach to classifying crops in remote sensing data. This first requires the imagery to be segmented into 'objects' using any of a range of computer vision approaches (in this case, eCognition software is used). It then classifies each object by cutting a patch ('window') centred on the object's centroid and passing this patch through a CNN. The novelty of the presented method is that different sized patches, in a 'scale' sequence - whereby the 'smallest scale' is first - are applied to the CNN such that the predicted probability of each class is dependant on the pixels values of the data at the current 'scale' and (for iterations after the first) the class probabilities output using the previous scale.

This is a very well written paper and I have very few comments on the language (I'll put these at the bottom). On the whole the approach is clearly described. However, I believe there are some considerable ambiguities that would make it difficult to repeat the experiments presented. Further, the paper misses a useful intuition about why this approach is necessary for the given problem of crop classification.

The first and greatest ambiguity is the use of the word 'scale'. What is presented in the paper are a series of image patches or windows that have an increasing size i.e. Number of pixels. The smallest 'scale' in the examples presented (and I appreciate that this can vary depending on the application) is 8 by 8 pixels and the largest scale is 56 x 56 pixels square. Unless some sort of interpolation of pixel values is occurring, I would disagree that scale is changing from one stage to the next - in every case, the pixels represent the same size on the ground. However, if interpolation is happening (and this can be automatic depending on the ML framework being used - the authors need to clarify this) the 8 x 8 pixel patch is the maximum scale since each pixel represents the most detail on the ground (maybe we should talk about fine to coarse scale?). Therefore, the authors must clarify what is happening to the image patches before they are passed into the CNN - does the number of pixels remain the same or are later patches resampled to a lower spatial resolution?

There are also some unexplained characteristics of the deep neural network being used. Firstly, the authors describe this as similar to AlexNet but AlexNet has 5 convolutional layers, 3 of which are followed by pooling. AlexNet is also considerably wider, having 2 or 3 hundred units per convolutional layer. Secondly, and more importantly, the authors don't seem to have acknowledged the impact of the convolutions and pooling on the size of the feature maps as the data is fed forward through the network. By my calculations, given the architecture described in the paper, the first two 'scales' would result in almost no data reaching the final dense layer since the feature map leaving the second conv-pool layer is less than the size of 3x3 filters and so the resulting convolutions and pooling would include a great deal of padding. Further, input patches with 32 - 56 pixel size would result in 2D outputs before the dense layer but the authors don't explain whether pooling or flattening is used to create the vector of features for input to the dense layer. Of course, neither the loss of data or the 2D output would be happening if the input patches are being resampled. In this case I would guess that everything is being resampled to 24 pixels because this would result in a single value output at each unit before the dense layer. 

-The authors need to clarify how input image size is impacted by the architecture as the data are fed forward through convolutional and pooling layers.

Not enough intuition is given to why this CNN-based approach would be superior for a crop classification problem. The authors say in several places that crop classification is a spectral problem as well as having spatial and temporal aspects. As I understand it, the majority of information on crop type is in the spectral information, including texture information. The paper indicates that the major problem with crops classification is the 'complexity if spatial patterns' which I understand to mean that fields can be different sizes and shapes.  A small amount may also be gained from context, such as the type of crop that lies in the next field. The use of high spectral resolution imagery addresses much of spectral component of the problem and the use of image segmentation addresses the spatial complexity. This leaves the requirement to recognise texture and perhaps incorporate context, both of which the CNN supports. However, the case for the 'scale sequence' is not made strongly. The fields of crops do not appear to manifest differently at different scales. 

-The authors need to better justify why a 'scale sequence' is necessary for a problem that appears, on the face of it, scale invariant.

The paper goes to great length to benchmark the approach. Whilst this is popular approach in machine learning but is not terribly robust since authors always have interest in tuning their featured approach more than their benchmarking approaches. 

-The authors need to demonstrate that they have applied the same tuning rigor to every single approach. This would also mean specifying which and how many manually-defined filters are being used for the OBIA approach. Filter banks are available and could be used to easily generate sets of features of a similar dimensionality to those being learned in the CNNs (i.e. the same number of texture filters as there are units in the CNN, or at least in 1 layer of the CNN). 

-The authors need to demonstrate a thorough effort with applying OBIA methods to the presented problem.

Visually there is little difference between the results from the OCNN approaches. What differences there are could come down to labelling error. Many uses of crop classifications are for crop area calculations. Do the differences between the approaches lead to significantly different area calculations? As stated above, the OBIA approach could almost certainly be improved to be on a par with the OCNN results, indicating that the most important part of the featured approach is the initial segmentation. 

-The authors should demonstrate more clearly the contribution that segmentation makes to the approach - my intuition is that this is more important than the size of image patches provided to the network.

This paper promotes the SS-OCNN method for the ease with which parameters are chosen.  However, choice of image size is rarely an issue when using CNNs because it usually comes down to the architecture being used (e.g. most architectures are suited to patches that are 224 x 224 pixels and for this network 24 x 24 pixels).  Also, this advantage fails to acknowledge the complexity of tuning a segmentation approach that can require a great deal of time adjusting parameters and writing rules. 

-The authors need to give more detail about how the objects were derived, what the ruleset for this is and show the resulting object data.

An obvious interpretation that arises from this work is that it is not the different 'scales' of data that are making the improvement, but the overall architecture of the approach. Effectively, during training, at each iteration, the network is being told ``this is what you thought last time, here are the actual answers, now try again''. 

-What would happen if, instead of different sized patches, the same size patch was introduced, even if this is an arbitrary size? 

-Alternatively, does this problem really require 3 convolutional layers - has the author tested it with a single, perhaps wider, layer of convolutions?

Finally, I have a concern about the accuracy assessment. It appears that the set is split randomly and therefore it may occur that a patch that will appear in test set overlaps with a patch in the training set. If the CNN approach really is useful because it includes context (and I stated above that some context may be useful because the crops in nearby fields my provide some information) then this could mean that effectively the network has already seen data during training when it encounters it during test. It would be better to have divided the region so that there is no possible overlap between the training, validation and testing data. 

-The authors need to demonstrate that the training, validation and test data are completely independent.

Detailed comments
Lines 67-68 - its generally accepted that the 3 image processing approaches to which CNNs are applied are image classification, object detection and semantic segmentation. The Krizhevsky and He papers are, in the main part, doing the same thing (image classification).
Line 67 - the He paper is not in the references - please check that you have all the citations in the references
Line 83 - ``via a fix-sized patch using several filters'' is a very awkward way of describing convolution (actually cross-correlation) with filters. You are applying filters across image patches of a fixed size
Line 129 - please add 'feed' before 'forward'
Line 141 - please add ', in' before 'which'
Line 154 - most people would understand a convolutional window to relate the (convolutional) filter but the patch size to relate to the input image, this need disambiguating
Lines 55-47 - ``...each object ... convolutional window''  this clause does not make sense
Lines 167, 168 and 341 - 'interpolate' is the wrong word here. Mathematically you are defining a range with a fixed interval.
Line 197 - put in space after Concat
Line 198 - replace ``inputted at'' with ``input from'' 
Line 206 - what is ``space size'' - do you mean number of pixels or similar?
Line 259 - replace ``divided subsequently randomly into three parts'' with ``split randomly into 3 sets''
Line 311 - these networks have 3 layers by traditional approaches, pooling is part of the same layer as convolution
Line 319 - why were only 32 filters used? Doesn't this disadvantage PCNN?
Line 321 - how many are 'several' hand crafted features used for OBIA?
Line 331 ... 565 - please choose a consistent phrase - window or patch - window can sometimes apply to a filter so may be ambiguous if used to denote the input image size
Line 399 and 459 - what do you mean by 'linear noise'?
Line 457 - please clarify that 'improved' is referring to the OBIA approach 
Line 487 - low-level and high-level features - this needs better intuition, what are 'high level features' in this context? Field shape? Field neighbours? It's not clear why this is useful
Line 491-492 - 'trial and error' is not usually used with CNNs. Usually the architecture determines the image size (or vice versa)
Lines 512 and 513 - replace i.e. with e.g.
Lines 536-538 ``the crops were observed at different scale dimensions which was extremely beneficial to capture their unique spectral or structural characteristics `` - why is scale beneficial to capture spectral characteristics?},
  creationdate = {2021-04-01},
  owner        = {ISargent},
}

@Article{StewartMB2020,
  author       = {Alexander J. Stewart and Nolan McCarty and Joanna J. Bryson},
  date         = {2020-12-11},
  journaltitle = {SCIENCE ADVANCES},
  title        = {Polarization under rising inequality and economic decline},
  comment      = {Found after looking through works by Joanna Bryson who did keynote at ATI AI UK.

Paper demonstrates how economic stress - poverty or inequality - can lead to more polarized views because the likelihood of individuals interacting outside of their 'group' is reduced, since the benefits of those interaction are reduced in comparison with interactions within groups.

''Instead of postulating direct conflict over scarce resources, we assume that behavioural changes leading to greater polarization during economically challenging periods are driven by risk aversion. Of course, both intergroup competition for resources and increased risk aversion for out-group interactions can occur at the same time and may reinforce one another, leading to even greater entrenchment of polarized attitudes.''

''other authors (38, 39) have shown that calamitous economic shocks such as the Great Depression and the Global Financial Crisis increased support for right-wing politicians, especially those of the far right who denigrate social out-groups. Economic stress and fear have been shown to be excellent highly localized predictors for the success of a number of populist movements including those in the Ukraine (40) as well as the United States and United Kingdom (41, 42).''

''Better than morbidly waiting for further economic crises or war, we will end by pointing out an obvious recommendation as to what political leaders and governments should do to prevent persistent group polarization. Our work unambiguously highlights the importance of building and maintaining a social safety net. Social institutions may serve as a means to provide redistribution, thus reducing inequality, but our work emphasizes another important role: preventing the income of groups from falling sufficiently far to trigger the risk aversion that might lead to persistent group polarization.''},
  creationdate = {2021-04-19},
  keywords     = {Social Science},
  owner        = {ISargent},
}

@Article{BatemanM2020,
  author    = {Bateman, I and Mace, G},
  title     = {The natural capital framework for sustainable, efficient and equitable decision making},
  doi       = {10.1038/s41893-020-0552-3},
  number    = {3},
  pages     = {776--783},
  comment   = {A key paper in understanding natural capital approaches},
  journal   = {Nature Sustainability},
  keywords  = {Environment},
  owner     = {ISargent},
  priority  = {prio1},
  creationdate = {2021-04-20},
  year      = {2020},
}

@Article{ArribaBelNS2011,
  author    = {Daniel Arribas-Bel and Peter Nijkamp and Henk Scholten},
  title     = {Multidimensional urban sprawl in Europe: A self-organizing map approach},
  doi       = {https://doi.org/10.1016/j.compenvurbsys.2010.10.002},
  issn      = {0198-9715},
  number    = {4},
  pages     = {263-275},
  url       = {https://www.sciencedirect.com/science/article/pii/S0198971510000992},
  volume    = {35},
  abstract  = {The present paper addresses the issue of urban sprawl in Europe from a multidimensional point of view, identifying the most sprawled areas and characterizing them in terms of population size. The literature is reviewed to categorize and extract the most relevant six dimensions that define the concept and several indices are specified to implement them. These are then calculated for a sample of the main European cities that uses several sources to obtain the best possible dataset to measure urban sprawl. All this information is brought together using the self-organizing map (SOM) algorithm to be visualized and further studied, taking advantage of its properties as a data-reduction as well as a clustering technique. The analysis locates the hot-spots of urban sprawl in Europe in the centre of the continent, around Germany, and characterizes such urban areas as small, always half the size of the average city of the sample.},
  comment   = {One of the papers that make up Dani's PhD thesis. Applies self-organising network to data about urban areas, specifically 6 features:
Connectivity - as defined by commute time (longer than average commutes will be interpreted as more sprawl)
Decentralization - devise their own indicator - that measures the proportion of people who live decentralized over that who live in the core, so higher values will imply more sprawl
Density - the population divided by the urban area  - lower desities indicate sprawl
Scattering - the number of urban patches (they normalise by population size) - higher number of patches for population size is more sprawl
Availability of open space - percentage of the urban area within the urban region that is classified as urban green space
 Land-use mix - the degree to which different land uses are mixed into urban land use

Found that no one measure defines sprawl and that the measures can be used to characterise sprawl},
  journal   = {Computers, Environment and Urban Systems},
  keywords  = {Urban sprawl, Self-organizing maps, Connectivity, Density, Scattering, Europe, social science},
  owner     = {ISargent},
  creationdate = {2021-04-20},
  year      = {2011},
}

@InProceedings{PerelloNietoFKF2016,
  author           = {Perello-Nieto, Miquel and Filho, Telmo De Menezes E Silva and Kull, Meelis and Flach, Peter},
  booktitle        = {2016 IEEE 16th International Conference on Data Mining (ICDM)},
  title            = {Background Check: A General Technique to Build More Reliable and Versatile Classifiers},
  doi              = {10.1109/ICDM.2016.0150},
  pages            = {1143-1148},
  url              = {https://ieeexplore.ieee.org/document/7837963},
  comment          = {View paper at \url{https://research-information.bris.ac.uk/ws/portalfiles/portal/92014727/Peter_Flach_Background_Check_A_general_technique_to_build_more_reliable_and_versatile_classifiers.pdf}

My adding in a 'background' class - which has higher likelihood outside of the region occupied by the data - it is possible to prevent a model from being overconfident and extrapolating answers where the model cannot make a confident judgement.},
  creationdate     = {2021-04-21},
  keywords         = {machine learning, extrapolation, quality},
  modificationdate = {2023-05-04T10:58:59},
  owner            = {ISargent},
  year             = {2016},
}

@Article{SeresinhePM2017,
  author       = {Seresinhe, Chanuki Illushka and Preis, Tobias and Moat, Helen Susannah},
  title        = {Using deep learning to quantify the beauty of outdoor places},
  doi          = {http://doi.org/10.1098/rsos.170170},
  url          = {https://royalsocietypublishing.org/doi/full/10.1098/rsos.170170},
  comment      = {Notes from Turing AI UK 2021 presentation (will hopefully be added to thi playlist: https://www.youtube.com/playlist?list=PLuD_SqLtxSdUXt1kmms31Tlvtp5zjlsbi) (my original notes with screenshots: https://ordnancesurvey-my.sharepoint.com/personal/isabel_sargent_os_uk/_layouts/OneNote.aspx?id=%2Fpersonal%2Fisabel_sargent_os_uk%2FDocuments%2FIsabel%20%40%20Ordnance%20Survey&wd=target%28Conferences%20etc.one%7C78BBA81C-AF1F-43ED-9DEC-4A6D419FCAF5%2FTuring%20AI%20UK%7C629E67F2-8E70-443B-AD81-BA10E8B65B3B%2F%29
onenote:https://ordnancesurvey-my.sharepoint.com/personal/isabel_sargent_os_uk/Documents/Isabel%20@%20Ordnance%20Survey/Conferences%20etc.one#Turing%20AI%20UK&section-id={78BBA81C-AF1F-43ED-9DEC-4A6D419FCAF5}&page-id={629E67F2-8E70-443B-AD81-BA10E8B65B3B}&end):
Data on the beauty of an entire country much harder than land cover types etc
CNNs already trained on Places dataset used on scores for images around GB on Geograph
Geograph images are good enough spread. Applied to Google Streetview to get a more dense view of scenicness. Compared to happiness data from the Mappiness app and determined that people are happier in more scenic areas.
Able to determine the types of view that are considered scenic and those that are not and use this to drive planning policy},
  creationdate = {2021-04-21},
  journal      = {Royal Scociety Open Science},
  owner        = {ISargent},
  year         = {2017},
}

@InProceedings{PuigcerverRMRPGKH2021,
  author    = {Joan Puigcerver and Carlos Riquelme Ruiz and Basil Mustafa and Cedric Renggli and Andr{\'e} Susano Pinto and Sylvain Gelly and Daniel Keysers and Neil Houlsby},
  booktitle = {International Conference on Learning Representations},
  title     = {Scalable Transfer Learning with Expert Models},
  url       = {https://openreview.net/forum?id=23ZjUGpjcc},
  comment   = {Pretrain with labelled data and create label hierachies. These hierarchies can be used to create 'expert' models - one for each subcategory (e.g. transport). Then an automatic approach choose the best expert when presented with a new problem. Only 1 expert per new model but suggest experts could be combined.
This could be interesting if experts are geographical regions and/or data types.},
  keywords  = {transfer learning, deep learning, representation learning},
  owner     = {ISargent},
  creationdate = {2021-05-03},
  year      = {2021},
}

@InProceedings{MummadiSHVFM2021,
  author    = {Chaithanya Kumar Mummadi and Ranjitha Subramaniam and Robin Hutmacher and Julien Vitay and Volker Fischer and Jan Hendrik Metzen},
  booktitle = {International Conference on Learning Representations},
  title     = {Does enhanced shape bias improve neural network robustness to common corruptions?},
  url       = {https://openreview.net/forum?id=yUxUNaj2Sl},
  comment   = {It has been shown that CNN are biased towards texture. This work is looking at can the bias be moved towards shape.
Improve shape bias using edge maps of images and reduce the texture bias by 'stylizing' images. ResNet18.
But enhanced shape bias doesn't improve the robustness towards common corruptions.
Look at role of shape bias - data augmentation via stylisation is the reason for improved corruption robustness
style distribution - intra-stylization using the network itself
preserved image statistics - superposition of edges onto (stylized?) image - best results},
  keywords  = {deep learning, representation learning, shape, resnet},
  owner     = {ISargent},
  creationdate = {2021-05-03},
  year      = {2021},
}

@InProceedings{ThiryABO2021,
  author    = {Louis Thiry and Michael Arbel and Eugene Belilovsky and Edouard Oyallon},
  booktitle = {International Conference on Learning Representations},
  title     = {The Unreasonable Effectiveness of Patches in Deep Convolutional Kernels Methods},
  url       = {https://openreview.net/forum?id=aYuZO9DIdnn},
  comment   = {Present SimplePatch, a recent update of the CoatesHN11 method - Kernel - random features of Coates 2011, Recht 2019, whitening, shrinked convolutions. Apply whitening to patches, knn applied to random patches.
Achieves best accuracy for linear classifiction (no hidden layer) and very good with non-linear (1 hidden layer) but far for SOTA with ResNet. Regularise based on number of neighbours.},
  keywords  = {kernel, representation learning, k nearest neighbour},
  owner     = {ISargent},
  creationdate = {2021-05-03},
  year      = {2021},
}

@InProceedings{LurzBWJWWCMCTES2021,
  author    = {Konstantin-Klemens Lurz and Mohammad Bashiri and Konstantin Willeke and Akshay Jagadish and Eric Wang and Edgar Y. Walker and Santiago A Cadena and Taliah Muhammad and Erick Cobos and Andreas S. Tolias and Alexander S Ecker and Fabian H. Sinz},
  booktitle = {International Conference on Learning Representations},
  title     = {Generalization in data-driven models of primary visual cortex},
  url       = {https://openreview.net/forum?id=Tp7kI90Htd},
  comment   = {Goal is produce a more generalised model that can predict the cortical reponses of different mammals. CNNs are used to predict neural readouts. Use two methods to improve generality from primates to rodents - one puts together responses from different parts of brain (I think) and the other models what part of the trained model's feature maps is the most useful (and thus reduces the size of the feature vector c.f. using the whole feature map.},
  keywords  = {deep learning, neuroscience},
  owner     = {ISargent},
  creationdate = {2021-05-03},
  year      = {2021},
}

@InProceedings{PopeZAGG2021,
  author       = {Phil Pope and Chen Zhu and Ahmed Abdelkader and Micah Goldblum and Tom Goldstein},
  booktitle    = {International Conference on Learning Representations},
  title        = {The Intrinsic Dimension of Images and Its Impact on Learning},
  url          = {https://openreview.net/forum?id=XJk19XzGq2J},
  comment      = {How low dimensional are image datasets?
Can we measure this?
What is the impact of dimensionality on learning?

Use method of Levina \& Bickel, NeurIPS 2004 https://proceedings.neurips.cc/paper/2004/file/74934548253bcab8490ebd74afed7031-Paper.pdf. This uses kNN and is based on there being more ways for points to be close together in high dimensions so knn statistics decay less rapidly at higher dimensions. Calc stats locally and average over the set.

First test on GAN generated images to determine if intrinsic dimensionality can be estimated using this technique. Extrinsic dimensionality for simple data set is, say, 128. Find that choice of k is problem-dependant so report on a range of ks. However, find that the intrinsic dimensionality is much lower than extrinsic dimensionality.

Then apply to real data sets. Find that MNIST lowest (~12) dimensionality and ImageNet highest - as would be expected, but ImageNet is only in 40s.

Importantly, apply this to training. Find that generating data at lower dimensionality achieves good (better) results in supervised problem much more rapidly than original dimensionality data. Compared to changing the extrinsic dimensionality - by changing image resolution - but found no such improvement.

Useful for understanding and intelligently adjusting dimensionality of our imagery? How does RS data differ from ImageNet? Masters project?
https://github.com/ppope/dimensions

From <https://www.researchgate.net/publication/350991432_The_Intrinsic_Dimension_of_Images_and_Its_Impact_on_Learning>},
  creationdate = {2021-05-03},
  keywords     = {representation learning, dimensionality},
  owner        = {ISargent},
  year         = {2021},
}

@InProceedings{PanDLLL2021,
  author    = {Xingang Pan and Bo Dai and Ziwei Liu and Chen Change Loy and Ping Luo},
  booktitle = {International Conference on Learning Representations},
  title     = {Do 2D {\{}GAN{\}}s Know 3D Shape? Unsupervised 3D Shape Reconstruction from 2D Image {\{}GAN{\}}s},
  url       = {https://openreview.net/forum?id=FGqiDsBUKL0},
  comment   = {From <https://iclr.cc/virtual/2021/oral/3389> 

Unsupervised 3D shape reconstruction with GANs based on exploring viewpoint and lighting variations. GAN inversion to obtain projected samples. Iterative training and projection to improve the 3D shape. 
Better than lidar? Useful for better surface models and 3D reconstruction from aerial imagery?},
  keywords  = {3D, deep learning},
  owner     = {ISargent},
  creationdate = {2021-05-03},
  year      = {2021},
}

@InProceedings{WuMZYG2021,
  author    = {Mike Wu and Milan Mosse and Chengxu Zhuang and Daniel Yamins and Noah Goodman},
  booktitle = {International Conference on Learning Representations},
  title     = {Conditional Negative Sampling for Contrastive Learning of Visual Representations},
  url       = {https://openreview.net/forum?id=v8b3e5jN66j},
  comment   = {From <https://iclr.cc/virtual/2021/poster/3245> https://openreview.net/forum?id=v8b3e5jN66j

In contrastive learning, are all negative examples equally useful?

Noise contrastive estimate NCE - a lower bound on mutual information

Could choose negative examples to be more difficult but needs to be done in order to continue to lower bound mutual information. I don't know what that means. 

Effectively choosing negatives that are closer to the example - in practise this apparently only needs a few lines of code. Experiments shows an improvement of a few percent.},
  keywords  = {contrastive learning, deep learning, represtation learning},
  owner     = {ISargent},
  creationdate = {2021-05-03},
  year      = {2021},
}

@InProceedings{FrankleDRC2021,
  author    = {Jonathan Frankle and Gintare Karolina Dziugaite and Daniel Roy and Michael Carbin},
  booktitle = {International Conference on Learning Representations},
  title     = {Pruning Neural Networks at Initialization: Why Are We Missing the Mark?},
  url       = {https://openreview.net/forum?id=Ig-VyQc-MLK},
  comment   = {From <https://iclr.cc/virtual/2021/poster/3159>  https://openreview.net/forum?id=Ig-VyQc-MLK

Pruning is usually during or after training - but what about at initialization to improve training. 

Compare different approaches to pruning before initialisation for how well training  goes. Pruning after produces much better results, why?

Personally I can't get my head round how you can prune when you haven't set the weights yet - surely the weight values are important?

Tried the same pruning techniques during and after training - most are a bit better but still don't do as well as after training. But I suppose trained weights look the same as randomely intiated weights are you are simply trying to remove those that are redundant at whichever stage...?},
  keywords  = {deep networks, machine learning},
  owner     = {ISargent},
  creationdate = {2021-05-03},
  year      = {2021},
}

@InProceedings{FischbacherS2021,
  author    = {Thomas Fischbacher and Luciano Sbaiz},
  booktitle = {International Conference on Learning Representations},
  title     = {Single-Photon Image Classification},
  url       = {https://openreview.net/forum?id=CHLhSw9pSw8},
  comment   = {From <https://iclr.cc/virtual/2021/poster/3024> https://openreview.net/forum?id=CHLhSw9pSw8

Google. Bridge from quantum and ML communities - quantum ML may become a reality before large scale quantum computers.

Fastest possible ML algorithm would need to make a call after receiving the very first photon.},
  keywords  = {quantum, machine learning},
  owner     = {ISargent},
  creationdate = {2021-05-03},
  year      = {2021},
}

@InProceedings{SlavutskyB2021,
  author    = {Yuli Slavutsky and Yuval Benjamini},
  booktitle = {International Conference on Learning Representations},
  title     = {Predicting Classification Accuracy When Adding New Unobserved Classes},
  url       = {https://openreview.net/forum?id=Y9McSeEaqUh},
  comment   = {From <https://iclr.cc/virtual/2021/poster/2595> https://openreview.net/forum?id=Y9McSeEaqUh

With new classes, usually the accuracy will decline, but it isn't clear by how much. Use reversed ROC to demonstrate what the classification accuracy will be given number of classes. Develop CleaneX algorithm. Predict accuracy after only a little training, I think.},
  keywords  = {transfer learning},
  owner     = {ISargent},
  creationdate = {2021-05-03},
  year      = {2021},
}

@InProceedings{XiaoWED2021,
  author    = {Tete Xiao and Xiaolong Wang and Alexei A Efros and Trevor Darrell},
  booktitle = {International Conference on Learning Representations},
  title     = {What Should Not Be Contrastive in Contrastive Learning},
  url       = {https://openreview.net/forum?id=CZ8Y3NzuVzO},
  comment   = {From <https://iclr.cc/virtual/2021/poster/2809> https://openreview.net/forum?id=CZ8Y3NzuVzO

Self-supervised learning where supervised learning is meeting limits extrapolating beyond training data.

Learning invariance is beset with problems - orientation is important for some things, colour for others - we don't necessarily want our network invariant to these things.

Propose to learn embedding subspaces that are sensitive to specific augmentation while invariant to others. Shared backbone. Each task is free to utilise different task-specific heads or backbone. 

Relevant to weird augmentations tried during A4I project.},
  keywords  = {contrastive learning, data augmentation, deep learning, representation learning, self supervised learning},
  owner     = {ISargent},
  creationdate = {2021-05-03},
  year      = {2021},
}

@InProceedings{ZhouWB2021,
  author    = {Tianyi Zhou and Shengjie Wang and Jeff Bilmes},
  booktitle = {International Conference on Learning Representations},
  title     = {Robust Curriculum Learning: from clean label detection to noisy label self-correction},
  url       = {https://openreview.net/forum?id=lmTWnm3coJJ},
  comment   = {Problem of training with incorrect labels.

Want to be able to detect correctly labelled and use these for supervised learning and incorrectly labelled examples to use for self-supervised learning because these examples still contain useful information. Outputs from wrongly labelled examples can be used as pseudo labels

Find that correctly labelled samples have smaller exponential moving average losses loss during training. Data with correct pseudo labels has more consistent moving outputs over time. 

Combine these two metrics to to perform supervised learning with data that has correct label but wrong pseudo label and then later self-supervised with data with the wrong label and correct pseudo label. This curriculum learning improves output accuracy. Method called RoCL.

Find that data augmentation is important for identifying the correct label/pseudo label but class balance is only important for data with high noise rates.},
  keywords  = {data labelling, supervised training, self supervised training, machine learning, noisy labels},
  owner     = {ISargent},
  creationdate = {2021-05-04},
  year      = {2021},
}

@InProceedings{HuangMEBW2021,
  author    = {Hanxun Huang and Xingjun Ma and Sarah Monazam Erfani and James Bailey and Yisen Wang},
  booktitle = {International Conference on Learning Representations},
  title     = {Unlearnable Examples: Making Personal Data Unexploitable},
  url       = {https://openreview.net/forum?id=iAmZUo0DxC0},
  comment   = {Looking into adding 'noise' to images that make them unrecognisable to machines but do not affect human interpretation.
Error-minimising noise (I didn't understand what this is) makes the dataset unlearnable.},
  owner     = {ISargent},
  creationdate = {2021-05-04},
  year      = {2021},
}

@InProceedings{AgarwalaDJPSWZ2021,
  author    = {Atish Agarwala and Abhimanyu Das and Brendan Juba and Rina Panigrahy and Vatsal Sharan and Xin Wang and Qiuyi Zhang},
  booktitle = {International Conference on Learning Representations},
  title     = {One Network Fits All? Modular versus Monolithic Task Formulations in Neural Networks},
  url       = {https://openreview.net/forum?id=uz5uw6gM0m},
  comment   = {Are modular architectures more statistically efficient than no modular ones

Can learning unrelated tasks interfere with learning? Doesn't seem to be a cost of learning lost of functions together.

Relevant to Toponet - should we be training huge models?

Seem to find that neural nets can learn complex models without modularity even with problems that have inherent modularity.},
  keywords  = {deep learning, machine learning, model architecture},
  owner     = {ISargent},
  creationdate = {2021-05-04},
  year      = {2021},
}

@InProceedings{PhooH2021,
  author    = {Cheng Perng Phoo and Bharath Hariharan},
  booktitle = {International Conference on Learning Representations},
  title     = {Self-training For Few-shot Transfer Across Extreme Task Differences},
  url       = {https://openreview.net/forum?id=O3Y56aqpChA},
  comment   = {Very relevant to toponet and globenet - really good presentation.

Backbone (from representation learning) used as feature extractor which are input to a new head trained (few-shot learning) with a small set of examples of a novel class.

But new domains - e.g. if started with ImageNet and then moved to x-ray, remote sensing, medical - can have poor outcomes. There are often few labelled examples in these new domains.

Few shot learners don't tend to perform as well as naive transfer learning when it come to cross-domain problems.

In a lot of domains there are a lot of unlabelled data.

They therefore modify representation learning phase by adding in unlabelled data. But how? Options:
Transfer learning only from source domain - but doesn't have any knowledge of target domain
Self-supervised learning using unlabelled target domain data - but doesn't have any discriminatory information relating to ultimate problem

Observe that inputs from target domain can obtain an incorrect but consistent output - e.g. always the same class, even though it is wrong (because the source domain doesn't not have the correct class).

Look at such pseudo prediction for different domains based on ImageNet source and compared adjusted mutual information - AMI - between the prediction (source domain) and the 'ground turth' (target domain). Find that for target domains like satellite images and crop disease AMI is around 0.3 (where 0 is no MI and 1 is perfect overlap).

From this they present STARTUP - train classifier in source domain, produce pseudo labels from this using target domain imagery, use these pseudo labels for self-supervised training to improve the backbone. Results of this are better than naive transfer (and thus obviously few shot learning approaches). Also compare to SimCLR and transfer+SimCLR (which is initialised by transferred). In all domains STARTUP outperforms although in some domains SimCLR or transfer+SimCLR are also good.

We would use this by starting with ImageNet trained network, use this to extract pseudo labels in RS data and they use these labels to futher train the network using self-supervision. The resulting network would be backbone for few shot learning as new problems come along.},
  keywords  = {representation leanring, deep learning, transfer learning, Globenet},
  owner     = {ISargent},
  creationdate = {2021-05-04},
  year      = {2021},
}

@InProceedings{CarbonnelleD2021,
  author    = {Simon Carbonnelle and Christophe De Vleeschouwer},
  booktitle = {International Conference on Learning Representations},
  title     = {Intraclass clustering: an implicit learning ability that regularizes {\{}DNN{\}}s},
  url       = {https://openreview.net/forum?id=tqOvYpjPax2},
  comment   = {Do deep networks learn intraclass clusters (clusters of similar examples within a class)? What effect does learning intraclass clusters have on model generalisation?

Measure the discrimination of intraclass clusters in 2 ways:
Use the existance of class hierachies as a proxy for clusters - e.g. different species of flowers in the flowers class - to what extent does a network discriminate thes e sub classes
Use variance of representations - pre-activations - of a class - if this is larger for a class than for whole dataset likely to be discriminating intraclass clusters

Plotting these measures against test set accuracy there is a very consistent correlation (a concave curve)

Observe that the measures change at the early phase of training.

Conclude: some neurons play a crucial role in intraclass clustering and this emergence regularises the DNN and improves generalisation.},
  owner     = {ISargent},
  creationdate = {2021-05-04},
  year      = {2021},
}

@InProceedings{LiZXH2021,
  author    = {Junnan Li and Pan Zhou and Caiming Xiong and Steven Hoi},
  booktitle = {International Conference on Learning Representations},
  title     = {Prototypical Contrastive Learning of Unsupervised Representations},
  url       = {https://openreview.net/forum?id=KmykpuSrjcq},
  comment   = {Current methods of contrastive learning can exploit just low level features and so doesn't learn the desired representations.

This method prototypical contrastive learning creates prototypes by preclustering the examples. The aim of training is to draw examples closer to their prototypes. Maximisation expectation approach.

Say that InfoNCE loss is a special case of PCL.},
  keywords  = {contrastive learning, deep learning, self supervised learning, unsupervised learning},
  owner     = {ISargent},
  creationdate = {2021-05-04},
  year      = {2021},
}

@InProceedings{LiuHLJL2021,
  author    = {Lu Liu and William L. Hamilton and Guodong Long and Jing Jiang and Hugo Larochelle},
  booktitle = {International Conference on Learning Representations},
  title     = {A Universal Representation Transformer Layer for Few-Shot Image Classification},
  url       = {https://openreview.net/forum?id=04cII6MumYV},
  comment   = {how to use information from different domains - cross-domain representation transfer.

URT: Universal Representation Transformer},
  keywords  = {representation learning, transfer learning, cross domain},
  owner     = {ISargent},
  creationdate = {2021-05-04},
  year      = {2021},
}

@InProceedings{FosterPR2021,
  author    = {Adam Foster and Rattana Pukdee and Tom Rainforth},
  booktitle = {International Conference on Learning Representations},
  title     = {Improving Transformation Invariance in Contrastive Representation Learning},
  url       = {https://openreview.net/forum?id=NomEDgIEBwE},
  comment   = {I don't really understand this but talk about diffrentiating the augmentations and so the _amount_ something has been augmented is something to do with the training - want representations for augmentations to be close. Or something. Obviously its better than SOTA such as SimCLR because everything is when you try it... propose Spirograph dataset which is fully differentiable generation.},
  owner     = {ISargent},
  creationdate = {2021-05-04},
  year      = {2021},
}

@InProceedings{BorowskiZSGWBB2021,
  author           = {Judy Borowski and Roland Simon Zimmermann and Judith Schepers and Robert Geirhos and Thomas S. A. Wallis and Matthias Bethge and Wieland Brendel},
  booktitle        = {International Conference on Learning Representations},
  title            = {Exemplary Natural Images Explain {\{}CNN{\}} Activations Better than State-of-the-Art Feature Visualization},
  url              = {https://openreview.net/forum?id=QO9-y8also-},
  comment          = {How useful are synthetic visualisations of what activates CNN units for human interpretation.
Taking an approach such as Olah 2017, visualisations of what acitvates a single node can be created iteratively. But are these useful.
Performed a study where, given a maximally and a minimally activated image, partificpants had to say which of two natural images would most activate that node. Also gave a sit of minimally and maximally activiation natural images to ask the same task.
Although the synthetic images were useful - better than random - the natural images were more useful and responses were faster indicating that they were easier to interpret.

This is useful because when we've looked at unit activations we've done the 'top 16' method demonstrating the parts of the dataset that maximally activate a unit. These are effectively natural images. We hav considered generating synthetic images using iterative approaches but not implemented it. This paper justifies the 'top x' approach. Also, it indicates providing the bottom X too (although I suspect this would be much more confused).},
  creationdate     = {2021-05-04},
  keywords         = {deep learning, visualisation, explainability, representation learning, IncisiveTagging},
  modificationdate = {2022-06-27T15:14:58},
  owner            = {ISargent},
  priority         = {prio1},
  year             = {2021},
}

@InProceedings{IslamKEJODB2021,
  author           = {Md Amirul Islam and Matthew Kowal and Patrick Esser and Sen Jia and Bj{\''o}rn Ommer and Konstantinos G. Derpanis and Neil Bruce},
  booktitle        = {International Conference on Learning Representations},
  title            = {Shape or Texture: Understanding Discriminative Features in {\{}CNN{\}}s},
  url              = {https://openreview.net/forum?id=NcFEZOi-rLa},
  comment          = {It is understood that CNNs learn texture better than shape and yet humans recognise using shape. Easy to 'fool' a CNN by texturing the shape of one thing with the texture of another.

This paper proposes two methods of quantifying the amount of shape information learned by the CNN.

Method 1: Estimate the number of encoding neurons: randomly stylised images to produce a texture pair that shre the same texture and a shape pair that share the same shape. Pass these through network to obtain the latent representations. Then look at the mutual information between the latent representations for the images in each pair. A soft max is then used to estimate the number of neurons in the network that respond to shape.

Method 2: Decoding per-pixel from a pretrained network train a head to predict the binary versus semantic segmentation mask.

Find that BagNet have fewer shape encoding neurons compared to ResNet. Also found that shape biased training results in more shape encoding neurons.

Find that considerably better results are obtained for binary versus semantic segmentation.

Shape information is found at stage f2 but more in f5 (later layer). That shape info is not as coherent in the earlier layer. Doesn't encode the the full object shape.

Interesting to try to replicate on networks trained with aerial imagery? Maybe shape isn't as important anyway?},
  creationdate     = {2021-05-05},
  keywords         = {deep learning, discovery, visualisation, metrics, explainability},
  modificationdate = {2022-04-05T09:44:01},
  owner            = {ISargent},
  year             = {2021},
}

@InProceedings{GoukHP2021,
  author    = {Henry Gouk and Timothy Hospedales and Massimiliano Pontil},
  booktitle = {International Conference on Learning Representations},
  title     = {Distance-Based Regularisation of Deep Networks for Fine-Tuning},
  url       = {https://openreview.net/forum?id=IFqrg1p5Bc},
  comment   = {When fine tuning we may want to constrain the change of weights to be within a given bound (the network doesn't change too much from the original weights). But how to we perform this?

Could use a distance metric that measures the total distance in weight space used - there are two ways of measure this distance given in this paper MARS and one based on euclidean distance. Relgularising can be performed by adding a penalty to the loss or by applying a projection funciton to pull the values back to within the given bounds.

Comparing the two metrics and the two ways of applying regularisation across different fine tuning problems demonstrates that the MARS approach applied as a projection function is consistently better but MARS as a penalty method is worst.},
  keywords  = {fine tuning, transfer learning, regularisation, deep learning, outlier detection},
  owner     = {ISargent},
  creationdate = {2021-05-05},
  year      = {2021},
}

@InProceedings{AdebayoGMGHK2018,
  author           = {Julius Adebayo and Justin Gilmer and Michael Muelly and Ian J. Goodfellow and Moritz Hardt and Been Kim},
  booktitle        = {32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.},
  date             = {2018-12-03},
  title            = {Sanity Checks for Saliency Maps},
  eprint           = {1810.03292},
  url              = {https://dl.acm.org/doi/10.5555/3327546.3327621},
  volume           = {abs/1810.03292},
  archiveprefix    = {arXiv},
  bibsource        = {dblp computer science bibliography, https://dblp.org},
  biburl           = {https://dblp.org/rec/journals/corr/abs-1810-03292.bib},
  comment          = {Evidence that saliency maps do not provide interpretatbility. 
From invited talk at ICLR2021 - salience maps are similar even with random weights and humans don't seem to interpret saliency maps in a meaningful way},
  creationdate     = {2021-05-05},
  journal          = {CoRR},
  keywords         = {visualisation, interpretability, machine learning, deep learning, metrics, explainability},
  modificationdate = {2022-04-29T11:56:13},
  owner            = {ISargent},
  year             = {2018},
}

@InProceedings{Kawaguchi2021,
  author           = {Kenji Kawaguchi},
  booktitle        = {International Conference on Learning Representations},
  title            = {On the Theory of Implicit Deep Learning: Global Convergence with Implicit Layers},
  url              = {https://openreview.net/forum?id=p-NZIuwqhI4},
  comment          = {I can't decide whether the presentation for this at ICLR is genius or terrible. Read from slides except any mathematical concept which was either 'this equation' or just silence. Weird. Anyway, this is working on the deep equilibrium model DEQ idea in the linear case and the paper should have useful insights.},
  creationdate     = {2021-05-05},
  keywords         = {deep learning, DEQ, , continious depth, implicit models},
  modificationdate = {2022-05-04T19:18:31},
  owner            = {ISargent},
  year             = {2021},
}

@InProceedings{MaKZH2021,
  author    = {Xuezhe Ma and Xiang Kong and Shanghang Zhang and Eduard H Hovy},
  booktitle = {International Conference on Learning Representations},
  title     = {Decoupling Global and Local Representations via Invertible Generative Flows},
  url       = {https://openreview.net/forum?id=iWLByfvUhN},
  comment   = {Use encoder-decoder architecture to extract local information from global information. This means that, given two images, the local information from can be applied to the reconstruction from the global information to the other resulting in colour and texture change but scene layout remains the same.},
  keywords  = {deep learning},
  owner     = {ISargent},
  creationdate = {2021-05-05},
  year      = {2021},
}

@InProceedings{DuSUTW2021,
  author    = {Yilun Du and Kevin A. Smith and Tomer Ullman and Joshua B. Tenenbaum and Jiajun Wu},
  booktitle = {International Conference on Learning Representations},
  title     = {Unsupervised Discovery of 3D Physical Objects},
  url       = {https://openreview.net/forum?id=lf7st0bJIA5},
  comment   = {Use motion and local foveation to discover and segment 3D objects in videos.
Use physics rules improve segmentation.
Loss is based on 2D segmentation mask. Without physics constraints results are OK but physics constraints make it much better. without any prior or supervision. Currently workong on simple synthetic and real images (blocks on a plain background).},
  keywords  = {3D, unsupervised learning, machine learning, video},
  owner     = {ISargent},
  creationdate = {2021-05-05},
  year      = {2021},
}

@InProceedings{NguyenRK2021,
  author           = {Thao Nguyen and Maithra Raghu and Simon Kornblith},
  booktitle        = {International Conference on Learning Representations},
  title            = {Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth},
  url              = {https://openreview.net/forum?id=KJNcAkY8tY4},
  comment          = {Investigate the effect of network width and depth
Central Kernel Alignment - representation similarity of any pair of layer activations - images of - correlation between representations
results in block structure where there are a lot of similar representations - arises in models that heavily overparameterised relative to the problem. Its possible to remove some of these representations. 
Different block structures result from different seeds but there are similarities between different models. 
In ImageNet, find that wider networks are better with scenes and deep networks better with consumer goods.},
  creationdate     = {2021-05-05},
  keywords         = {model architecture, deep learning, discovery, visualisation, metrics, explainability},
  modificationdate = {2023-01-25T09:49:31},
  owner            = {ISargent},
  year             = {2021},
}

@InProceedings{ZarkaGM2021,
  author    = {John Zarka and Florentin Guth and St{\'e}phane Mallat},
  booktitle = {International Conference on Learning Representations},
  title     = {Separation and Concentration in Deep Networks},
  url       = {https://openreview.net/forum?id=8HhkbjrWLdE},
  comment   = {try to structure network with as little learning as possible but which produces good separation of classes.
Wavelet filters that are not learned
Scattering network and pruning. I haven't really followed the video...},
  keywords  = {deep learning},
  owner     = {ISargent},
  creationdate = {2021-05-05},
  year      = {2021},
}

@InProceedings{LiznerskiRVFKM2021,
  author    = {Philipp Liznerski and Lukas Ruff and Robert A. Vandermeulen and Billy Joe Franks and Marius Kloft and Klaus Robert Muller},
  booktitle = {International Conference on Learning Representations},
  title     = {Explainable Deep One-Class Classification},
  url       = {https://openreview.net/forum?id=A5VV3UyIQz},
  comment   = {One class classification - identify anomalies.
Fully convolutional data description - FCN + hypersphere loss
Minimise distance to in-class examples and maximise distance to out class examples.
I can't quite grasp the leap from training to the anomaly maps - I suspect this is trained on anomaly maps.},
  keywords  = {deep learning, anomaly detection},
  owner     = {ISargent},
  creationdate = {2021-05-05},
  year      = {2021},
}

@InProceedings{CsordasvS2021,
  author       = {R{\'o}bert Csord{\'a}s and Sjoerd van Steenkiste and J{\''u}rgen Schmidhuber},
  booktitle    = {International Conference on Learning Representations},
  title        = {Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks},
  url          = {https://openreview.net/forum?id=7uVcpu-gMD},
  comment      = {Look at compositionality and modularity in trained networks

Use weight masking to understand how different combinations of subnets work in trained network

Try training 1 net to do addition and multiplication and demonstrated there is separation in network but also some shared weights

Also a task that hidden weights would be expected to be shared - addition of first 2 or 2nd 2 of 4 inputs. However, found separation in the internal network. The subnetworks performed well on the opposite task confirming their separation.

Continually learning MNIST with FCN - no sharing of similar function for similar tasks.

interesting because if correct add to other papers suggesting successful networks are overcomplete.},
  creationdate = {2021-05-05},
  keywords     = {machine learning, metrics, model architecture},
  owner        = {ISargent},
  year         = {2021},
}

@InProceedings{GeGZP2021,
  author    = {Songwei Ge and Vedanuj Goswami and Larry Zitnick and Devi Parikh},
  booktitle = {International Conference on Learning Representations},
  title     = {Creative Sketch Generation},
  url       = {https://openreview.net/forum?id=gwnoVHIES05},
  comment   = {Facebook AI
Sketch datasets are a bit limited so created a new set in which the sketcher semantically labels the parts. 
Resulted is a human-AI collaboration in sketch generation},
  owner     = {ISargent},
  creationdate = {2021-05-05},
  year      = {2021},
}

@InProceedings{ForetKMN2021,
  author    = {Pierre Foret and Ariel Kleiner and Hossein Mobahi and Behnam Neyshabur},
  booktitle = {International Conference on Learning Representations},
  title     = {Sharpness-aware Minimization for Efficiently Improving Generalization},
  url       = {https://openreview.net/forum?id=6Tm1mposlrM},
  comment   = {SAM
Sharpness is in the error surface - avoid this to benefit generalisation.
Maximising the loss in a neighbourhood reduces the population loss.

Also finetuning with SAM.

Robust to noisy labels.

Find that m-sharpness is a good predictor of generalisation.

I like the mental visualisation of this - why we shouldn't push towards perfect trianing loss - because this minimum depends on data, batch size, hyperparamenters and how we actually aim for a local maximum loss to get a global minimum loss.},
  keywords  = {training, generalisation, error surface, machine learning},
  owner     = {ISargent},
  creationdate = {2021-05-05},
  year      = {2021},
}

@InProceedings{FangWWZYL2021,
  author    = {Zhiyuan Fang and Jianfeng Wang and Lijuan Wang and Lei Zhang and Yezhou Yang and Zicheng Liu},
  booktitle = {International Conference on Learning Representations},
  title     = {{\{}SEED{\}}: Self-supervised Distillation For Visual Representation},
  url       = {https://openreview.net/forum?id=AHm3dbp7D1D},
  comment   = {Contrastive learning does not work well with small networks
Conjecture that this is because smaller networks with fewer parameters cannot effectively discriminate instances in a large dataset. Propose distilling from a large network rather than trying to train the small architecture directly.
SEED is self-supervised training of smaller architecture based on frozen large architecture.
Smaller network tries to learn the large network's representation (final layer?)},
  keywords  = {model architecture, deep learning, self supervised learning},
  owner     = {ISargent},
  creationdate = {2021-05-05},
  year      = {2021},
}

@InProceedings{ZhouZLNCE2021,
  author    = {Sharon Zhou and Eric Zelikman and Fred Lu and Andrew Y. Ng and Gunnar E. Carlsson and Stefano Ermon},
  booktitle = {International Conference on Learning Representations},
  title     = {Evaluating the Disentanglement of Deep Generative Models through Manifold Topology},
  url       = {https://openreview.net/forum?id=djwS0m4Ft_A},
  comment   = {Disentaglement - pull out different factores that are meaninful in data.
Usually more latent dimensions than factors.
Present a metric for measuring the disentanglement in a model.},
  keywords  = {machine learning, metrics, discovery},
  owner     = {ISargent},
  creationdate = {2021-05-05},
  year      = {2021},
}

@InProceedings{AghajanyanSGGZG2021,
  author    = {Armen Aghajanyan and Akshat Shrivastava and Anchit Gupta and Naman Goyal and Luke Zettlemoyer and Sonal Gupta},
  booktitle = {International Conference on Learning Representations},
  title     = {Better Fine-Tuning by Reducing Representational Collapse},
  url       = {https://openreview.net/forum?id=OQ08SN70M1V},
  comment   = {When finetuning we don't want the weights to change dramatically but its been shown (see papare for refs) that SGD tends to move weights too far. Natural gradient is shift in representation space rather than euclidean space (see paper for ref) is better. Could explicitly bound the steps in SGD but other papers have shown this isn't great. This paper gives a method of approximating natural gradient. What I like about this presentation is that it builds well on other's work in the area.},
  keywords  = {machine learning, transfer leanring, fine tuning, model training},
  owner     = {ISargent},
  creationdate = {2021-05-05},
  year      = {2021},
}

@InProceedings{ChenGW2021,
  author    = {Wuyang Chen and Xinyu Gong and Zhangyang Wang},
  booktitle = {International Conference on Learning Representations},
  title     = {Neural Architecture Search on ImageNet in Four {\{}GPU{\}} Hours: A Theoretically Inspired Perspective},
  url       = {https://openreview.net/forum?id=Cnon5ezMHtu},
  comment   = {Neural architeure search NAS uses a lot of compute and isn't yet solved. Look at two ways of assessing how good a found neural architectures are.
Trainability: Neural Tangent Kernal NTK has a strong correlation with accuracy
Expressivity: number of linear regions - number of unique ReLU activation patterns has a strong correlation with
Formulate a method that is training-free and label-free for efficient NAS},
  keywords  = {architecture, neural networks, machine learning, metrics},
  owner     = {ISargent},
  creationdate = {2021-05-05},
  year      = {2021},
}

@InProceedings{ChenLRL2021,
  author       = {Boyuan Chen and Yu Li and Sunand Raghupathi and Hod Lipson},
  booktitle    = {International Conference on Learning Representations},
  title        = {Beyond Categorical Label Representations for Image Classification},
  url          = {https://openreview.net/forum?id=MyHwDabUHZm},
  comment      = {Children are trained to reognised objects with sounds ``dog'', ``cat'' - should we train networks with these words instead of labels? Input image, output is sound.
This paper attempts this with good accuracy. The high dimensionality of output encourages network to learn more robust representations. Clear advantage for high entropy and high dimensional labels. Ooh intersting - training images against sounds!},
  creationdate = {2021-05-05},
  keywords     = {deep leraning, model training},
  owner        = {ISargent},
  year         = {2021},
}

@InProceedings{TangM2021,
  author    = {Binh Tang and David S. Matteson},
  booktitle = {International Conference on Learning Representations},
  title     = {Graph-Based Continual Learning},
  url       = {https://openreview.net/forum?id=HHSEKOnPvaO},
  comment   = {Catastrophic Forgetting in Continual Learning when tuned with dataset with a different distribution - can't do old task any more. Provide task ID with each dataset.
Rehearsal approaches replay old datasets as new dataests come in to prevent forgetting. Most of these approaches learn correlations between samples. In biological systems learn relational structures between examples. Propose method to explicitly learn graph that connects similar samples.},
  keywords  = {deep learning, fine tuning},
  owner     = {ISargent},
  creationdate = {2021-05-05},
  year      = {2021},
}

@InProceedings{XuYR2021,
  author    = {Da Xu and Yuting Ye and Chuanwei Ruan},
  booktitle = {International Conference on Learning Representations},
  title     = {Understanding the role of importance weighting for deep learning},
  url       = {https://openreview.net/forum?id=_WnwtieRHxM},
  comment   = {I haven't really grasped this, however, its looking at the impact of applying importance weighting in training deep networks, specifically CNNs, in which it has previously been shown https://papers.nips.cc/paper/2018/file/0e98aeeb54acf612b9eb4e48a269814c-Paper.pdf that there is an implicit bias of gradient decent that is not present in fully connected networks (I think). I understand that this bias means that the optimal solution in this condition is based on regions with greater separability of the classes. I haven't particuarly grasped the impact of importance weighting from this current paper, however. :-/},
  keywords  = {deep learning, model training, bias},
  owner     = {ISargent},
  creationdate = {2021-05-05},
  year      = {2021},
}

@InProceedings{WangCCTH2021,
  author    = {Ruochen Wang and Minhao Cheng and Xiangning Chen and Xiaocheng Tang and Cho-Jui Hsieh},
  booktitle = {International Conference on Learning Representations},
  title     = {Rethinking Architecture Selection in Differentiable {NAS}},
  url       = {https://openreview.net/forum?id=PKubaeJkw3},
  comment   = {Neural architecture search has huge computation cost. Liu 2019 proposed Differentiable Architecture Search - DARTS which constructs a supernet from subgraphs. Architecture is defined as a directed acyclic graph in which nodes are feature maps and edges are operations (such as conv 3x3, skip, etc). Supernet training ends with selection of strongest operation based on a magnitude measure, alpha. However, DARTS is not very robust - with training selected architecture accuracy degrades. There is a domination of skip connections which tend to get selected over convolutions. 
This work shows that skip domination is reasonable. Demonstrates that DARTS supernet resembles ResNet - i.e. That  it is robust to reordering at test time because it can be considered to be a  ensemble of smaller network and thus performs unrolled estimation. But this does not mean that skip connection is not the best choice for a supernet - the alpha value is perhaps not a good way of computing operation strength and some operations with a small alpha are more important for accuracy. Suggest using accuracy to measure operation strength although this is operationally costly because training supernet to convergence before measuring accuracy. Instead suggest perturbation based selection (PT) in which the importance of an operation is evaluated by the validation accuracy when that operation is removed. Degradation of robustness does not occur with DARTS-PT and more recent version of DARTS and NAS algorithms. In fact don't use alpha at all.},
  keywords  = {deep learning, architecture},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{ArakelyanDMC2021,
  author    = {Erik Arakelyan and Daniel Daza and Pasquale Minervini and Michael Cochez},
  booktitle = {International Conference on Learning Representations},
  title     = {Complex Query Answering with Neural Link Predictors},
  url       = {https://openreview.net/forum?id=Mos9F9kDwkz},
  comment   = {First Neural Link Prediction - fill gaps in knowledge graphs
Then complex queries in incomplete graphs
Graph is passed to NN but
-need to train with millions of queries which are not available
Train with simple queries, turn each into optimisation problem, than solve for interactions...?
Results are explainable and errors in reasoning can be detected and corrected},
  keywords  = {graphs, neural networks},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{NitandaS2021,
  author    = {Atsushi Nitanda and Taiji Suzuki},
  booktitle = {International Conference on Learning Representations},
  title     = {Optimal Rates for Averaged Stochastic Gradient Descent under Neural Tangent Kernel Regime},
  url       = {https://openreview.net/forum?id=PULSD5qI2N1},
  comment   = {All about improving the optimisation of networks and gradient descent. Characterise function space where optimisation is performed
NTK is an inner product of something to do with the gradient...},
  keywords  = {machine learning, model training},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{ZhangTZCLHF2021,
  author    = {Aston Zhang and Yi Tay and SHUAI Zhang and Alvin Chan and Anh Tuan Luu and Siu Hui and Jie Fu},
  booktitle = {International Conference on Learning Representations},
  title     = {Beyond Fully-Connected Layers with Quaternions: Parameterization of Hypercomplex Multiplications with \$1/n\$ Parameters},
  url       = {https://openreview.net/forum?id=rcQdycl0zyk},
  comment   = {A quaternion is 4D hypercomplex that have been used instead of convolutions and to reduce cost of fully-connected architecture. Quaternions are only 4D or other option are 8D or 16D. Instead this paper proposes replacing fully-connected layer with parameterised hypercomplex multiplications PHM and matrix multiplication with Hamilton product of quaternions (product of quaternion is a Hamilton product).},
  keywords  = {machine learning, model architectures},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{RichardMGKBTS2021,
  author    = {Alexander Richard and Dejan Markovic and Israel D. Gebru and Steven Krenn and Gladstone Alexander Butler and Fernando Torre and Yaser Sheikh},
  booktitle = {International Conference on Learning Representations},
  title     = {Neural Synthesis of Binaural Speech From Mono Audio},
  url       = {https://openreview.net/forum?id=uAX8q61EVRu},
  comment   = {Experience of being in virtual reality - training NN to get spatialised sound that is more natural.},
  keywords  = {virtual reality, machine learning, audio},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{GempMVG2021,
  author    = {Ian Gemp and Brian McWilliams and Claire Vernade and Thore Graepel},
  booktitle = {International Conference on Learning Representations},
  title     = {EigenGame: {\{}PCA{\}} as a Nash Equilibrium},
  url       = {https://openreview.net/forum?id=NzTU59SYbNq},
  comment   = {Google. Propose SVD/PCA as a game. Idea is to think about machine learning from a multi-agent approach - more than one objective, decentralisation is possible and biologially plausible. A lot of effort has gone into designing loss functions over recent years.
SVD is the basis of many things - sorting, PCA, spectral clustering, latent semantic analysis, least squares
A player for each axis. Players balance two goals - to maximise variance and remain orthogonal to all other players.
Possibly use EigenGame to interpret whole trained networks as it demonstrates the main characteristics of activation of units throughout whole network},
  keywords  = {model training, machine learning, multi-agent, metrics},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{PfaffFSB2021,
  author    = {Tobias Pfaff and Meire Fortunato and Alvaro Sanchez-Gonzalez and Peter Battaglia},
  booktitle = {International Conference on Learning Representations},
  title     = {Learning Mesh-Based Simulation with Graph Networks},
  url       = {https://openreview.net/forum?id=roNqYL0_XP},
  comment   = {Meshes are a much more efficient way of simmulating complex dynamics. Proposed MeshGraphNets, a framework for learning mesh-based simulations using graph neural networks which learn how to form and deform mesh based on measured mesh. Can even train on low resolution data and generate in high resolution.},
  keywords  = {mesh, graph networks},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{RamsauerEtAl2021,
  author       = {Hubert Ramsauer and Bernhard Sch{\''a}fl and Johannes Lehner and Philipp Seidl and Michael Widrich and Lukas Gruber and Markus Holzleitner and Thomas Adler and David Kreil and Michael K Kopp and G{\''u}nter Klambauer and Johannes Brandstetter and Sepp Hochreiter},
  booktitle    = {International Conference on Learning Representations},
  title        = {Hopfield Networks is All You Need},
  url          = {https://openreview.net/forum?id=tL89RnzIiCd},
  comment      = {Redesign deep learning using Hopfield networks. What astonished me about this paper is that it has 13 authors and they are all white men.},
  creationdate = {2021-05-06},
  owner        = {ISargent},
  year         = {2021},
}

@InProceedings{BuchananGW2021,
  author       = {Sam Buchanan and Dar Gilboa and John Wright},
  booktitle    = {International Conference on Learning Representations},
  title        = {Deep Networks and the Multiple Manifold Problem},
  url          = {https://openreview.net/forum?id=O-6Pm_d_Q-},
  comment      = {References the finding in PopeZAGG2021 that many image datasets are low dimensional and looks at why depth and width benefit problems with a low dimensional manifold. This is done by creating a dataset that consists of two classes each on a different low dimensional manifold with know minimum separation and curvature (literally these are two wiggly lines on the surface of a sphere). 
''Our analysis demonstrates concrete benefits of depth and width in the context of a practically-motivated model problem: the depth acts as a fitting resource, with larger depths corresponding to smoother networks that can more readily separate the class manifolds, and the width acts as a statistical resource, enabling concentration of the randomly-initialized network and its gradients.''},
  creationdate = {2021-05-06},
  keywords     = {deep learning, manifolds, architecture},
  owner        = {ISargent},
  year         = {2021},
}

@InProceedings{LavoieMarchildonAC2021,
  author    = {Samuel Lavoie-Marchildon and Faruk Ahmed and Aaron Courville},
  booktitle = {International Conference on Learning Representations},
  title     = {Integrating Categorical Semantics into Unsupervised Domain Translation},
  url       = {https://openreview.net/forum?id=IMPA6MndSXU},
  comment   = {Want to transfer between domains without supervision. Propose to incorporate shared semantics between domains. 
Learn an embedding of the samples. Cluster one domain using one method and the other domain with another clustering method (weird) - not sure why.},
  keywords  = {domain adaptation, machine learning, transfer learning, clustering, unsupervised learning},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{NguyenCL2021,
  author    = {Timothy Nguyen and Zhourong Chen and Jaehoon Lee},
  booktitle = {International Conference on Learning Representations},
  title     = {Dataset Meta-Learning from Kernel Ridge-Regression},
  url       = {https://openreview.net/forum?id=l-PrrQrK0QR},
  comment   = {Noisy data sometimes achieve better results than clean data with training networks. This paper suggests methods of distilling datasets for kernal ridge regression. Interesting to see a data-centric approach.},
  keywords  = {machine learning, ridge regression, data distillation},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{XiaoEIM2021,
  author    = {Kai Yuanqing Xiao and Logan Engstrom and Andrew Ilyas and Aleksander Madry},
  booktitle = {International Conference on Learning Representations},
  title     = {Noise or Signal: The Role of Image Backgrounds in Object Recognition},
  url       = {https://openreview.net/forum?id=gl3D-xY7wLq},
  comment   = {Charactersing the exact correlations that models depend on. Specifically looking at bias of models for using image backgrounds. Disentagle foreground and background using annotated bounding boxes and segmentation and then recombine to isolate the effect of changing the background. Train on unmodified images and then test on different backgrounds. Background changes impact accuracy, also find that models are particularly bias towards certain backgrounds. More robust models are less affected by backgrounds.},
  keywords  = {metrics, deep learning, model understanding},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{UtreraKEKM2021,
  author    = {Francisco Utrera and Evan Kravitz and N. Benjamin Erichson and Rajiv Khanna and Michael W. Mahoney},
  booktitle = {International Conference on Learning Representations},
  title     = {Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification},
  url       = {https://openreview.net/forum?id=ijJZbomCJIm},
  comment   = {More robust features seem to be learned using adversarial training (Tsipiras et al 2019). Demonstrate that this leads to better transfer to new domains.

Find that adversarially-trained models are less sensitive to texture perturbations and use shapes more than textures for classification. Adversarial perturbations are very good for robustness and adding noise is better than nothing.

Weirdly refer to adversarially-trained models as 'robust models' and non-adversarially-trained models (in which there are not perturbations in the training) as 'natural models' although the scope of what is tested until the class 'natural model' is not clear.},
  keywords  = {deep learning, adversarial training, CNN, metric, model understanding},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{WuDN2021,
  author    = {Xiaoxia Wu and Ethan Dyer and Behnam Neyshabur},
  booktitle = {International Conference on Learning Representations},
  title     = {When Do Curricula Work?},
  url       = {https://openreview.net/forum?id=tW4QEInpni},
  comment   = {Ordered learning has shown benefits but is not often used. GPT-3 used a sort of ordering. 
What is ordering - perform a study into the difficulty of images and find that there is a consistency across different CNNs regarding difficulty in a dataset so this is independant of model (FC networks this is less so).
Perform a detailed study 25,000 models over 4 datasets of the benefit of curricula and find that benefits of ordered learning is entirely down to dataset size. 
However, do find that ordered learning works better when there is limited time/budget.},
  keywords  = {model training, dataset},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{LiZA2021,
  author    = {Zhiyuan Li and Yi Zhang and Sanjeev Arora},
  booktitle = {International Conference on Learning Representations},
  title     = {Why Are Convolutional Nets More Sample-Efficient than Fully-Connected Nets?},
  url       = {https://openreview.net/forum?id=uCY5MuAxcxU},
  comment   = {Can we rigorously justify the gap between convnets and FC nets?
FC can simulate convnets but this is constrained not y expressiveness but training algorithm and network architecture.
Demonstrate that ConvNets have better inductive bias than FC nets.
Number of samples required for training is proportional to the dimensionality of FC nets whereas it is a constant number of samples for convnets.},
  keywords  = {machine learning, model architecture},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{LiTZWHD2021,
  author    = {Hao Li and Chenxin Tao and Xizhou Zhu and Xiaogang Wang and Gao Huang and Jifeng Dai},
  booktitle = {International Conference on Learning Representations},
  title     = {Auto Seg-Loss: Searching Metric Surrogates for Semantic Segmentation},
  url       = {https://openreview.net/forum?id=MJAqnaC2vO1},
  comment   = {Misalignment between loss function and evaluation (e.g. IoU). Search through the loss function space to find loss function surrogates that better align to evaluation metrics. Example mIoU - surrogate for this in loss function by replacing argmax with softmax, extend logic operations also add constraints to regularize the search space. Result are searched versions of evaluation metrics which apparently work well.},
  keywords  = {model training, segmentation, deep learning, loss function, metrics},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{WeiSCM2021,
  author    = {Colin Wei and Kendrick Shen and Yining Chen and Tengyu Ma},
  booktitle = {International Conference on Learning Representations},
  title     = {Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data},
  url       = {https://openreview.net/forum?id=rC8sJ4i6kaH},
  comment   = {Lots on using pseudolabels (labels applied to examples from a pretrained model potentially in another domain or possibly even a random model.},
  keywords  = {deep learning, self supervised learning},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{HansenJSAAEPW2021,
  author    = {Nicklas Hansen and Rishabh Jangir and Yu Sun and Guillem Aleny{\`a} and Pieter Abbeel and Alexei A Efros and Lerrel Pinto and Xiaolong Wang},
  booktitle = {International Conference on Learning Representations},
  title     = {Self-Supervised Policy Adaptation during Deployment},
  url       = {https://openreview.net/forum?id=o_V-MjyyGV_},
  comment   = {In reinforcement learning using deep model to learn policy, but policy in one environment may not generalise to another. Can radomize elements to make policies more robust but too much randomization and randomization on the wrong things can cause more problems and so need a lot of data. 
So learn policy in single environment, try it in target environment, then adapt policy at deployment time - online adaptation. Use self supervision at same time as RL to obtimise a shared encoder. This encoder is then used in deployment and use self supervision with augmentation to update policies. Can be used to train a physical robot with simulated data at training time.},
  keywords  = {online learning, machine learning, reinforcement learning, fine tuning},
  owner     = {ISargent},
  creationdate = {2021-05-07},
  year      = {2021},
}

@Article{MaharanaN2018,
  author       = {Maharana, Adyasha and Nsoesie, Elaine Okanyene},
  title        = {{Use of Deep Learning to Examine the Association of the Built Environment With Prevalence of Neighborhood Adult Obesity}},
  doi          = {10.1001/jamanetworkopen.2018.1535},
  eprint       = {https://jamanetwork.com/journals/jamanetworkopen/articlepdf/2698635/maharana\_2018\_oi\_180097.pdf},
  issn         = {2574-3805},
  number       = {4},
  pages        = {e181535-e181535},
  url          = {https://doi.org/10.1001/jamanetworkopen.2018.1535},
  volume       = {1},
  abstract     = {More than one-third of the adult population in the United States is obese. Obesity has been linked to factors such as genetics, diet, physical activity, and the environment. However, evidence indicating associations between the built environment and obesity has varied across studies and geographical contexts.To propose an approach for consistent measurement of the features of the built environment (ie, both natural and modified elements of the physical environment) and its association with obesity prevalence to allow for comparison across studies.The cross-sectional study was conducted from February 14 through October 31, 2017. A convolutional neural network, a deep learning approach, was applied to approximately 150 000 high-resolution satellite images from Google Static Maps API (application programing interface) to extract features of the built environment in Los Angeles, California; Memphis, Tennessee; San Antonio, Texas; and Seattle (representing Seattle, Tacoma, and Bellevue), Washington. Data on adult obesity prevalence were obtained from the Centers for Disease Control and Prevention's 500 Cities project. Regression models were used to quantify the association between the features and obesity prevalence across census tracts.Model-estimated obesity prevalence (obesity defined as body mass index ≥30, calculated as weight in kilograms divided by height in meters squared) based on built environment information.The study included 1695 census tracts in 6 cities. The age-adjusted obesity prevalence was 18.8\\\% (95\\\% CI, 18.6\\%-18.9\\%) for Bellevue, 22.4\\\% (95\\\% CI, 22.3\\%-22.5\\%) for Seattle, 30.8\\\% (95\\\% CI, 30.6\\%-31.0\\%) for Tacoma, 26.7\\\% (95\\\% CI, 26.7\\%-26.8\\%) for Los Angeles, 36.3\\\% (95\\\% CI, 36.2\\%-36.5\\%) for Memphis, and 32.9\\\% (95\\\% CI, 32.8\\%-32.9\\%) for San Antonio. Features of the built environment explained 64.8\\\% (root mean square error [RMSE],4.3) of the variation in obesity prevalence across all census tracts. Individually, the variation explained was 55.8\\\% (RMSE,3.2) for Seattle (213 census tracts), 56.1\\\% (RMSE,4.2) for Los Angeles (993 census tracts), 73.3\\\% (RMSE,4.5) for Memphis (178 census tracts), and 61.5\\\% (RMSE,3.5) for San Antonio (311 census tracts).This study illustrates that convolutional neural networks can be used to automate the extraction of features of the built environment from satellite images for studying health indicators. Understanding the association between specific features of the built environment and obesity prevalence can lead to structural changes that could encourage physical activity and decreases in obesity prevalence.},
  comment      = {Find that obesity prevalence can be predicted from satellite data - indicating that aspects of the built environment correlate with obesity.},
  creationdate = {2021-05-07},
  journal      = {JAMA Network Open},
  keywords     = {satellite imagery, machine learning, socio-econmics},
  month        = {08},
  owner        = {ISargent},
  year         = {2018},
}

@InProceedings{KrahenbuhlK2011,
  author       = {Philipp Kr{\''{a}}henb{\''{u}}hl and Vladlen Koltun},
  title        = {Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials},
  url          = {http://graphics.stanford.edu/projects/densecrf/densecrf.pdf},
  comment      = {The NIPS 2011 DenseCRF paper. Some helpful descriptions given here https://graphics.stanford.edu/projects/densecrf/

''We use a fully connected CRF that establishes pairwise potentials on all pairs of pixels in the image.''

Fully connected in that there is an edge from every pixel (node) to every other pixel in the image (hence, dense).

Unfortunately I don't understand the rest to understand how they then approximate this function to achieve efficient inference.},
  creationdate = {2021-05-07},
  owner        = {ISargent},
  year         = {2011},
}

@Article{PattersonEtAl2021,
  author    = {David Patterson and Joseph Gonzalez and Quoc Le and Chen Liang and Lluis-Miquel Munguia and Daniel Rothchild and David So and Maud Texier and Jeff Dean},
  title     = {Carbon Emissions and Large Neural Network Training},
  eprint    = {2104.10350},
  url       = {https://arxiv.org/abs/2104.10350},
  abstract  = {The computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information. We calculate the energy use and carbon footprint of several recent large models-T5, Meena, GShard, Switch Transformer, and GPT-3-and refine earlier estimates for the neural architecture search that found Evolved Transformer. We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO2e vary ~5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. Specific datacenter infrastructure matters, as Cloud datacenters can be ~1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be ~2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to ~100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO2e explicit when practical. We are working to be more transparent about energy use and CO2e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark.},
  keywords  = {cloud computing, carbon emissions, environment, deep learning},
  owner     = {ISargent},
  priority  = {prio1},
  creationdate = {2021-05-20},
  year      = {2021},
}

@Article{TalebiM2021,
  author        = {Hossein Talebi and Peyman Milanfar},
  title         = {Learning to Resize Images for Computer Vision Tasks},
  eprint        = {2103.09950},
  archiveprefix = {arXiv},
  comment       = {From TheBatch:
a learned image preprocessor that improved the accuracy of image recognition models trained on its output 
The network comprises a bilinear resizer layer sandwiched between convolutional layers to enable it to accept any input image size. 
•	The authors downsized ImageNet examples to 224x224 using a garden-variety bilinear resizer and used them to train a DenseNet-121. This resizer-classifier pair served as a baseline.
•	They further trained the DenseNet-121 while training their resizer jointly, optimizing for both classification accuracy and input size.},
  keywords      = {deep learning, data processing, datasets},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-05-27},
  year          = {2021},
}

@Article{LakkarajuKCH2016,
  author           = {Himabindu Lakkaraju and Ece Kamar and Rich Caruana and Eric Horvitz},
  title            = {Identifying Unknown Unknowns in the Open World: Representations and Policies for Guided Exploration},
  eprint           = {1610.09064},
  url              = {https://arxiv.org/abs/1610.09064},
  archiveprefix    = {arXiv},
  comment          = {Unknown unknowns are those erroneous predictions that are made with high confidence. They occur because of a mismatch between the training data and the data used at deployment and could be cause by the model is using wrong assumptions for prediction (e.g. all huskies are found in snow).

The only way to identify them is to use an oracle that confirms the correctness of a prediction.

Assumptions:
- Unknown unknowns arising due to biases in training data typically occur in certain specific portions of the feature space and not at random
- the features available in the data can effectively characterize different kinds of unknown unknowns, but the biases in the training data prevented the predictive model from leveraging these discriminating features for prediction

This paper develops an approach that identifies possible unknown unknowns using two phases (2 algorithms):
- multiple partitioning of the data in a way that is intended to draw more unknown unknowns together in partitions (low entropy)
- selecting examples for labelling by the oracle and then using the result to update the expected utility of selecting from each partition

The problem is formulated for a particular class (one for which it would be most detrimental to get wrong) with a threshold confidence (i.e. so that the most confident misclassified examples are found). 

The partitioning is performed by developing a set of patterns, each being a tuple (feature, operator, value) whereby the operator can be ==, ~=, <, etc (so creating rules with binary outcomes). These patterns are found using frequent pattern mining (e.g. using Apriori) and this includes both features in the data and the confidence value. The algorithm uses these, with the dataset and optimises over 5 weighted metrics which describe the separation of partitions and so try to minimise the number of patterns. Its a bit like kmeans but with fewer features (the patterns). 

They test against kmeans by looking at the entropy of unknown unknowns (where the lowest entropy would mean that the unknown unknowns are more grouped - which is what we ant) and find the results of their approach is better than kmeans using features and confidence (but not always by much) and considerably better than using kmeans alone with just features or just confidence.

The selection of examples for the oracle involves, I think, first randomly choosing one example from every partition and then using the outcome (whether the example in that partition was erroneously predicted) to update the expected value for there being unknown unknowns in that partition. This way the benefit (finding all the unknown unknowns) can be optimised against the cost (having to request a label from the oracle).

Say that their method allows an interpretation of why the algorithm misclassified - I don't really see how this works and they don't go into much detail.

Well worth trying to repeat - who has applied or developed this work?},
  creationdate     = {2021-06-14},
  keywords         = {trust, ethics, metrics, machine learning},
  modificationdate = {2023-12-11T07:59:13},
  owner            = {ISargent},
  primaryclass     = {cs.AI},
  year             = {2016},
}

@Article{AmershiCKK2014,
  author       = {Amershi, Saleema and Cakmak, Maya and Knox, William Bradley and Kulesza, Todd},
  title        = {Power to the People: The Role of Humans in Interactive Machine Learning},
  doi          = {10.1609/aimag.v35i4.2513},
  number       = {4},
  pages        = {105-120},
  url          = {https://ojs.aaai.org/index.php/aimagazine/article/view/2513},
  volume       = {35},
  abstractnote = {Intelligent systems that learn interactively from their end-users are quickly becoming widespread. Until recently, this progress has been fueled mostly by advances in machine learning; however, more and more researchers are realizing the importance of studying users of these systems. In this article we promote this approach and demonstrate how it can result in better user experiences and more effective learning systems. We present a number of case studies that characterize the impact of interactivity, demonstrate ways in which some existing systems fail to account for the user, and explore new ways for learning systems to interact with their users. We argue that the design process for interactive machine learning systems should involve users at all stages: explorations that reveal human interaction patterns and inspire novel interaction methods, as well as refinement stages to tune details of the interface and choose among alternatives. After giving a glimpse of the progress that has been made so far, we discuss the challenges that we face in moving the field forward.},
  comment      = {''three key points. 

First, interactive machine learning differs from traditional machine learning. The interaction cycles in interactive machine learning are typically more rapid, focused, and incremental than in traditional machine learning. This increases the opportunities for users to affect the learner and, in turn, for the learner to affect the users. As a result, the contributions of the system and the user to the final outcome cannot be decoupled, necessitating an increased need to study the system together with its potential users.

Second, explicitly studying the users of learning systems is critical to advancing this field. Formative user studies can help identify user needs and desires, and inspire new ways in which users could interact with machine-learning systems. User studies that evaluate interactive machinelearning systems can reveal false assumptions about potential users and common patterns in their interaction with the system. User studies can also help to identify common barriers faced by users when novel interfaces are introduced. 

Finally, the interaction between learning systems and their users need not be limited. We can build powerful interactive machine-learning systems by giving more control to end users than the ability to label instances, and by providing users with more transparency than just the learner's predicted outputs. However, more control for the user and more transparency from the learner do not automatically result in better systems, and in some situations may not be appropriate or desired by end users. We must continue to evaluate novel interaction methods with real users to understand whether they help or hinder users' goals''.},
  creationdate = {2021-06-15},
  journal      = {AI Magazine},
  keywords     = {human in the loop, machine learning, interactive machine learning},
  month        = {12},
  owner        = {ISargent},
  year         = {2014},
}

@Article{NushiKH2018,
  author           = {Besmira Nushi and Ece Kamar and Eric Horvitz},
  title            = {Towards Accountable AI: Hybrid Human-Machine Analyses for Characterizing System Failure},
  eprint           = {1809.07424},
  url              = {https://arxiv.org/abs/1809.07424},
  archiveprefix    = {arXiv},
  comment          = {Present Pandora which characterises failures and shortcomings of machine learning systems.

''Content-based views use detailed ground truth or automatically detected content (i.e., input data) features to learn common situations associated with poor performance. For instance, a face recognizer could report that the system may make more mistakes in recognizing faces of old men wearing eyeglasses. Component-based views instead model the relationship between the uncertainty as well as the individual performance state of each component and system failures. For a face recognizer that includes a face detector, component-based views can describe how often the system fails when the detector is uncertain (i.e., low confidence) or wrong (i.e., false detection)''.

I feel like this moves on from LakkarajuKCH2016 but I can't see the direct route between them. However, this work then developed into https://erroranalysis.ai/ (https://github.com/microsoft/responsible-ai-widgets)},
  creationdate     = {2021-06-15},
  keywords         = {ethics, trust, artificial intelligence},
  modificationdate = {2023-12-11T07:59:40},
  owner            = {ISargent},
  primaryclass     = {cs.LG},
  year             = {2018},
}

@Article{HuangRCBM2019,
  author        = {Bohao Huang and Daniel Reichman and Leslie M. Collins and Kyle Bradbury and Jordan M. Malof},
  title         = {Tiling and Stitching Segmentation Output for Remote Sensing: Basic Challenges and Recommendations},
  eprint        = {1805.12219},
  url           = {https://arxiv.org/abs/1805.12219},
  archiveprefix = {arXiv},
  comment       = {Paper about how to get the best out of segmentation of images when the goal is to segment an image that is larger than the input size of the model - e.g. RS data for a region. Reviews other work and suggests why error occur - e.g. by propagation of error caused by zero padding to deeper layers. Suggest at inference time only, make the input image size larger - the classification for the outer pixels can then be thrown away. Also find that some averaging of the pixel outputs from mutliple patches can result is slightly higher accuracy but think that enlarging the input and then clipping is most important.},
  keywords      = {image segmentation, deep learning},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-06-23},
  year          = {2019},
}

@Misc{EUAILegislationProposal2021,
  author           = {{European Union}},
  date             = {2021-04-21},
  title            = {Proposal for a Regulation laying down harmonised rules on artificial intelligence},
  url              = {https://digital-strategy.ec.europa.eu/en/library/proposal-regulation-laying-down-harmonised-rules-artificial-intelligence},
  abstract         = {Faced with the rapid technological development of AI and a global policy context where more and more countries are investing heavily in AI, the EU must act as one to harness the many opportunities and address challenges of AI in a future-proof manner. To promote the development of AI and address the potential high risks it poses to safety and fundamental rights equally, the Commission is presenting both a proposal for a regulatory framework on AI and a revised coordinated plan on AI.},
  comment          = {From The Batch
''The 100-plus page document divides AI systems into three tiers based on their level of risk. The definition of AI includes machine learning approaches, logic-based approaches including expert systems, and statistical methods. 
- The rules would forbid systems deemed to pose an “unacceptable” risk. These include real-time face recognition, algorithms that manipulate people via subliminal cues, and those that evaluate a person's trustworthiness based on behavior or identity. 
- The “high risk” category includes systems that identify people; control traffic, water supplies and other infrastructure; govern hiring, firing, or doling out essential services; and support law enforcement. Such systems would have to demonstrate proof of safety, be trained using high-quality data, and come with detailed documentation. Chatbots and other generative systems would have to let users know they were interacting with a machine. 
- For lower-risk applications, the proposal calls for voluntary codes of conduct around issues like environmental sustainability, accessibility for the disabled, and diversity among technology developers. 
- Companies that violate the rules could pay fines of up to 6 percent of their annual revenue. ``

Assessed 5 options of different degrees of regulatory intervention and prefers option 3+ in which ``Horizontal EU legislative instrument following a proportionate risk-based approach + codes of conduct for non-high-risk AI systems''. 

Estimates the costs to organisations of complying (does anyone ever check these estimates against the reality?).

Lists the fundamental rights that may be adversely affected by AI and that this regulation is aimed at protecting these rights from impacts of AI.

''The purpose of this Regulation is to improve the functioning of the internal market by laying down a uniform legal framework in particular for the development, marketing and use of artificial intelligence in conformity with Union values'' and I assume it would therefore apply to anyone trading with EU - the CE mark is required for AI-containing products.

As it may not be clear if our use of AI is high-risk or lower-risk (I presume not unacceptible risk), it seems prudent to be implementing systems to comply with the regulations for high-risk from the outset. These include provisions under:
- Risk management system
- Data and data governance (these two are relevant to Exeter PhD)
--- ``Training, validation and testing data sets shall be relevant, representative, free of errors and complete. They shall have the appropriate statistical properties''
--- ``Training, validation and testing data sets shall take into account ... specific geographical, behavioural or functional setting within which the high-risk AI system is intended to be used''
- Technical documentation
- Record keeping
--- ``designed and developed with capabilities enabling the automatic recording of events (`logs')''
--- ``ensure a level of traceability''
--- ``the logging capabilities shall provide, at a minimum:
------ start date and time and end date and time
------ reference database against which input data has been checked
------ input data
------ identification of the natural persons involved in the verification of the results
- Transparency
--- intended purpose
--- level of accuracy, robustness and cybersecurity
--- foreseeable circumstance ... which may lead to risks to the health and safety or fundamental rights
--- performance as regards the persons or groups of persons
--- appropriate, specifications for the input data
- Human oversight (relevant to Swansea PhD)
--- appropriate human-machine interface tools
- Accuracy, robustness and cybersecurity
--- appropriate level of accuracy
--- appropriate level of accuracy ... declared
--- resilient as regards errors, faults or inconsistencies
--- resilient as regards attempts by unauthorised third parties ... (`data poisoning') ... (`adversarial examples')},
  creationdate     = {2021-06-23},
  keywords         = {ethics, trust, artificial intelligence, EthicsWS},
  modificationdate = {2023-12-11T08:04:32},
  owner            = {ISargent},
  year             = {2021},
}

@Article{FieldHH1993,
  author       = {Field, D. J. and Hayes, A. and Hess, R. F.},
  date         = {1993 Jan},
  title        = {Contour integration by the human visual system: evidence for a local ``association field''},
  doi          = {10.1016/0042-6989(93)90156-q},
  issue        = {2},
  pages        = {173--93},
  url          = {https://pubmed.ncbi.nlm.nih.gov/8447091/},
  volume       = {33},
  comment      = {This is the one with images of distributed Gabor filters, some of which define a contour.

''Gestalt psychologists developed a list of “laws” to account for many of the known phenomena of perceptual grouping (e.g. Wertheimer, 1938).''
'' the “law of good continuation” is little more than a description of these phenomena. As an explanation, the law has provided little predictive power.''

Perform 5 experiements with oriented Gabor (?) filters tha describe a contour in some way against a background of identical filters.

''In each of these experiments, the elements in the path cannot be identified on the basis of the stimulus properties of the elements. These properties (e.g. spatial frequency, orientation, intensity, contrast, etc.) were the same for the elements in both the path and the background. The path is identified by the relative alignment of its constituent elements''

''observers are able to segregate the path from the background when the elements in the path differ up to 60 deg in orientation and when they are separated by distances up to 7 times their width ... the ability to detect the path is significantly worse when the elements are placed side-to-side as opposed to end-to-end, ... results suggest that the association between elements is stronger along the axis of the element than orthogonal to the axis''

''For a given element in our displays, there appears to be a region around the element where other elements group together and segregate from the background. We describe this region of association as an “association field”''

''The integration process thus appears to show strong joint constraints of position and orientation. ``

''We believe that the results of our study do not support the notion that the detection of the path is mediated by a cell with a receptive field that conforms to the layout of the path.''},
  creationdate = {2021-07-01},
  journal      = {Vision research},
  keywords     = {vision psychology},
  owner        = {ISargent},
  year         = {1993},
}

@Article{TriantafillouLZD2021,
  author       = {Eleni Triantafillou and Hugo Larochelle and Richard Zemel and Vincent Dumoulin},
  title        = {Learning a Universal Template for Few-shot Dataset Generalization},
  url          = {https://arxiv.org/abs/2105.07029},
  comment      = {Paper from Google. From The Batch:

''designed Few-shot Learning with a Universal Template (FLUTE). 
Key insight: Training some layers on several tasks while training others on only one reduces the number of parameters that need to be trained for a new task. Since fewer parameters need training, the network can achieve better performance with fewer training examples.
How it works: The authors trained a ResNet-18 to classify the eight sets in Meta-Dataset: ImageNet, Omniglot, Aircraft, Birds, Flowers, Quickdraw, Fungi, and Textures. Then they fine-tuned the model on 500 examples and tested it separately on Traffic Signs, MSCOCO , MNIST, CIFAR-10, and CIFAR-100.''},
  creationdate = {2021-07-01},
  keywords     = {transfer learning, deep learning},
  owner        = {ISargent},
  year         = {2021},
}

@Article{KimRS2018,
  author       = {Junkyung Kim and Matthew Ricci and Thomas Serre},
  date         = {15 June 2018},
  journaltitle = {Interface Focus},
  title        = {Not-So-CLEVR: learning same-different relations strains feedforward neural networks},
  issue        = {4},
  url          = {https://royalsocietypublishing.org/doi/10.1098/rsfs.2018.0011},
  volume       = {8},
  comment      = {From a special issue that Andrew Schofield and others put together. Paper showing what deep networks are not very good at (although at the end it seems that Siamese networks are actually OK at these tasks). Key finding is that feedforward neural networks are not able to ``efficiently and robustly learn visual relations''.

''Gülçehre \& Bengio [10], after showing how CNNs fail to learn a same-different task with simple binary `sprite' items, only managed to train a multi-layer perceptron on this task by providing carefully engineered training schedules.''

Consider CNNs and Relational Networks (RNs) and, later, Siamese networks.

''In this study, we probe the limits of feedforward neural networks, including CNNs and RNs, on visual-relation tasks .. systematic performance analysis ... reveals a dichotomy of visual-relation problems: hard same-different [SD] problems and easy spatial-relation [SR] problems .. CNNs solve same-different tasks only inefficiently, via rote memorization of all possible spatial arrangements of individual items ... we examine two models, the RN and a novel Siamese network ... the former struggles to learn the notion of sameness and tends to overfit to particular item features, but that the latter can render seemingly difficult visual reasoning problems rather trivial.''

''The comparatively weak effects of item size and item number shed light on the computational strategy used by CNNs to solve SD. Our working hypothesis is that CNNs learn `subtraction templates', filters with one positive region and one negative region (like a Haar or Gabor wavelet)''

''Taken together, these results suggest that, when CNNs learn a PSVRT condition, they are simply building a feature set tailored to the relative positional arrangements of items in a particular dataset, instead of learning the abstract `rule' per se ... This seems to be the case only in SR... suggests that the features learned by CNNs are not invariant rule detectors, but rather merely a collection of templates covering a particular distribution in the image space.''

''Is object individuation needed to solve visual relations?''

''feature vectors [in trained CNNs] will sometimes represent parts of the background, incomplete items or even multiple items because the network does not explicitly represent individual objects. This makes the `objects' used by an RN rather different from those discussed in the psychophysical literature, where perceptual objects are speculated to obey gestalt rules like boundedness and continuity''

''The mean ALC curves for the Siamese network on PSVRT were strikingly different from those of the CNN ... implies that object individuation makes visual relation detection a rather trivial problem for feedforward networks''

''key dissimilarities between current deep network models and various aspects of visual cognition ... adversarial perturbation ... poor generalization''

''The present study adds to this body of literature by demonstrating feedforward neural networks' fundamental inability to efficiently and robustly learn visual relations ... While learning feature templates for single objects appears tractable for modern deep networks, learning feature templates for arrangements of objects becomes rapidly intractable because of the combinatorial explosion in the requisite number of templates''

''substantial evidence that visual-relation detection in primates depends on re-entrant/feedback signals beyond feedforward, pre-attentive processes''

Sadly doesn't touch upon the results with the siamese networks in the disucssion.






''The SVRT is a collection of 23 binary classification problems in which opposing classes differ based on whether or not images obey an abstract rule''

My thoughts - thinking about CNNs as template creaters, looking at the SVRT problems, which are shape outlines with blank background and inner, the SR problems have simpler useful features such as adjoining or parallel edges for which a simple template may be learned. The SD problems would require more complexity with templates for each different shape of edge and the relationships between them.

''To address the limitations of SVRT, we constructed a new visual-relation benchmark consisting of two idealized problems ... from the dichotomy that emerged from experiment 1: SR and SD.'' This is the parametrized SVRT (PSVRT)

''Evidence that CNNs use rote memorization of examples was found in a study by Stabinger \& Rodriguez-Sanchez ... found that CNN accuracy was lower on datasets whose images were rendered with higher degrees of freedom in viewpoint.''

Intersting metric for learning: ``We trained the same CNN repeatedly from scratch over multiple subsets of the data in order to see if learnability depends on the dataset's image parameters ... averaged across the length of a training run as well as over multiple trials for each condition, yielding a scalar measure of learnability called `mean area under the learning curve' (mean ALC). The ALC is high when accuracy increases earlier and more rapidly throughout the course of training and/or when it converges to a higher final accuracy by the end of training.''},
  creationdate = {2021-07-05},
  keywords     = {quality, metrics, deep learning, vision},
  owner        = {ISargent},
  year         = {2018},
}

@Book{Syed2019,
  author           = {Matthew Syed},
  title            = {Rebel Ideas},
  comment          = {A quick and compelling read about how diverse thinking is essential to solve problems. Each chapter covers a single topic using real-world examples to illustrate the point.

1. Collective Blindness
Groups that don't have diversity of thought, no matter how talented the individuals, will have blind spots.
- Why the CIA failed to spot the 9/11 bombers despite all the clues that were left in the run up
- Experiment showing that Japanese and Americans focus on different aspects of a situation - Americans the objects, Japanese the context.
Together these are powerful

2. Rebels Versus Clones
This chapter introduces the central idea that no individual can have knowledge and insights over the whole of the problem space and so solutions will only be identified if the group contains people whose knowledge and experience covers the problem space best.
- How diversity in the Football Association's technical Advisory Board helped address problems in English men's football from 2016 despite not having many football experts Discusses homophilly ('birds of a feather flock together'') can lead to blindness of other perspectives
- Men-only councils in Sweden focussed on clearing snow from roads over pavements, women joining the council's identified that a more productive approach is the prioritise pedestrians, with economic benefits for the town 'Group wisdom' - the average of many predictions can be the best prediction
- economic forecasting
Meritocratic hiring isn't necessarily the solution - if we hire only on merits individually we may not consider the requirements of the team for diverse thinking. However, diversity has to be relevant, not arbitrary.
- Hiring for Bletchley Park deliberately used a diverse range of techniques to recruit a diverse group of people

3. Constructive Dissent
People who have different perspective need to also have a voice otherwise their insights are wasted. This is a particular problem with strict hierarchies.
- disaster on Everest where group were told to obey instructions and not question - those with different positions and knowledge didn't speak up even thought they observed things like an oncoming storm
- United Airlines 173 in 1978 ran out of fuel and crashed even thought the engineer could see the fuel gauge With a dominant leader, or dominant members, teams can begin to parrot a single view which loses the team their valuable diversity.
- research into team dynamics and meetings demonstrating how ineffective these often are are using the diversity they have Divides leadership into two types - dominance and prestige - the second is desirable and arises from a generous, prosocial approach and can be any member of a team. Creating psychological safety is important to obtain everyone's perspective.
- Bezos's method of meetings at Amazon that required attendees to write their positions in advance and then all read them

4. Innovation
Incremental innovation is when things are improved in steps, usually small, the results are not disruptive. Recombinant innovation is when ideas are brought together from different fields, and this can be dramatic
- It was 25 years before electrification changed manufacturing because it permitted a different approach way of running a factory that factory owners couldn't envision (they 'missed the point') and instead tried to use electricity like it was steam
- David Dudley Bloom invented the wheeled suitcase in the 1920's but shops couldn't see beyond the existing use of suitcases to understand why this was necessary Many great innovators are immigrants, and immigrants are twice as likely to become entrepreneurs. They have outsider mindset - an external perspective and exposure to different ideas and ways of thinking. It is possible to achieve this using techniques such as visualisation (e.g. of living abroad), assumption reversal (e.g. restaurants have no menus - instead dishes change daily and are customisable) Unlike goods, ideas can be shared, 'information spillover' and this often generates even more ideas. Innovation is not about individuals but connections.
- Great philosophers were not singletons - they were all linked to many other great minds. Their ideas were not generated in a vacuum but with the aide of those around them.
- This explains why many innovations occur simultaneously in unconnected places - all the necessary ideas are already in place for the new innovation to arise. It also explains why some societies advance quickly but others do not.
- Tasmania had a small population and slightly regressed after separation from mainland Australia due to sea-level rise, while the mainland developed it's technologies over the Holocene
- Silicon Valley developed and overtook the innovation at 'America's Technology Highway' because there was so much more mixing between organisations

5. Echo Chambers
We tend towards people who are similar to us, creating echo chambers who endorse and don't challenge our beliefs. The bigger the source of acquaintances, the more polarised are beliefs because there is such a huge pool of individuals. Large colleges experience them more than small ones. The internet is the ultimate huge pool.
Opposing ideas are often delegitimised, as are their carriers, making it difficult for people to accept ideas that conflict with their own views.
Even evidence rarely impacts these beliefs because the sources of the evidence has already been discredited - epistemic walls. Ad hominem is used instead of discussion whereby the source of the information is attached rather than the information itself.
The walls can only be broken down by developing trust.
- Derek White, brought up with and broadcasting white supremacist ideas was gently persuaded otherwise by an Orthodox Jewish peer at his small college who befriended him and earned Derek's respect

6. Beyond Average
A simple average my result in something that doesn't exist anywhere or suit anyone. We need to be wary of standardisation and create opportunities for flexibility.
- diet advice doesn't suit everyone
- US Airforce aircraft cockpits in 1940's that used average measurements for each part of a body fitted no-one at all and resulted in many fatal accidents
- Workers perform better when able to personalise their working space Some people have a mindset that accepts things as they are, others are always looking for a solution
- Applicants who used Chrome and Firefox were more likely to perform well and stay in a job (rather than those using the default browser)

7. The Big Picture
The major focus of society remain individuality and yet we have developed to the point that we not because of our individual intelligence but our collective brain. Bigger brains do not create our intelligence, they are a consequence of them.
- We could not have developed as we did if we didn't learn to produce fire and carry water which then allowed selection for individuals that put energy into brain mass (instead of digestion) and allowed us to hunt (and still carry enough water to cope with our excessive sweating).
Individually we were probably of similar intelligence to Neanderthals, but we have the advantage of being more social. The difference between chimpanzees and humans is more about the ability for social learning than individual intelligence - learning from others We need to use this knowledge well.
- We all have unconscious bias and so need approaches to prevent it affecting our decisions.
- Shadow boards, peopled by young members of staff have been shown to improve the performance of organisations by challenging and informing the board of directors
- Fostering an approach that shares knowledge is good for the group and the individual},
  creationdate     = {2021-07-12},
  keywords         = {ethics, trust},
  modificationdate = {2023-12-11T08:03:45},
  owner            = {ISargent},
  year             = {2019},
}

@Online{UKSAEthics2021,
  author           = {{UK Statistics Authority}},
  date             = {10 June 2021},
  title            = {Ethical considerations in the use of geospatial data for research and statistics},
  url              = {https://uksa.statisticsauthority.gov.uk/publication/ethical-considerations-in-the-use-of-geospatial-data-for-research-and-statistics/},
  comment          = {tick boxes for each of 16 points:
1. Think
2. Do no harm
3. Public Good
4. Methods and Quality
5. Transparency
6. Legal Compliance
7. Public Views and Engagement
8. Confidentiality and Data Security
9. The choice of geography
10. Disclosure by location
11. Ensuring inclusion
12. Avoiding bias
13. Unintended consequences
14. Double-check any strange results
15. Mapping and geovisualisation
 16. What will the user think?},
  creationdate     = {2021-07-12},
  keywords         = {ethics, EthicsWS, data, geospatial},
  modificationdate = {2022-12-10T10:05:09},
  owner            = {ISargent},
}

@Thesis{Kramer2021,
  author       = {Iris Kramer},
  institution  = {University of Southampton},
  title        = {Machine Learning for the Detection of Archaeological Site from Remote Sensor Data},
  type         = {phdthesis},
  comment      = {Considers the issues that are particularly challenging for detection of archaeology using aerial imagery:
Small datasets - solution is to data augmentation and transfer learning
Class imbalance - solution is to use specialist algorithm, RetinaNet
Noise (archaeology is the most overwritten human signature) - solution is to iteratively improve data by assessing results and relabelling where necessary
Scale (small objects) - looked at Feature Pyramid Network for this
Low contrast (again as a consequence of being overwritten) - solution is to use DTM visualisations
Non-conventional data format (rarely 8-bit rgb) - visualisations help here but suggest that pretraining with given format may help
Changing appearance (geology, geography, season)
Fuzzy site definitions - used non-maximal suppression to clean up overlapping detections

Undertook 2 main case studies:
 - in New Forest with OS aerial imagery (OS) and lidar (University of Cambridge and Natural England) identifying Bronze Age barrows
 - on Arran with lidar data (Historic Environment Scotland, HES) identifying prehistoric roundhouses, medieval or post-medieval shielding huts and cairns from agricultural field clearance (data set now shared publically, Kramer and Hare 2020)

New Forest case study:

Compared different dataset combinations with a simple 3-layer CNN. Find that NIR is important, 1m DTM is best *better than 0.5m DTM) but it seems that RGB is better than RGBN. Combining DTM with optical was worse.

Looked at transfer learning. 
Took features from layers of ResNet-50 pretrained with ImageNet and TopoNet (V2? 12 classes) and trained a SVM with these.
Also took pretrained ImageNet and TopoNet ResNet-50s and fine-tuned them using different strategies of freezing and training layers.
Also trained a SVM from scratch as a baseline. 

Using ImageNet backbone, no progressions with depth of layers (results over layers are noisy). Best performing layer was layer 48 (55%) but this was no better than scratch-trained SVM.  
With TopoNet backbone this is more progression towards better results with depth and overall resulted in better validation accuracy (71%).
Best results with fine-tuning are by training without frozen layers and are slightly better with TopoNet (83%) initialised weights.

Nice method of visualising results that shows 4 top examples for:
Most correct positive identification (most confident true positives)
Most incorrect negative identification (most confident false negatives)
Most correct negative identification (most confident true negative)
Most incorrect positive identification (most confident false positive)

Because DTMs are so promising, created 3-band imagery using hillshading from 3 different directions (225, 270 and 315 azimuth) and improved the result to 81%. Using the above method of visualising the best and worst results it was possible to understand why mistakes were made and this information was used to clean data set.

With cleaned dataset tried different combinations of DTM visualisation using options from Somrak et al 2020 (e.g. hillshade, slope, open positive, sky-view factor). Combinations were put into each of the 3 input bands. Open positive resulted in better results, hillshade with worse. Conclude that cleaning data is better than tweaking hyperparameters.

Arran case study:

Tested with different visualisations again and settled on 3-band combination of open positive, local dominance and slope.
Identified that issue with networks when classes are imbalanced and so moved to using RetinaNet (Fizyr implementation on Keras.
Best results achieved when training separately for each class.
Iterated results manual inspection from staff at HES - discovered 139 archaeological sites that were not in the training dataset.},
  creationdate = {2021-07-15},
  owner        = {ISargent},
  year         = {2021},
}

@Article{TolstikhinEtAl2021,
  author       = {Ilya Tolstikhin and Neil Houlsby and Alexander Kolesnikov and Lucas Beyer and Xiaohua Zhai and Thomas Unterthiner and Jessica Yung and Andreas Steiner and Daniel Keysers and Jakob Uszkoreit and Mario Lucic and Alexey Dosovitskiy},
  date         = {2021},
  title        = {MLP-Mixer: An all-MLP Architecture for Vision},
  url          = {https://arxiv.org/abs/2105.01601},
  comment      = {Google paper using MLP for image classification rather than CNN. This is what The Batch says:

Revenge of the Perceptrons
Why use a complex model when a simple one will do? New work shows that the simplest multilayer neural network, with a small twist, can perform some tasks as well as today's most sophisticated architectures.
What's new: Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, and a team at Google Brain revisited multilayer perceptrons (MLPs, also known as vanilla neural networks). They built MLP-Mixer, a no-frills model that approaches state-of-the-art performance in ImageNet classification.
Key insight: Convolutional neural networks excel at processing images because they're designed to discern spatial relationships, and pixels that are nearby one another in an image tend to be more related than pixels that are far apart. MLPs have no such bias, so they tend to learn interpixel relationships that exist in the training set and don't hold in real life. By modifying MLPs to process and compare images across patches rather than individual pixels, MLP-Mixer enables this basic architecture to learn useful image features.
How it works: The authors pretrained MLP-Mixer for image classification using ImageNet-21k, which contains 21,000 classes, and fine-tuned it on the 1,000-class ImageNet.
•	Given an image divided into patches, MLP-Mixer uses an initial linear layer to generate 1,024 representations of each patch. MLP-Mixer stacks the representations in a matrix, so each row contains all representations of one patch, and each column contains one representation of every patch.
•	MLP-Mixer is made of a series of mixer layers, each of which contains two MLPs, each made up of two fully connected layers. Given a matrix, a mixer layer uses one MLP to mix representations within columns (which the authors call token mixing) and another to mix representations within rows (which the authors call channel mixing). This process renders a new matrix to be passed along to the next mixer layer.
•	A softmax layer renders a classification.
Results: An MLP-Mixer with 16 mixer layers classified ImageNet with 84.15 percent accuracy. That's comparable to the state-of-the-art 85.8 percent accuracy achieved by a 50-layer HaloNet, a ResNet-like architecture with self-attention.
Yes, but: MLP-Mixer matched state-of-the-art performance only when pretrained on a sufficiently large dataset. Pretrained on 10 percent of JFT300M and fine-tuned on ImageNet, it achieved 54 percent accuracy on ImageNet, while a ResNet-based BiT trained the same way achieved 67 percent accuracy.
Why it matters: MLPs are the simplest building blocks of deep learning, yet this work shows they can match the best-performing architectures for image classification.
We're thinking: If simple neural nets work as well as more complex ones for computer vision, maybe it's time to rethink architectural approaches in other areas, too.},
  creationdate = {2021-08-18},
  owner        = {ISargent},
}

@Article{BanerjeeEtAl2021,
  author           = {Imon Banerjee and Ananth Reddy Bhimireddy and John L. Burns and Leo Anthony Celi and Li-Ching Chen and Ramon Correa and Natalie Dullerud and Marzyeh Ghassemi and Shih-Cheng Huang and Po-Chih Kuo and Matthew P Lungren and Lyle Palmer and Brandon J Price and Saptarshi Purkayastha and Ayis Pyrros and Luke Oakden-Rayner and Chima Okechukwu and Laleh Seyyed-Kalantari and Hari Trivedi and Ryan Wang and Zachary Zaiman and Haoran Zhang and Judy W Gichoya},
  title            = {Reading Race: AI Recognises Patient's Racial Identity In Medical Images},
  eprint           = {2107.10356},
  url              = {https://arxiv.org/abs/2107.10356},
  archiveprefix    = {arXiv},
  comment          = {AI can identify the self-reported race in medical images - that human cannot identify race. Not down to interpreting physical factors e.g. body mass, tissue density and even persisted with cropping, blurring and adding noise. i.e. unclear how machines are doing this. Demonstrates bias in AI and highlights how difficult it is to correct for.

More info in this edition of The Batch https://info.deeplearning.ai/the-batch-ai-recognizes-race-in-x-rays-robots-do-bees-work-transformers-pay-closer-attention-new-research-centers-1},
  creationdate     = {2021-11-01},
  keywords         = {ethics, AI, deep learning, bias},
  modificationdate = {2023-12-11T08:02:13},
  primaryclass     = {cs.CV},
  year             = {2021},
}

@Article{GuettaSSME2021,
  author        = {Nitzan Guetta and Asaf Shabtai and Inderjeet Singh and Satoru Momiyama and Yuval Elovici},
  title         = {Dodging Attack Using Carefully Crafted Natural Makeup},
  eprint        = {2109.06467},
  url           = {https://arxiv.org/abs/2109.06467},
  archiveprefix = {arXiv},
  comment       = {By looking at trained face recognition networks, team were able to design natural-looking face make up that caused the FR to fail to recognise people a large proportion fo the time.

From The Batch:
Researchers at Ben-Gurion University and NEC developed a system for applying natural-looking makeup that makes people unrecognizable to face recognition models. 
How it works: Working with 20 volunteers, the researchers used FaceNet, which learns a mapping from face images to a compact Euclidean space, to produce heat maps that showed which face regions were most important for identification. 
•	They used the consumer-grade virtual makeover app YouCam Makeup to adapt the heatmaps into digital makeup patterns overlaid on each volunteer's image. 
•	They fed iterations of these digitally done-up face shots to FaceNet until the subject was unrecognizable. 
•	Then a makeup artist physically applied the patterns to actual faces in neutral tones.
•	The volunteers walked down a hallway, first without and then with makeup, while being filmed by a pair of cameras that streamed their output to the ArcFace face recognizer. 
Results: ArcFace recognized participants wearing adversarial makeup in 1.2 percent of frames. It recognized those wearing no makeup in 47.6 percent of video frames, and those wearing random makeup patterns in 33.7 percent of frames.
Why it matters: This new technique requires only ordinary, unobtrusive makeup, doing away with accessories that might raise security officers' suspicions. It offers perhaps the easiest way yet for ordinary people to thwart face recognition — at least until the algorithms catch on.},
  creationdate  = {2021-10-04},
  keywords      = {deep learning, face recognition, adversarial attacks},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  year          = {2021},
}

@Article{SillaF2011,
  author    = {Silla, Carlos N. Jr and Alex A. Freitas},
  title     = {A survey of hierarchical classification across different application domains},
  doi       = {DOI 10.1007/s10618-010-0175-9},
  pages     = {31--72},
  volume    = {22},
  comment   = {A survey looking specifically at methods of classification that applies data with a a pre-established taxonomy including classes and meta-classes.},
  journal   = {Data Mining Knowledge Discovery},
  owner     = {ISargent},
  creationdate = {2021-10-06},
  year      = {2011},
}

@InProceedings{BinderKB2010,
  author    = {Binder, Alexander and Kawanabe, Motoaki and Brefeld, Ulf},
  booktitle = {Computer Vision -- ACCV 2009},
  date      = {2010},
  title     = {Efficient Classification of Images with Taxonomies},
  editor    = {Zha, Hongbin and Taniguchi, Rin-ichiro and Maybank, Stephen},
  isbn      = {978-3-642-12297-2},
  location  = {Berlin, Heidelberg},
  pages     = {351--362},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {We study the problem of classifying images into a given, pre-determined taxonomy. The task can be elegantly translated into the structured learning framework. Structured learning, however, is known for its memory consuming and slow training processes. The contribution of our paper is twofold: Firstly, we propose an efficient decomposition of the structured learning approach into an equivalent ensemble of local support vector machines (SVMs) which can be trained with standard techniques. Secondly, we combine the local SVMs to a global model by re-incorporating the taxonomy into the training process. Our empirical results on Caltech256 and VOC2006 data show that our local-global SVM effectively exploits the structure of the taxonomy and outperforms multi-class classification approaches.},
  comment   = {In Izzy's local library in 2010_LNCS5996_Book_ComputerVisionACCV2009.pdf},
  owner     = {ISargent},
  creationdate = {2021-10-06},
}

@Article{DimitrovskiKLD2011,
  author       = {Ivica Dimitrovski and Dragi Kocev and Suzana Loskovska and Sašo Džeroski},
  date         = {2011},
  journaltitle = {Pattern Recognition},
  title        = {Hierarchical annotation of medical images},
  doi          = {https://doi.org/10.1016/j.patcog.2011.03.026},
  issn         = {0031-3203},
  note         = {Semi-Supervised Learning for Visual Content Analysis and Understanding},
  number       = {10},
  pages        = {2436-2449},
  url          = {https://www.sciencedirect.com/science/article/pii/S0031320311001300},
  volume       = {44},
  abstract     = {We present a hierarchical multi-label classification (HMC) system for medical image annotation. HMC is a variant of classification where an instance may belong to multiple classes at the same time and these classes/labels are organized in a hierarchy. Our approach to HMC exploits the annotation hierarchy by building a single predictive clustering tree (PCT) that can simultaneously predict all annotations of an image. Hence, PCTs are very efficient: a single classifier is valid for the hierarchical semantics as a whole, as compared to other approaches that produce many classifiers, each valid just for one given class. To improve performance, we construct ensembles of PCTs. We evaluate our system on the IRMA database that consists of X-ray images. We investigate its performance under a variety of conditions. To begin with, we consider two ensemble approaches, bagging and random forests. Next, we use several state-of-the-art feature extraction approaches and combinations thereof. Finally, we employ two types of feature fusion, i.e., low and high level fusion. The experiments show that our system outperforms the best-performing approach from the literature (a collection of SVMs, each predicting one label at the lowest level of the hierarchy), both in terms of error and efficiency. This holds across a range of descriptors and descriptor combinations, regardless of the type of feature fusion used. To stress the generality of the proposed approach, we have also applied it for automatic annotation of a large number of consumer photos with multiple annotations organized in semantic hierarchy. The obtained results show that this approach is general and easily applicable in different domains, offering state-of-the-art performance.},
  keywords     = {Automatic image annotation, Hierarchical multi-label classification, Predictive clustering trees, Feature extraction from images},
  owner        = {ISargent},
  creationdate    = {2021-10-06},
}

@InProceedings{AhujaT2007,
  author       = {Ahuja, Narendra and Todorovic, Sinisa},
  booktitle    = {2007 IEEE 11th International Conference on Computer Vision},
  title        = {Learning the Taxonomy and Models of Categories Present in Arbitrary Images},
  doi          = {10.1109/ICCV.2007.4409039},
  pages        = {1-8},
  url          = {https://ieeexplore.ieee.org/abstract/document/4409039},
  comment      = {''approach: (1) An image is represented by a segmentation tree [1, 14] which captures the low-level, spatial and photometric image structure. Nodes at upper levels correspond to larger segments, while their children nodes capture embedded, smaller details (e.g., the quadwindow-group nodes in Fig. 1 are parents to the window nodes). (2) Category instances (e.g., roofs, doors, windows in Fig. 1) appear as similar subimages, whose corresponding subtrees are accessible in the segmentation trees. To identify the instances, we measure the similarity of all segments across the image set, in terms of their intrinsic photometric, geometric and topological properties, as well as in terms of the same properties of their embedded subregions. (3) The identified similar subimages are clustered, and the resulting clusters are treated as evidence and exact instances of the categories present. The similar subimages within a cluster (e.g., of all doors) together provide for robust learning of the subtree properties characterizing the associated category. (4) The clusters containing less complex subimages are associated with more common, simple catgories (rectangular panels). These subimages form components of the hierarchical definitions of subimages in other clusters representing more complex categories (windows, doors). The clusters inherit the containment properties of their constituent subimages, which allows us to establish hierarchical, containment links between the clusters (child link from the window cluster to the panel cluster), yielding a directed acyclic graph (DAG). The root nodes of the DAG represent the set of most complex categories, while those near the leaves represent the simplest, often most shared subcategories, as illustrated in Fig. 1. (5) The categories found in (3) may indeed represent different parts of a complex category. (roof and front wall of the house), and, may not belong to any single subtree in the segmentation tree. The detection of the parts can be used to encode such “cooccurrence” categories (house front marked cyan in Fig. 1). (6) To recognize the occurrence of any of the learned categories in a new image, its segmentation tree is searched for matches with the DAG. Any matches found denote the occurrences of the corresponding categories as well as all the associated subcategories. The subcategories, along with their hierarchical structure within the DAG, serve as a semantic (category-space) explanation of why the category is found. Simultaneously, the matches also specify the exact boundaries of the detected objects.'' ``These subcategories are discriminative, category-specific, and facilitate cross-category resolvability. These results suggests that the discovered taxonomy is meaningful.''},
  creationdate = {2021-10-06},
  owner        = {ISargent},
  year         = {2007},
}

@Article{AlsallakhJYLR2018,
  author       = {Bilal, Alsallakh and Jourabloo, Amin and Ye, Mao and Liu, Xiaoming and Ren, Liu},
  title        = {Do Convolutional Neural Networks Learn Class Hierarchy?},
  doi          = {10.1109/TVCG.2017.2744683},
  number       = {1},
  pages        = {152-162},
  volume       = {24},
  comment      = {''This suggests that classification decisions should be taken at different layers in deep CNNs to account for the varying complexity of the classes''},
  creationdate = {2021-10-06},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  owner        = {ISargent},
  year         = {2018},
}

@Article{LiYCLH2016,
  author        = {Yixuan Li and Jason Yosinski and Jeff Clune and Hod Lipson and John Hopcroft},
  title         = {Convergent Learning: Do different neural networks learn the same representations?},
  eprint        = {1511.07543},
  url           = {https://arxiv.org/abs/1511.07543},
  archiveprefix = {arXiv},
  comment       = {Different initiations of the same architecture learn some very similar representations and other completely different ones.

''. By defining a measure of similarity between units in different neural networks, can we come up with a permutation for the units of one network to bring it into a one-to-one alignment with the units of another network trained on the same task? Is this matching or alignment close, because features learned by one network are learned nearly identically somewhere on the same layer of the second network, or is the approach ill-fated, because the representations of each network are unique? (Answer: a core representation is shared, but some rare features are learned in one network but not another; see Section 3).
2. Are the above one-to-one alignment results robust with respect to different measures of neuron similarity? (Answer: yes, under both linear correlation and estimated mutual information metrics; see Section 3.2).
3. To the extent that an accurate one-to-one neuron alignment is not possible, is it simply because one network's representation space is a rotated version3 of another's? If so, can we find and characterize these rotations? (Answers: by learning a sparse weight LASSO model to predict one representation from only a few units of the other, we can see that the transform from one space to the other can be possibly decoupled into transforms between small subspaces; see Section 4).
4. Can we further cluster groups of neurons from one network with a similar group from another network? (Answer: yes. To approximately match clusters, we adopt a spectral clustering algorithm that enables many-to-many mappings to be found between networks. See Section S.3).
5. For two neurons detecting similar patterns, are the activation statistics similar as well? (Answer: mostly, but with some differences; see Section S.4).''},
  creationdate  = {2021-10-06},
  keywords      = {deep learning, representation learning},
  owner         = {ISargent},
  primaryclass  = {cs.LG},
  year          = {2016},
}

@Article{RoyEtAl2021,
  author        = {Abhijit Guha Roy and Jie Ren and Shekoofeh Azizi and Aaron Loh and Vivek Natarajan and Basil Mustafa and Nick Pawlowski and Jan Freyberg and Yuan Liu and Zach Beaver and Nam Vo and Peggy Bui and Samantha Winter and Patricia MacWilliams and Greg S. Corrado and Umesh Telang and Yun Liu and Taylan Cemgil and Alan Karthikesalingam and Balaji Lakshminarayanan and Jim Winkens},
  title         = {Does Your Dermatology Classifier Know What It Doesn't Know? Detecting the Long-Tail of Unseen Conditions},
  eprint        = {2104.03829},
  url           = {https://arxiv.org/abs/2104.03829v1},
  archiveprefix = {arXiv},
  comment       = {An out-of-distribution (OOD) detection problem is where there are some poorly represented classes that aren't represented by the majority of the data and therefore by the train algorithm. This may try to assign labels to OOD examples when we would actually want them flagged as OOD.

This paper proposes and demonstrates Hierarchical Outlier Detection (HOD) which has a fine then coarse level of classification - the fine level classifies to the class (including poorly represented classes) and the coarse level is binary - inlier or outlier - which is based on the summed class predictions.

Pretrained backbone (e.g. resnet) creates features which are then used to predict classes (in this case its skin conditions - which has a very long tail of underrepresented classes). Only some outlier classes are used in training, different ones are used in validation and testing. 

The HOD loss function contains two terms. One encourages the algorithm to identify the correct condition. The other encourages it to assign an accurate inlier or outlier label.

GoukHP2021 used a different approach.},
  keywords      = {transfer learning, outlier examples, deep learning},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-10-11},
  year          = {2021},
}

@Article{YuK2016,
  author        = {Fisher Yu and Vladlen Koltun},
  title         = {Multi-Scale Context Aggregation by Dilated Convolutions},
  eprint        = {1511.07122},
  url           = {https://arxiv.org/abs/1511.07122v3},
  archiveprefix = {arXiv},
  comment       = {State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction problems such as semantic segmentation are structurally different from image classification''.

''Modern image classification networks integrate multi-scale contextual information via successive pooling and subsampling layers that reduce resolution until a global prediction is obtained''. Transferring this approach to semantic segmentation requires upsampling - is the downsampling truly necessary?

This method keeps convolutions dense throughout the network using a context module that ``takes C feature maps as input and produces C feature maps as output. The input and output have the same form, thus the module can be plugged into existing dense prediction architectures''

''found that random initialization schemes were not effective for the context module. We found an
alternative initialization'' with a form of identity initialisation.

Also show ``that the accuracy of existing convolutional networks for semantic segmentation can be increased by removing vestigial components that had been developed for image classification''.},
  creationdate  = {2021-10-11},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  year          = {2016},
}

@Article{YamadaPWPT2021,
  author        = {Takaki Yamada and Adam Prügel-Bennett and Stefan B. Williams and Oscar Pizarro and Blair Thornton},
  title         = {GeoCLR: Georeference Contrastive Learning for Efficient Seafloor Image Interpretation},
  eprint        = {2108.06421},
  url           = {https://arxiv.org/abs/2108.06421},
  archiveprefix = {arXiv},
  comment       = {The GeoCLR paper.

Contrastive learning on seafloor imagery to learn latent representations. They use the geo location of images to define what is similar. The resulting network then projects data into latent space for labelling. Also develop an efficient method for human labelling - perform k-means on latent features and then select systemmatically from the cluster centres to obtain manual labels for training a mapping linear model (very similar to Toponet approach!)

When the clusters are unbalanced:
''However, the method suffers when the number of images in each class is not balanced, since classes are represented in proportion to their relative abundance, those with small populations tend to exhibit poor performance. The hierarchical kmeans clustering (Nister and Stewenius, 2006), or H-kmeans, method allows for balanced representation of the variety of images present in a dataset without the need for additional human effort, and was shown to be effective for guiding human labelling effort in (Yamada et al., 2021b). In this method, kmeans clustering is first applied to latent representations with k=m to find representative clusters of images in the dataset. An appropriate value for m can be automatically determined for each dataset using the elbow method (Satopaa et al., 2011)''


From discusison with Blair - contrastive learning is much more sensitive that LGA to the hypothesis that nearby images are similar being incorrect. But contrastive learning is better oever all the datasets that they've worked with (because hypothesis is usually correct).},
  creationdate  = {2021-10-11},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  year          = {2021},
}

@Article{BardesPL2021,
  author           = {Adrien Bardes and Jean Ponce and Yann LeCun},
  title            = {VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning},
  eprint           = {2105.04906},
  url              = {https://arxiv.org/abs/2105.04906},
  archiveprefix    = {arXiv},
  comment          = {''We introduce VICReg (Variance-Invariance-Covariance Regularization), a new self-supervised algorithm for learning image representations based on three simple principles, variance, invariance and covariance with clear objectives and interpretations. The variance principle constraints the variance of the embeddings along each dimension independently, and is a simple yet effective method against collapse. More precisely we use a hinge loss which constrains the standard deviation computed along the batch dimension of the embeddings to reach a fixed target. Unlike contrastive methods, no negative pairs are required and the embeddings are implicitly encouraged to be different from each other without any direct comparison between them. The invariance principle uses a standard mean-squared euclidean distance to learn invariance to multiple views of an image. Finally, the covariance principle borrows the covariance criterion of Barlow Twins [49], which decorrelates the different dimensions of the learned representations with the objective to spread the information across the dimensions, avoiding a dimension collapse. This criterion consists in penalizing the off-diagonal coefficients of the covariance matrix of the embeddings.''

''After pretraining, the projector is discarded and the representations of the encoder are used for downstream tasks''.

''We define the variance regularization term v as a hinge loss on the standard deviation of the projections along the batch dimension''.

''This criterion will enforce the variance inside the current batch to be γ along each dimension, preventing collapsing solutions where all the inputs are mapped to the same vector''.

''Inspired by Barlow Twins''

''decorrelating the different dimensions of the projections and preventing these dimensions from encoding similar information ... the invariance criterion as between Z and Z0 as the mean-squared euclidean distance between each pair of vectors ... overall loss function is a weighted average of the invariance, variance and covariance terms''.

''we train a linear classifier on top of the frozen representations of our ResNet-50 backbone pretrained with VICReg. We also evaluate the performance of the backbone when fine-tuned with a linear classifier on a subset of ImageNet's training set''.

''VICReg replaces the stop-gradient operation [used in SimSiam and BYOL], which is an architectural trick, by an explicit constraint on the variance and the covariance of the projections, which achieves the same goal of decorrelating the representations and avoiding collapse, while being clearer and more interpretable''.

''VICReg eliminates these negative comparisons and replace them by an explicit constraint on the variance of the embeddings, which efficiently plays the role of a negative term between the vectors''.},
  creationdate     = {2021-10-11},
  keywords         = {Self-supervised learning, pretraining},
  modificationdate = {2022-05-04T11:54:09},
  owner            = {ISargent},
  primaryclass     = {cs.CV},
  year             = {2021},
}

@Article{PoravMN2018,
  author        = {Horia Porav and Will Maddern and Paul Newman},
  title         = {Adversarial Training for Adverse Conditions: Robust Metric Localisation using Appearance Transfer},
  eprint        = {1803.03341},
  url           = {https://arxiv.org/abs/1803.03341},
  archiveprefix = {arXiv},
  comment       = {This is related to the talk that I saw at a BMVA meeting in 2018 that used GANs to produce views of the same location under different weather conditions. See also PoravBN2019.},
  keywords      = {deep learning, generative},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-10-18},
  year          = {2018},
}

@Article{PoravBN2019,
  author        = {Horia Porav and Tom Bruls and Paul Newman},
  title         = {Don't Worry About the Weather: Unsupervised Condition-Dependent Domain Adaptation},
  eprint        = {1907.11004},
  url           = {https://arxiv.org/abs/1907.11004},
  archiveprefix = {arXiv},
  comment       = {This is related to the talk that I saw at a BMVA meeting in 2018 that used GANs to produce views of the same location under different weather conditions. See also PoravMN2018.},
  keywords      = {deep learning, generative},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-10-18},
  year          = {2019},
}

@Article{TanL2020,
  author        = {Mingxing Tan and Quoc V. Le},
  title         = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
  eprint        = {1905.11946},
  url           = {https://arxiv.org/abs/1905.11946v5},
  archiveprefix = {arXiv},
  comment       = {The EfficientNet paper. Look at the impact of scaling model depth, width (number of units per layer) and resolution (dimensions of input image) and find that scaling all uniformly obtains best results whilst allowing user to stay within limits of computation. Define a baseline network (EfficientNet-B0). Also demonstrate that per FLOPS, EfficientNet results are more accurate c.f. Resnets, Xception, Inception-resnet, DenseNet...

''We empirically observe that different scaling dimensions are not independent. Intuitively, for higher resolution images, we should increase network depth, such that the larger receptive fields can help capture similar features that include more pixels in bigger images''
''we propose a new compound scaling method, which use a compound coefficient φ to uniformly scales network width, depth, and resolution in a principled way''
''Notably, the FLOPS of a regular convolution op is proportional to d, $w^2$, $r^2$, i.e., doubling network depth will double FLOPS, but doubling network width or resolution will increase FLOPS by four times.''.},
  creationdate  = {2021-10-19},
  keywords      = {convolutional neural networks, deep learning, architecture, training},
  owner         = {ISargent},
  primaryclass  = {cs.LG},
  year          = {2020},
}

@Article{FrankF2020,
  author        = {Steven J. Frank and Andrea M. Frank},
  title         = {Salient Slices: Improved Neural Network Training and Performance with Image Entropy},
  eprint        = {1907.12436},
  url           = {https://arxiv.org/abs/1907.12436},
  archiveprefix = {arXiv},
  comment       = {Uses entropy to identify the most 'salient' images in a dataset. The method finds the information content of each image, compared to the dataset, so that those with the most information can be selected - feels like a good way of selecting training data},
  keywords      = {dataset balancing},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-11-09},
  year          = {2020},
}

@Article{ChenAP2021,
  author     = {Boyuan Chen and Pieter Abbeel and Deepak Pathak},
  title      = {Unsupervised Learning of Visual 3D Keypoints for Control},
  eprint     = {2106.07643},
  eprinttype = {arXiv},
  url        = {https://arxiv.org/abs/2106.07643},
  volume     = {abs/2106.07643},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2106-07643.bib},
  comment    = {This paper could hint at something relevant, especially in the 3D realm - use a combination of unsupervised approaches with multiple views of an object and reinforcement learning (RL, keep trying and adjusting until a task - often robotic - is achieved) to identify the keypoints of an object that result in the best outcomes for the RL task. All we have to do is think what is our RL task for 3D capture - something like drawing a wireframe that results in the same shading effects?},
  journal    = {CoRR},
  keywords   = {3D, unsupervised, keypoints, reinforcement learning},
  owner      = {ISargent},
  creationdate  = {2021-11-15},
  year       = {2021},
}

@Article{GilensP2014,
  author       = {Gilens, M. and Page, B.},
  title        = {Testing Theories of American Politics: Elites, Interest Groups, and Average Citizens.},
  doi          = {doi:10.1017/S1537592714001595},
  issue        = {3},
  pages        = {564--581},
  volume       = {12},
  comment      = {From Kelton2020: ``while there's a fair amount of overlap between the political preferences of everyday Americans and the political preferences of rich elites, which the two sets of interests diverge, it's almost invariable the rich whose desires are served by the political system''},
  creationdate = {2021-11-15},
  journal      = {Perspectives on Politics},
  keywords     = {politics, society},
  owner        = {ISargent},
  year         = {2014},
}

@Book{BanerjeeD2019,
  author           = {Abhijit V Banerjee and Esther Duflo},
  date             = {2019},
  title            = {Good Economics for Hard Times},
  comment          = {Chock full of empirical evidence for the effect on economy and people's lives of different interventions and policies particularly from studying poor countries. Concludes that at worst immigration makes no difference to a host economy - and it often improves it (immigrants are consumer as well as workers). Trade is also good, but governments need to support regions that are affected by losses due to trade (e.g. industries that decline because goods are imported). People are much more 'sticky' than conventional economics assumes - there are very many reasons why people don't follow jobs and opportunities such as it is risky to leave your social support network (even if a job is guaranteed). 

Things that struck me include: 

Move from government control of economy seems to result in increase in growth but also inequality: China 1979, Korea early 1960s, Vietnam 1990s and India now - page 57 

A green new deal - funding the poor to leapfrog development into better technology and adapt to changing climate - page 225 

``In the 1980s, whilst growth remained sluggish, inequality exploded. Thanks to the outstanding and painstaking work of Thomas Piketty and Emmanuel Saez, the world now knows what happened: 1980 is the year Reagan was elected. It was almost exactly the year the share of national income that goes to the richest 1 percent reverses fifty years of decline and starts a relentless climb in the United States...the increase in income inequality was accompanied by a rise in wealth inequality...the story in the UK is very similar...Mrs Thatcher'' - page 238 

``Even if there had been a trickle-down effect of lower taxes, as its advocates claimed, one would expect wage growth to have accelerated … but the opposite happened... that this great reversal takes place during the Reagan and Thatcher years is probably not coincidental but there is no reason to assume Reagan and Thatcher are the reason it happened'' - page 239 

When the premium for a job is unrelated to its usefulness you end up with a loss of talent, e.g. coders, away from public service sectors into sectors such as finance (where arguably their talents are then implicated in financial instability as they create algorithms that the financial system is not design to withstand) - page 245 

``high top tax rates may actually lead to a reduction not just in inequality after taxes but also in inequality before taxes'' - page 247 

``any policy sold in the name of growth … is likely to be bogus'' ``perhaps we should be even more scared if we thank that such a policy might work, because growth will benefit on the happy few'' - page 262 

People's willingness to work does not seem to be changed by how much they are taxed (against Friedman's claim that taxing disincentivises work) - the evidence is from Switzerland when it changed its tax policy in the late 1990s/early 2000s. Different Cantons had a tax holiday at different times and people knew in advance the year they would not be taxed for but it made no difference to their working [izzy: c.f. with Kelton/MMT view that taxation incentivises work] - page 265 

Various studies looking at what self-image compels people to cheat/lie - whether or not they are reminded of their role can make a difference, in either direction - page 272 

Trade is generally good, and trade wars are generally bad but `left-behind' people must be helped - page 304 

Increasingly valued skills include empathy and social skills - could these be taught? (they can certainly be learned) - page 306},
  creationdate     = {2021-11-15},
  keywords         = {economics, macroeconmics},
  modificationdate = {2023-03-04T15:21:54},
  owner            = {ISargent},
  relevance        = {relevant},
}

@Book{Kelton2020,
  author           = {Stephanie Kelton},
  date             = {2020},
  title            = {The Deficit Myth},
  comment          = {How Modern Monetary Theory (MMT) proposes a different way of running national economies for the benefit of everyone. Sovereign/fiat economies - those that issue their own currency and that don't borrow in other currencies - can issue any amount into their economy for purposes deemed beneficial. The claim that the economy has run out of money is false - that money has simply gone from the Government purse into the pockets and bank accounts of the public (including business) where it is capable of doing good. The only risk is that of inflation - should too much money be injected. This can be mitigated with taxation and borrowing. Taxation is not a means of obtaining money for the public purse but rather a way of limiting inflation and driving positive outcomes for the country (e.g. by taxing those things that are bad for the country). Put simply, the economy won't work unless money is issued into it - it's not (Tax And Borrow) then Spend ((TAB)S) but Spend then (Tax And Borrow) (S(TAB)). Further, the theory states that taxation is the motivation to keep people working - because people need to pay tax in the national currency, they are motivated to earn in that currency, otherwise they would not work. [I struggle a bit with the rationale for this last part]. Focussed on US economy but with relevance to UK, Japan and other sovereign economies. 

Monopoly illustrates this: it can't get going without issuing money first and the instructions state that should the 'bank' (state) run out of money, more can be written on any piece of paper - page 27 

At least four important reasons for taxation: - page 32 

Taxes enable the governments to provision themselves without the use of explicit force 

Preventing inflation (if the government wants to boost spending on health care and education it may need to remove some spending power from the rest of use to prevent generous outlays from pushing up prices) - page 33 

Taxes are a powerful way to alter the distribution of wealth and income - page 33 

Use taxes to encourage and discourage certain behaviours - page 34 

''About half of all new income goes to the top 1 percent and just three families own more wealth than the bottom half of America'' - page 33 

Kelton arrived at an agreement with the ideas of MMT (which she initially rejected) after research into answering the question ``Do taxes and bonds finance government spending?'' To which her answer was eventually ``No'' - page 35 

''Trumps personal income tax cuts did little to boost the overall economy because they were heavily skewed in favour of those at the very top of the income distribution, more than 80 percent of the benefits went to those in the top 1 percent'' - page 62 

''...we agree that we should reply on adjustments in taxes and spending (fiscal policy) rather than interest rates (monetary policy) to balance our economy. We also agree that fiscal deficits, in and of themselves are neither good nor bad. What matters is not whether the government's budget is in surplus or deficit but whether the government is using its budget to achieve good outcomes for the rest of the economy...'' - page 63 

''In our view, the most effective full employment policy is one that targets the unemployed directly. Instead of aiming spending at infrastructure and hoping jobs will trickle down to the unemployed, MMT proposes [a bottom up approach]. It takes workers as they are, and where they are, and it fits the job to their individual capabilities and needs of the communities...federal job guarantee...the private sector would shed jobs, but new jobs would immediately spring forth in the public service'' - pages 66-67 

Wealth tax does not damage the economy ``how many fewer cars, swimming pools, tennis courts or luxury vacations will Bezos purchase after 2 percent of his wealth is taxed away?'' - page 71 

Benefits of being a currency-issuing economy rather than non-sovereign economy such as members of the Euro - pages 84-85 

When the government substantially reduces national debt, the economy falls into depression - page 96 

''we should come to grips with the fact the the thing we call the national debt is nothing more than a footprint form the past. It tells us where we've been, not where we're going. It records the history of the many deficits that have been run since the birth of our government in 1789...wars...recessions...decisions'' - page 100 

''the truth is, government deficits aren't the villains of progress. They don't make it harder for the private sector to borrow and invest. In almost all cases, they make it easier...Whether those dollars arrive in the form of tax cuts or increased spending, they leave some of us with greater spending power. And spending power is the lifeblood of capitalism'' - page 126 

 ''the neoliberal term … structural reform … the polite way of describing an agenda aimed at driving down labor costs (wages and pensions) to increase competitiveness by reducing the costs of production'' - page 135 

Countries have different monetary sovereignties: 

Many advanced economies enjoy a high degree of monetary sovereignty - enormous opportunities to invest, demand for the currency is high - page 144 

Many countries weaken their monetary sovereignty by continuing to peg their currencies to the US dollar (e.g. Saudi Arabia, Lebanon, Jordan) - page 145 

Some strip their monetary sovereignty by joining a currency union (e.g. Euro) - page 145 

Developing countries have low sovereignty because they need to buy in good of value - page 145 

''Many developing countries also lack the ability - or have been told they don't have the ability - to produce enough food, energy and medicine to meet their own domestic demand. So they rely on developed countries to supply them with imports … almost always need US dollars to pay for crucial imports … exporting cheap labor and commodities, while importing expensive high-value items, tends to … perpetual trade deficits'' - page 146 

Organisations like IMF, WTO and World Bank, often run by bankers and diplomats from wealthy countries, no commitment to full employment around the work, tend to recommend developing countries in crisis undertake drastic cuts to government expenditure, tight monetary policy (e.g. high interst rates) to lure back investors and more free trade [see Klein: Shock Doctrine] - page 149 

Nice example of Argentina deciding to stop pegging exchange rate to US dollars, defaulting on foreign debt, massive job creation - page 152  

Free trade agreements often exploit labour, force fossil fuel extraction and undermine monetary sovereignty for developing nations - page 153-4 

The deficits that matter, that have been ignored for far too long: 
* The good jobs deficit, unemployment, underemployment, being forced to choose between job and network of family and friends 
* The savings deficit, lack of savings and pensions, worse for whole sections of society 
* The health care deficit (much worse in US, but possibly increasing in UK) 
* The education deficit (again worse in US, but still huge differences of quality and opportunity in UK) 
* The infrastructure deficit, networks and public buildings, services and amenities. Possibly most important is affordable, quality housing 
* The climate deficit 
* The democracy deficit - the deficit between the few and the many, the powerful and the powerless, those with a voice and those without. Those with money and wealth have influence and leverage. References GilensP2014 

The Gini coefficient - a measure of the income inequality, 0 is perfectly egalitarian, 1 means that one personal literally gets all the income generated - US has the worst of all advanced economies - page 220 

''A 2015 study by the IMF found that ``an increase in the income share of the bottom 20\% (the poor) is associated with higher GDP growth, `` while ``GDP growth actually declines'' when ``the income share of the top 20\% (the rich) increases'' - page 226 

The fiscal policy should sit in the driver's seat, instead of monetary policy as it is now, with a hands-free feature (such as unemployment entitlements, healthcare, etc) - page 243 

Such a major national commitment would need the government to command more of the economy's real resources - scientists, engineers, civil servants, … and managing these so that inflation does not accelerate - page 259-60 

A transition to a cleaner more equitable economy could be achieved e.g. by government buying dirty assets such as high-emissions generators e.g. by increasing funding into research and development - page 262

MMT},
  creationdate     = {2021-11-15},
  keywords         = {macroeconomics, modern modetary theory},
  modificationdate = {2022-12-03T17:52:01},
  owner            = {ISargent},
}

@Article{WightmanTJ2021,
  author        = {Ross Wightman and Hugo Touvron and Hervé Jégou},
  title         = {ResNet strikes back: An improved training procedure in timm},
  eprint        = {2110.00476},
  url           = {https://arxiv.org/abs/2110.00476},
  archiveprefix = {arXiv},
  comment       = {Nice paper illustrating how it is the tweaking and new developments that result in better results with each iteration of ml algorithms - not necesarily the architecture.

Return to Resnet but apply a number of developments that have occurred since its publication in 2015:

Loss function considers different augmentations (mixup and cutmix) which blend images from different classes - loss calculated for n-hot encoding (1 for all classes in mix, 0 otherwise) - binary cross entropy. [Could be interesting to train toponet with all classes present, not just the central one.]

Regularize using (a) label smoothing which distrubutes some epsilon values between all other classes (b) repeated augmentation, which includes all the different augmentations of an image in the batch, and (c) stochastic-Depth  but which blocks are randomly removed, similarly to dropout.

For optimization they use LAMB, which is better for much larger batch sizes.

Find that they get excellent results for ResNet-50 using the right combination of these - results that beat more recent developments. However, using these same combinations of training proceedures does not always gain better results for other architectures, expecially those that are more disimilar to ResNet-50.

Basically, we don't know how to get the best results.

Nice summaries of the timm library, ResNet, training methods and architectures.

See https://www.youtube.com/watch?v=Gl0s0GDqN3c},
  keywords      = {deep learning, image machine learning, convolutional neural networks, resnets, architecture, training},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-11-23},
  year          = {2021},
}

@Article{DosovitskiyEtAl2021,
  author        = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  title         = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  eprint        = {2010.11929},
  url           = {https://arxiv.org/abs/2010.11929},
  archiveprefix = {arXiv},
  comment       = {The Vision Transformer paper ViT.

Take an image, split it into smaller non-overlapping patches and then flatten the pixel values in these patches before passing each patch's data into it's positions input into a fully-connected layer.},
  keywords      = {deep learning, image machine learning, architecture},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-11-23},
  year          = {2021},
}

@Article{PatchesAreAllYouNeed2021,
  author    = {Anonymous},
  title     = {Patches Are All You Need?},
  note      = {under review},
  url       = {https://openreview.net/forum?id=TVHS5Y4dNvM},
  booktitle = {Submitted to The Tenth International Conference on Learning Representations},
  comment   = {Demonstrate that its the  patches that were used in the vision transformer ViT DosovitskiyEtAl2021 paper, rather than the the transformer.

Use simple convolutional layers rather than self attention (as used in ViT).

Spatial- (depth-wise - kernel over space) and channel-wise (point-wise - 1x1 conv) mixing is key.  Convmix layer has depth-wise convs first, followed by residual connection, then pointwise convolution. Preseves the number of channels and spatial extent - output from layer is same dims as input - isotropic model.

Really easy implementation - including a 280 character implementation for twitter!

See also https://www.youtube.com/watch?v=Gl0s0GDqN3c},
  keywords  = {image machine learning, convolutional neural networks, architecture, training},
  owner     = {ISargent},
  creationdate = {2021-11-23},
  year      = {2022},
}

@Article{BelloFDCSLSZ2021,
  author        = {Irwan Bello and William Fedus and Xianzhi Du and Ekin D. Cubuk and Aravind Srinivas and Tsung-Yi Lin and Jonathon Shlens and Barret Zoph},
  title         = {Revisiting ResNets: Improved Training and Scaling Strategies},
  eprint        = {2103.07579},
  url           = {https://arxiv.org/abs/2103.07579},
  archiveprefix = {arXiv},
  comment       = {Improvements in ML image recognition performance broadly arise along four orthogonal axes: architecture, training/regularization methodology, scaling strategy and using additional training data. It can be difficult to disentangle the contribution of these.

This paper revisits ResNet with two architectural changes:
- ResNet-D which replaces the 7x7 conv in the stem with 3 3x3 convolutions as well as changing some stride sizes, swapping 1x1 convultions with 2x2 average pooling and replacing a 3x3 max pooling with a 3x3 convolution
- Squeeze-and-Excitation - see HuSASW2019
and various training methods:
- Matching EfficientNet setup - train over different width, depth and resolution multipliers
- Weight decay, label smoothing, dropout and stochastic depth (see WightmanTJ2021) regularization
- Data augmentation (with RandAugment)
- Hyperparameter tuning

Improving the training methods alone adds 3.2percent to the baseline 79percent top-1 accuracy. The two architectural changes added a further ~1percent

Loads of different experiements - ResNet architectures consistently performing more accurately for time taken on training step than EfficientNet (Pareto Curves).},
  keywords      = {deep learning, image machine learning, convolutional neural networks, resnets, architecture, training},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-11-24},
  year          = {2021},
}

@Article{HuSASW2019,
  author        = {Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu},
  title         = {Squeeze-and-Excitation Networks},
  eprint        = {1709.01507},
  url           = {https://arxiv.org/abs/1709.01507},
  archiveprefix = {arXiv},
  comment       = {The Squeeze-and-Excitation paper


which average pools signal from the entire feature map (squeeze) to produce a channel descriptor. This vector (one value from each feature maps) is then used to scale the original feature maps (excitation). I think the channel descriptor scales the feature maps using learned weights.

''The function of this descriptor is to produce an embedding of the global distribution of channel-wise feature responses, allowing information from the global receptive field of the network to be used by all its layers''},
  creationdate  = {2021-11-24},
  keywords      = {deep learning, image machine learning, convolutional neural networks, resnets, architecture},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  year          = {2019},
}


@Online{RadfordSKKA2021,
  author           = {Alec Radford and Ilya Sutskever and Jong Wook Kim and Gretchen Krueger and Sandhini Agarwal},
  date             = {2021-01-05},
  title            = {CLIP: ConnectingText and Images},
  url              = {https://openai.com/blog/clip/},
  comment          = {Create a more robust  task agnostic pre-trained backbone for image classification by training contrastively against the phrases that are associated with images (e.g. on the web).

Nice

''Our method uses an abundantly available source of supervision: the text paired with images found across the internet. This data is used to create the following proxy training task for CLIP: given an image, predict which out of a set of 32,768 randomly sampled text snippets, was actually paired with it in our dataset.''

Probably a better paper to reference is RadfordEtAl2021},
  creationdate     = {2021-12-02},
  keywords         = {deep learning, training, natural language processing, imagery},
  modificationdate = {2022-06-29T11:09:33},
}

@Article{HeCXLDG2021,
  author           = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollár, Piotr and Girshick, Ross},
  title            = {Masked Autoencoders Are Scalable Vision Learners},
  doi              = {10.48550/ARXIV.2111.06377},
  url              = {https://arxiv.org/abs/2111.06377},
  comment          = {Unsupervised training by masking out chunks of input image and output should be reconstruction of missing data.
  
  Method of training that requires less computation.

Asymmetric encoder-decoder with transformers for both. The encoder's parameter count was roughly an order of magnitude greater than the decoder's.

Input is images with portions masked out. Only the unmasked components are included - not the mask tokens.

Decoder reconstructs images using latent representations and the mask tokens. 

''We accelerate training (by 3x or more) and improve accuracy''.},
  copyright        = {Creative Commons Attribution 4.0 International},
  creationdate     = {2021-11-24},
  keywords         = {Vision transformers, efficiency, deep learning, autoencoder, unsupervised},
  modificationdate = {2022-07-11T08:40:19},
  owner            = {ISargent},
  publisher        = {arXiv},
  year             = {2021},
}

@Article{SargentHH2015,
  author           = {Isabel Sargent and David Holland and Jenny Harding},
  date             = {2015-12-21},
  journaltitle     = {ISPRS International Journal of Geo-Information},
  title            = {The building blocks of user-focused 3D city models},
  issue            = {4},
  volume           = {4},
  creationdate     = {2023-06-23T16:56:59},
  keywords         = {research, 3D},
  modificationdate = {2023-12-04T21:06:30},
  owner            = {izzy_olivine},
}

@Comment{jabref-meta: databaseType:biblatex;}
